---
title: å¤©æ± _äºŒæ‰‹è½¦ä»·æ ¼é¢„æµ‹_Task4_å»ºæ¨¡è°ƒå‚
date: 2020-05-13 02:53:38
categories: Algorithm
tags: 
    - Editor
    - DataMining
    - Tianchi
    - Study
    - Jupyter
    - seaborn
    - å†…å­˜ä¼˜åŒ–
    - äº”æŠ˜äº¤å‰éªŒè¯
    - cross validation
    - Linear Regression
    - Ridgeæ­£åˆ™åŒ–
    - Lassoæ­£åˆ™åŒ–
    - svm
    - å†³ç­–æ ‘
    - éšæœºæ£®æ—
    - æ¢¯åº¦æå‡æ ‘(GBDT)
    - å¤šå±‚æ„ŸçŸ¥æœº(MLP)
    - XGBoost
    - LightGBM
    - è´ªå¿ƒè°ƒå‚
    - ç½‘æ ¼è°ƒå‚
    - è´å¶æ–¯è°ƒå‚
    - pandas
mathjax: true
image: "https://blog-1259799643.cos.ap-shanghai.myqcloud.com/2020-05-13-8.jpg"
---


å¤©æ± æ•°æ®æŒ–æ˜ï¼Œç»„å¯¹å­¦ä¹ ç³»åˆ—ï¼Œç³»ç»Ÿåœ°å„ç§ç®—æ³•çš„æ‚ç³…ã€‚
<!-- more -->



# 0ï¸âƒ£ å‰è¨€
&emsp;&emsp;æœ¬ç« æ€ç»´å¯¼å›¾ï¼š

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200505184801159.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0V4Y2FsaWJ1clVsaW1pdGVk,size_16,color_FFFFFF,t_70#pic_center)


### 0ï¸âƒ£.1ï¸âƒ£ èµ›é¢˜é‡è¿°
    
&emsp;&emsp;è¿™æ˜¯ä¸€é“æ¥è‡ªäºå¤©æ± çš„æ–°æ‰‹ç»ƒä¹ é¢˜ç›®ï¼Œç”¨`æ•°æ®åˆ†æ`ã€`æœºå™¨å­¦ä¹ `ç­‰æ‰‹æ®µè¿›è¡Œ [äºŒæ‰‹è½¦å”®å–ä»·æ ¼é¢„æµ‹](https://tianchi.aliyun.com/competition/entrance/231784/information) çš„å›å½’é—®é¢˜ã€‚èµ›é¢˜æœ¬èº«çš„æ€è·¯æ¸…æ™°æ˜äº†ï¼Œå³å¯¹ç»™å®šçš„æ•°æ®é›†è¿›è¡Œåˆ†ææ¢è®¨ï¼Œç„¶åè®¾è®¡æ¨¡å‹è¿ç”¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæµ‹è¯•æ¨¡å‹ï¼Œæœ€ç»ˆç»™å‡ºé€‰æ‰‹çš„é¢„æµ‹ç»“æœã€‚å‰é¢æˆ‘ä»¬å·²ç»è¿›è¡Œè¿‡EDAåˆ†æåœ¨è¿™é‡Œ[å¤©æ± _äºŒæ‰‹è½¦ä»·æ ¼é¢„æµ‹_Task1-2_èµ›é¢˜ç†è§£ä¸æ•°æ®åˆ†æ
](https://blog.csdn.net/ExcaliburUlimited/article/details/105021630)ä»¥åŠ[å¤©æ± _äºŒæ‰‹è½¦ä»·æ ¼é¢„æµ‹_Task3_ç‰¹å¾å·¥ç¨‹](https://editor.csdn.net/md/?articleId=105170015)

### 0ï¸âƒ£.2ï¸âƒ£ æ•°æ®é›†æ¦‚è¿°
&emsp;&emsp;èµ›é¢˜å®˜æ–¹ç»™å‡ºäº†æ¥è‡ªEbay Kleinanzeigençš„äºŒæ‰‹è½¦äº¤æ˜“è®°å½•ï¼Œæ€»æ•°æ®é‡è¶…è¿‡**40w**ï¼ŒåŒ…å«**31åˆ—**å˜é‡ä¿¡æ¯ï¼Œå…¶ä¸­**15åˆ—**ä¸ºåŒ¿åå˜é‡ï¼Œå³`v0`è‡³`v15`ã€‚å¹¶ä»ä¸­æŠ½å–**15ä¸‡æ¡**ä½œä¸ºè®­ç»ƒé›†ï¼Œ**5ä¸‡**æ¡ä½œä¸ºæµ‹è¯•é›†Aï¼Œ**5ä¸‡**æ¡ä½œä¸ºæµ‹è¯•é›†Bï¼ŒåŒæ—¶å¯¹`name`ã€`model`ã€`brand`å’Œ`regionCode`ç­‰ä¿¡æ¯è¿›è¡Œè„±æ•ã€‚å…·ä½“çš„æ•°æ®è¡¨å¦‚ä¸‹å›¾ï¼š

<div class="table-wrapper" style = "center"><table style = "center">
<thead>
<tr style = "center">
<th><strong>Field</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody style = "center">
<tr style = "center">
<td >SaleID</td>
<td>äº¤æ˜“IDï¼Œå”¯ä¸€ç¼–ç </td>
</tr>
<tr>
<td>name</td>
<td>æ±½è½¦äº¤æ˜“åç§°ï¼Œå·²è„±æ•</td>
</tr>
<tr>
<td>regDate</td>
<td>æ±½è½¦æ³¨å†Œæ—¥æœŸï¼Œä¾‹å¦‚20160101ï¼Œ2016å¹´01æœˆ01æ—¥</td>
</tr>
<tr>
<td>model</td>
<td>è½¦å‹ç¼–ç ï¼Œå·²è„±æ•</td>
</tr>
<tr>
<td>brand</td>
<td>æ±½è½¦å“ç‰Œï¼Œå·²è„±æ•</td>
</tr>
<tr>
<td>bodyType</td>
<td>è½¦èº«ç±»å‹ï¼šè±ªåè½¿è½¦ï¼š0ï¼Œå¾®å‹è½¦ï¼š1ï¼Œå¢å‹è½¦ï¼š2ï¼Œå¤§å·´è½¦ï¼š3ï¼Œæ•ç¯·è½¦ï¼š4ï¼ŒåŒé—¨æ±½è½¦ï¼š5ï¼Œå•†åŠ¡è½¦ï¼š6ï¼Œæ…æ‹Œè½¦ï¼š7</td>
</tr>
<tr>
<td>fuelType</td>
<td>ç‡ƒæ²¹ç±»å‹ï¼šæ±½æ²¹ï¼š0ï¼ŒæŸ´æ²¹ï¼š1ï¼Œæ¶²åŒ–çŸ³æ²¹æ°”ï¼š2ï¼Œå¤©ç„¶æ°”ï¼š3ï¼Œæ··åˆåŠ¨åŠ›ï¼š4ï¼Œå…¶ä»–ï¼š5ï¼Œç”µåŠ¨ï¼š6</td>
</tr>
<tr>
<td>gearbox</td>
<td>å˜é€Ÿç®±ï¼šæ‰‹åŠ¨ï¼š0ï¼Œè‡ªåŠ¨ï¼š1</td>
</tr>
<tr>
<td>power</td>
<td>å‘åŠ¨æœºåŠŸç‡ï¼šèŒƒå›´ [ 0,  600 ]</td>
</tr>
<tr>
<td>kilometer</td>
<td>æ±½è½¦å·²è¡Œé©¶å…¬é‡Œï¼Œå•ä½ä¸‡km</td>
</tr>
<tr>
<td>notRepairedDamage</td>
<td>æ±½è½¦æœ‰å°šæœªä¿®å¤çš„æŸåï¼šæ˜¯ï¼š0ï¼Œå¦ï¼š1</td>
</tr>
<tr>
<td>regionCode</td>
<td>åœ°åŒºç¼–ç ï¼Œå·²è„±æ•</td>
</tr>
<tr>
<td>seller</td>
<td>é”€å”®æ–¹ï¼šä¸ªä½“ï¼š0ï¼Œéä¸ªä½“ï¼š1</td>
</tr>
<tr>
<td>offerType</td>
<td>æŠ¥ä»·ç±»å‹ï¼šæä¾›ï¼š0ï¼Œè¯·æ±‚ï¼š1</td>
</tr>
<tr>
<td>creatDate</td>
<td>æ±½è½¦ä¸Šçº¿æ—¶é—´ï¼Œå³å¼€å§‹å”®å–æ—¶é—´</td>
</tr>
<tr>
<td>price</td>
<td>äºŒæ‰‹è½¦äº¤æ˜“ä»·æ ¼ï¼ˆé¢„æµ‹ç›®æ ‡ï¼‰</td>
</tr>
<tr>
<td>vç³»åˆ—ç‰¹å¾</td>
<td>åŒ¿åç‰¹å¾ï¼ŒåŒ…å«v0-14åœ¨å†…15ä¸ªåŒ¿åç‰¹å¾</td>
</tr>
</tbody>
</table>
</div>


# 1ï¸âƒ£ æ•°æ®å¤„ç†
&emsp;&emsp;ä¸ºäº†åé¢å¤„ç†æ•°æ®æé«˜æ€§èƒ½ï¼Œæ‰€ä»¥éœ€è¦å¯¹å…¶è¿›è¡Œå†…å­˜ä¼˜åŒ–ã€‚
- å¯¼å…¥ç›¸å…³çš„åº“


```python
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
```

- é€šè¿‡è°ƒæ•´æ•°æ®ç±»å‹ï¼Œå¸®åŠ©æˆ‘ä»¬å‡å°‘æ•°æ®åœ¨å†…å­˜ä¸­å ç”¨çš„ç©ºé—´


```python
def reduce_mem_usage(df):
    """ è¿­ä»£dataframeçš„æ‰€æœ‰åˆ—ï¼Œä¿®æ”¹æ•°æ®ç±»å‹æ¥å‡å°‘å†…å­˜çš„å ç”¨        
    """
    start_mem = df.memory_usage().sum() 
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int': # åˆ¤æ–­å¯ä»¥ç”¨å“ªç§æ•´å‹å°±å¯ä»¥è¡¨ç¤ºï¼Œå°±è½¬æ¢åˆ°é‚£ä¸ªæ•´å‹å»
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            df[col] = df[col].astype('category')

    end_mem = df.memory_usage().sum() 
    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))
    return df
```


```python
sample_feature = reduce_mem_usage(pd.read_csv('../excel/data_for_tree.csv'))
```

    Memory usage of dataframe is 35249888.00 MB
    Memory usage after optimization is: 8925652.00 MB
    Decreased by 74.7%
    


```python
continuous_feature_names = [x for x in sample_feature.columns if x not in ['price','brand','model']]
```


```python
sample_feature.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>SaleID</th>
      <th>name</th>
      <th>model</th>
      <th>brand</th>
      <th>bodyType</th>
      <th>fuelType</th>
      <th>gearbox</th>
      <th>power</th>
      <th>kilometer</th>
      <th>notRepairedDamage</th>
      <th>...</th>
      <th>used_time</th>
      <th>city</th>
      <th>brand_amount</th>
      <th>brand_price_max</th>
      <th>brand_price_median</th>
      <th>brand_price_min</th>
      <th>brand_price_sum</th>
      <th>brand_price_std</th>
      <th>brand_price_average</th>
      <th>power_bin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1</td>
      <td>2262</td>
      <td>40.0</td>
      <td>1</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>15.0</td>
      <td>-</td>
      <td>...</td>
      <td>4756.0</td>
      <td>4.0</td>
      <td>4940.0</td>
      <td>9504.0</td>
      <td>3000.0</td>
      <td>149.0</td>
      <td>17934852.0</td>
      <td>2538.0</td>
      <td>3630.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <td>1</td>
      <td>5</td>
      <td>137642</td>
      <td>24.0</td>
      <td>10</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>109</td>
      <td>10.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>2482.0</td>
      <td>3.0</td>
      <td>3556.0</td>
      <td>9504.0</td>
      <td>2490.0</td>
      <td>200.0</td>
      <td>10936962.0</td>
      <td>2180.0</td>
      <td>3074.0</td>
      <td>10.0</td>
    </tr>
    <tr>
      <td>2</td>
      <td>7</td>
      <td>165346</td>
      <td>26.0</td>
      <td>14</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>101</td>
      <td>15.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>6108.0</td>
      <td>4.0</td>
      <td>8784.0</td>
      <td>9504.0</td>
      <td>1350.0</td>
      <td>13.0</td>
      <td>17445064.0</td>
      <td>1798.0</td>
      <td>1986.0</td>
      <td>10.0</td>
    </tr>
    <tr>
      <td>3</td>
      <td>10</td>
      <td>18961</td>
      <td>19.0</td>
      <td>9</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>101</td>
      <td>15.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>3874.0</td>
      <td>1.0</td>
      <td>4488.0</td>
      <td>9504.0</td>
      <td>1250.0</td>
      <td>55.0</td>
      <td>7867901.0</td>
      <td>1557.0</td>
      <td>1753.0</td>
      <td>10.0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>13</td>
      <td>8129</td>
      <td>65.0</td>
      <td>1</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>150</td>
      <td>15.0</td>
      <td>1.0</td>
      <td>...</td>
      <td>4152.0</td>
      <td>3.0</td>
      <td>4940.0</td>
      <td>9504.0</td>
      <td>3000.0</td>
      <td>149.0</td>
      <td>17934852.0</td>
      <td>2538.0</td>
      <td>3630.0</td>
      <td>14.0</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 39 columns</p>
</div>




```python
continuous_feature_names
```




    ['SaleID',
     'name',
     'bodyType',
     'fuelType',
     'gearbox',
     'power',
     'kilometer',
     'notRepairedDamage',
     'seller',
     'offerType',
     'v_0',
     'v_1',
     'v_2',
     'v_3',
     'v_4',
     'v_5',
     'v_6',
     'v_7',
     'v_8',
     'v_9',
     'v_10',
     'v_11',
     'v_12',
     'v_13',
     'v_14',
     'train',
     'used_time',
     'city',
     'brand_amount',
     'brand_price_max',
     'brand_price_median',
     'brand_price_min',
     'brand_price_sum',
     'brand_price_std',
     'brand_price_average',
     'power_bin']



# 2ï¸âƒ£ çº¿æ€§å›å½’
## 2ï¸âƒ£.1ï¸âƒ£ ç®€å•å»ºæ¨¡
&emsp;&emsp;è®¾ç½®è®­ç»ƒé›†çš„è‡ªå˜é‡`train_X`ä¸å› å˜é‡`train_y`


```python
sample_feature = sample_feature.dropna().replace('-', 0).reset_index(drop=True)
sample_feature['notRepairedDamage'] = sample_feature['notRepairedDamage'].astype(np.float32)

train = sample_feature[continuous_feature_names + ['price']]
train_X = train[continuous_feature_names]
train_y = train['price']
```

- ä»`sklearn.linear_model`åº“è°ƒç”¨çº¿æ€§å›å½’å‡½æ•°


```python
from sklearn.linear_model import LinearRegression
```

è®­ç»ƒæ¨¡å‹ï¼Œ`normalize`è®¾ç½®ä¸º`True`åˆ™è¾“å…¥çš„æ ·æœ¬æ•°æ®å°†$$\frac{(X-X_{ave})}{||X||}$$


```python
model = LinearRegression(normalize=True)
model = model.fit(train_X, train_y)
```

æŸ¥çœ‹è®­ç»ƒçš„çº¿æ€§å›å½’æ¨¡å‹çš„æˆªè·ï¼ˆinterceptï¼‰ä¸æƒé‡(coef)ï¼Œå…¶ä¸­`zip`å…ˆå°†ç‰¹å¾ä¸æƒé‡æ‹¼æˆå…ƒç»„ï¼Œå†ç”¨`dict.items()`å°†å…ƒç»„å˜æˆåˆ—è¡¨ï¼Œ`lambda`é‡Œé¢å–å…ƒç»„çš„ç¬¬2ä¸ªå…ƒç´ ï¼Œä¹Ÿå°±æ˜¯æŒ‰ç…§æƒé‡æ’åºã€‚


```python
print('intercept:'+ str(model.intercept_))

sorted(dict(zip(continuous_feature_names, model.coef_)).items(), key=lambda x:x[1], reverse=True)
```

    intercept:-74792.9734982533
    




    [('v_6', 1409712.605060366),
     ('v_8', 610234.5713666412),
     ('v_2', 14000.150601494915),
     ('v_10', 11566.15879987477),
     ('v_7', 4359.400479384727),
     ('v_3', 734.1594753553514),
     ('v_13', 429.31597053081543),
     ('v_14', 113.51097451363385),
     ('bodyType', 53.59225499923475),
     ('fuelType', 28.70033988480179),
     ('power', 14.063521207625223),
     ('city', 11.214497244626225),
     ('brand_price_std', 0.26064581249034796),
     ('brand_price_median', 0.2236946027016186),
     ('brand_price_min', 0.14223892840381142),
     ('brand_price_max', 0.06288317241689621),
     ('brand_amount', 0.031481415743174694),
     ('name', 2.866003063271253e-05),
     ('SaleID', 1.5357186544049832e-05),
     ('gearbox', 8.527422323822975e-07),
     ('train', -3.026798367500305e-08),
     ('offerType', -2.0873267203569412e-07),
     ('seller', -8.426140993833542e-07),
     ('brand_price_sum', -4.1644253886318015e-06),
     ('brand_price_average', -0.10601622599106471),
     ('used_time', -0.11019174518618283),
     ('power_bin', -64.74445582883024),
     ('kilometer', -122.96508938774225),
     ('v_0', -317.8572907738245),
     ('notRepairedDamage', -412.1984812088826),
     ('v_4', -1239.4804712396635),
     ('v_1', -2389.3641453624136),
     ('v_12', -12326.513672033445),
     ('v_11', -16921.982011390297),
     ('v_5', -25554.951071390704),
     ('v_9', -26077.95662717417)]



## 2ï¸âƒ£.2ï¸âƒ£ å¤„ç†é•¿å°¾åˆ†å¸ƒ
&emsp;&emsp;é•¿å°¾åˆ†å¸ƒæ˜¯å°¾å·´å¾ˆé•¿çš„åˆ†å¸ƒã€‚é‚£ä¹ˆå°¾å·´å¾ˆé•¿å¾ˆåšçš„åˆ†å¸ƒæœ‰ä»€ä¹ˆç‰¹æ®Šçš„å‘¢ï¼Ÿæœ‰ä¸¤æ–¹é¢ï¼šä¸€æ–¹é¢ï¼Œè¿™ç§åˆ†å¸ƒä¼šä½¿å¾—ä½ çš„é‡‡æ ·ä¸å‡†ï¼Œä¼°å€¼ä¸å‡†ï¼Œå› ä¸ºå°¾éƒ¨å äº†å¾ˆå¤§éƒ¨åˆ†ã€‚å¦ä¸€æ–¹é¢ï¼Œå°¾éƒ¨çš„æ•°æ®å°‘ï¼Œäººä»¬å¯¹å®ƒçš„äº†è§£å°±å°‘ï¼Œé‚£ä¹ˆå¦‚æœå®ƒæ˜¯æœ‰å®³çš„ï¼Œé‚£ä¹ˆå®ƒçš„ç ´ååŠ›å°±éå¸¸å¤§ï¼Œå› ä¸ºäººä»¬å¯¹å®ƒçš„é¢„é˜²æªæ–½å’Œç»éªŒæ¯”è¾ƒå°‘ã€‚å®é™…ä¸Šï¼Œåœ¨ç¨³å®šåˆ†å¸ƒå®¶æ—ä¸­ï¼Œé™¤äº†æ­£æ€åˆ†å¸ƒï¼Œå…¶ä»–å‡ä¸ºé•¿å°¾åˆ†å¸ƒã€‚

![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly93aWtpLm1iYWxpYi5jb20vdy9pbWFnZXMvNC80Yi8lRTklOTUlQkYlRTUlQjAlQkUlRTclOTAlODYlRTglQUUlQkEuZ2lm)


éšæœºæ‰¾ä¸ªç‰¹å¾ï¼Œç”¨éšæœºä¸‹æ ‡é€‰å–ä¸€å®šçš„æ•°è§‚æµ‹é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å·®åˆ«


```python
from matplotlib import pyplot as plt
subsample_index = np.random.randint(low=0, high=len(train_y), size=50)

plt.scatter(train_X['v_6'][subsample_index], train_y[subsample_index], color='black')
plt.scatter(train_X['v_6'][subsample_index], model.predict(train_X.loc[subsample_index]), color='red')
plt.xlabel('v_6')
plt.ylabel('price')
plt.legend(['True Price','Predicted Price'],loc='upper right')
print('çœŸå®ä»·æ ¼ä¸é¢„æµ‹ä»·æ ¼å·®è·è¿‡å¤§ï¼')
plt.show()
```

    çœŸå®ä»·æ ¼ä¸é¢„æµ‹ä»·æ ¼å·®è·è¿‡å¤§ï¼
    


    <Figure size 640x480 with 1 Axes>


ç»˜åˆ¶ç‰¹å¾`v_6`çš„å€¼ä¸æ ‡ç­¾çš„æ•£ç‚¹å›¾ï¼Œå›¾ç‰‡å‘ç°æ¨¡å‹çš„é¢„æµ‹ç»“æœï¼ˆçº¢è‰²ç‚¹ï¼‰ä¸çœŸå®æ ‡ç­¾ï¼ˆé»‘è‰²ç‚¹ï¼‰çš„åˆ†å¸ƒå·®å¼‚è¾ƒå¤§ï¼Œä¸”éƒ¨åˆ†é¢„æµ‹å€¼å‡ºç°äº†å°äº0çš„æƒ…å†µï¼Œè¯´æ˜æˆ‘ä»¬çš„æ¨¡å‹å­˜åœ¨ä¸€äº›é—®é¢˜ã€‚
ä¸‹é¢å¯ä»¥é€šè¿‡ä½œå›¾æˆ‘ä»¬çœ‹çœ‹æ•°æ®çš„æ ‡ç­¾ï¼ˆ`price`ï¼‰çš„åˆ†å¸ƒæƒ…å†µ


```python
import seaborn as sns
plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
sns.distplot(train_y)
plt.subplot(1,2,2)
sns.distplot(train_y[train_y < np.quantile(train_y, 0.9)])# å»æ‰å°¾éƒ¨10%çš„æ•°å†ç”»ä¸€æ¬¡ï¼Œä¾ç„¶æ˜¯å‘ˆç°é•¿å°¾åˆ†å¸ƒ
```




    <matplotlib.axes._subplots.AxesSubplot at 0x210469a20f0>




![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200505184904716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0V4Y2FsaWJ1clVsaW1pdGVk,size_16,color_FFFFFF,t_70#pic_center)



ä»è¿™ä¸¤ä¸ªé¢‘ç‡åˆ†å¸ƒç›´æ–¹å›¾æ¥çœ‹ï¼Œ`price`å‘ˆç°é•¿å°¾åˆ†å¸ƒï¼Œä¸åˆ©äºæˆ‘ä»¬çš„å»ºæ¨¡é¢„æµ‹ï¼ŒåŸå› æ˜¯å¾ˆå¤šæ¨¡å‹éƒ½å‡è®¾æ•°æ®è¯¯å·®é¡¹ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼Œè€Œé•¿å°¾åˆ†å¸ƒçš„æ•°æ®è¿èƒŒäº†è¿™ä¸€å‡è®¾ã€‚

åœ¨è¿™é‡Œæˆ‘ä»¬å¯¹`train_y`è¿›è¡Œäº†$log(x+1)$å˜æ¢ï¼Œä½¿æ ‡ç­¾è´´è¿‘äºæ­£æ€åˆ†å¸ƒ


```python
train_y_ln = np.log(train_y + 1)
plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
sns.distplot(train_y_ln)
plt.subplot(1,2,2)
sns.distplot(train_y_ln[train_y_ln < np.quantile(train_y_ln, 0.9)])
```




    <matplotlib.axes._subplots.AxesSubplot at 0x21046aa7588>




![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200505184913297.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0V4Y2FsaWJ1clVsaW1pdGVk,size_16,color_FFFFFF,t_70#pic_center)


å¯ä»¥çœ‹å‡ºç»è¿‡å¯¹æ•°å¤„ç†åï¼Œé•¿å°¾åˆ†å¸ƒçš„æ•ˆæœå‡å¼±äº†ã€‚å†è¿›è¡Œä¸€æ¬¡çº¿æ€§å›å½’ï¼š


```python
model = model.fit(train_X, train_y_ln)

print('intercept:'+ str(model.intercept_))
sorted(dict(zip(continuous_feature_names, model.coef_)).items(), key=lambda x:x[1], reverse=True)
```

    intercept:22.237755141260187
    




    [('v_1', 5.669305855573455),
     ('v_5', 4.244663233260515),
     ('v_12', 1.2018270333465797),
     ('v_13', 1.1021805892566767),
     ('v_10', 0.9251453991435046),
     ('v_2', 0.8276319426702504),
     ('v_9', 0.6011701859510072),
     ('v_3', 0.4096252333799574),
     ('v_0', 0.08579322268709569),
     ('power_bin', 0.013581489882378468),
     ('bodyType', 0.007405158753814581),
     ('power', 0.0003639122482301998),
     ('brand_price_median', 0.0001295023112073966),
     ('brand_price_max', 5.681812615719255e-05),
     ('brand_price_std', 4.2637652140444604e-05),
     ('brand_price_sum', 2.215129563552113e-09),
     ('gearbox', 7.094911325111752e-10),
     ('seller', 2.715054847612919e-10),
     ('offerType', 1.0291500984749291e-10),
     ('train', -2.2282620193436742e-11),
     ('SaleID', -3.7349069125800904e-09),
     ('name', -6.100613320903764e-08),
     ('brand_amount', -1.63362003323235e-07),
     ('used_time', -2.9274637535648837e-05),
     ('brand_price_min', -2.97497751376125e-05),
     ('brand_price_average', -0.0001181124521449396),
     ('fuelType', -0.0018817210167693563),
     ('city', -0.003633315365347111),
     ('v_14', -0.02594698320698149),
     ('kilometer', -0.03327227857575015),
     ('notRepairedDamage', -0.27571086049472),
     ('v_4', -0.6724689959780609),
     ('v_7', -1.178076244244115),
     ('v_11', -1.3234586342526309),
     ('v_8', -83.08615946716786),
     ('v_6', -315.0380673447196)]



å†ä¸€æ¬¡ç”»å‡ºé¢„æµ‹ä¸çœŸå®å€¼çš„æ•£ç‚¹å¯¹æ¯”å›¾ï¼š


```python
plt.scatter(train_X['v_6'][subsample_index], train_y[subsample_index], color='black')
plt.scatter(train_X['v_6'][subsample_index], np.exp(model.predict(train_X.loc[subsample_index])), color='blue')
plt.xlabel('v_6')
plt.ylabel('price')
plt.legend(['True Price','Predicted Price'],loc='upper right')
plt.show()
```


![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200505184922666.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0V4Y2FsaWJ1clVsaW1pdGVk,size_16,color_FFFFFF,t_70#pic_center)



æ•ˆæœç¨å¾®å¥½äº†ä¸€ç‚¹ï¼Œä½†æ¯•ç«Ÿæ˜¯çº¿æ€§å›å½’ï¼Œæ‹Ÿåˆå¾—è¿˜æ˜¯ä¸å¤Ÿå¥½ã€‚

# 3ï¸âƒ£ äº”æŠ˜äº¤å‰éªŒè¯Â¶ï¼ˆ`cross_val_score`ï¼‰
&emsp;&emsp;åœ¨ä½¿ç”¨è®­ç»ƒé›†å¯¹å‚æ•°è¿›è¡Œè®­ç»ƒçš„æ—¶å€™ï¼Œç»å¸¸ä¼šå‘ç°äººä»¬é€šå¸¸ä¼šå°†ä¸€æ•´ä¸ªè®­ç»ƒé›†åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼ˆæ¯”å¦‚mnistæ‰‹å†™è®­ç»ƒé›†ï¼‰ã€‚ä¸€èˆ¬åˆ†ä¸ºï¼šè®­ç»ƒé›†ï¼ˆ`train_set`ï¼‰ï¼Œè¯„ä¼°é›†ï¼ˆ`valid_set`ï¼‰ï¼Œæµ‹è¯•é›†ï¼ˆ`test_set`ï¼‰è¿™ä¸‰ä¸ªéƒ¨åˆ†ã€‚è¿™å…¶å®æ˜¯ä¸ºäº†ä¿è¯è®­ç»ƒæ•ˆæœè€Œç‰¹æ„è®¾ç½®çš„ã€‚å…¶ä¸­æµ‹è¯•é›†å¾ˆå¥½ç†è§£ï¼Œå…¶å®å°±æ˜¯å®Œå…¨ä¸å‚ä¸è®­ç»ƒçš„æ•°æ®ï¼Œä»…ä»…ç”¨æ¥è§‚æµ‹æµ‹è¯•æ•ˆæœçš„æ•°æ®ã€‚è€Œè®­ç»ƒé›†å’Œè¯„ä¼°é›†åˆ™ç‰µæ¶‰åˆ°ä¸‹é¢çš„çŸ¥è¯†äº†ã€‚

&emsp;&emsp;å› ä¸ºåœ¨å®é™…çš„è®­ç»ƒä¸­ï¼Œè®­ç»ƒçš„ç»“æœå¯¹äºè®­ç»ƒé›†çš„æ‹Ÿåˆç¨‹åº¦é€šå¸¸è¿˜æ˜¯æŒºå¥½çš„ï¼ˆåˆå§‹æ¡ä»¶æ•æ„Ÿï¼‰ï¼Œä½†æ˜¯å¯¹äºè®­ç»ƒé›†ä¹‹å¤–çš„æ•°æ®çš„æ‹Ÿåˆç¨‹åº¦é€šå¸¸å°±ä¸é‚£ä¹ˆä»¤äººæ»¡æ„äº†ã€‚å› æ­¤æˆ‘ä»¬é€šå¸¸å¹¶ä¸ä¼šæŠŠæ‰€æœ‰çš„æ•°æ®é›†éƒ½æ‹¿æ¥è®­ç»ƒï¼Œè€Œæ˜¯åˆ†å‡ºä¸€éƒ¨åˆ†æ¥ï¼ˆè¿™ä¸€éƒ¨åˆ†ä¸å‚åŠ è®­ç»ƒï¼‰å¯¹è®­ç»ƒé›†ç”Ÿæˆçš„å‚æ•°è¿›è¡Œæµ‹è¯•ï¼Œç›¸å¯¹å®¢è§‚çš„åˆ¤æ–­è¿™äº›å‚æ•°å¯¹è®­ç»ƒé›†ä¹‹å¤–çš„æ•°æ®çš„ç¬¦åˆç¨‹åº¦ã€‚è¿™ç§æ€æƒ³å°±ç§°ä¸ºäº¤å‰éªŒè¯ï¼ˆ`Cross Validation`ï¼‰ã€‚

&emsp;&emsp;ç›´è§‚çš„ç±»æ¯”å°±æ˜¯è®­ç»ƒé›†æ˜¯ä¸Šè¯¾ï¼Œè¯„ä¼°é›†æ˜¯å¹³æ—¶çš„ä½œä¸šï¼Œè€Œæµ‹è¯•é›†æ˜¯æœ€åçš„æœŸæœ«è€ƒè¯•ã€‚ğŸ˜

`Cross Validation`ï¼šç®€è¨€ä¹‹ï¼Œå°±æ˜¯è¿›è¡Œå¤šæ¬¡`train_test_split`åˆ’åˆ†ï¼›æ¯æ¬¡åˆ’åˆ†æ—¶ï¼Œåœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€æµ‹è¯•è¯„ä¼°ï¼Œä»è€Œå¾—å‡ºä¸€ä¸ªè¯„ä»·ç»“æœï¼›å¦‚æœæ˜¯5æŠ˜äº¤å‰éªŒè¯ï¼Œæ„æ€å°±æ˜¯åœ¨åŸå§‹æ•°æ®é›†ä¸Šï¼Œè¿›è¡Œ5æ¬¡åˆ’åˆ†ï¼Œæ¯æ¬¡åˆ’åˆ†è¿›è¡Œä¸€æ¬¡è®­ç»ƒã€è¯„ä¼°ï¼Œæœ€åå¾—åˆ°5æ¬¡åˆ’åˆ†åçš„è¯„ä¼°ç»“æœï¼Œä¸€èˆ¬åœ¨è¿™å‡ æ¬¡è¯„ä¼°ç»“æœä¸Šå–å¹³å‡å¾—åˆ°æœ€åçš„è¯„åˆ†ã€‚`k-fold cross-validation` ï¼Œå…¶ä¸­ï¼Œ`k`ä¸€èˆ¬å–5æˆ–10ã€‚

ä¸€èˆ¬æƒ…å†µå°†KæŠ˜äº¤å‰éªŒè¯ç”¨äºæ¨¡å‹è°ƒä¼˜ï¼Œæ‰¾åˆ°ä½¿å¾—æ¨¡å‹æ³›åŒ–æ€§èƒ½æœ€ä¼˜çš„è¶…å‚å€¼ã€‚æ‰¾åˆ°åï¼Œåœ¨å…¨éƒ¨è®­ç»ƒé›†ä¸Šé‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ç‹¬ç«‹æµ‹è¯•é›†å¯¹æ¨¡å‹æ€§èƒ½åšå‡ºæœ€ç»ˆè¯„ä»·ã€‚KæŠ˜äº¤å‰éªŒè¯ä½¿ç”¨äº†æ— é‡å¤æŠ½æ ·æŠ€æœ¯çš„å¥½å¤„ï¼šæ¯æ¬¡è¿­ä»£è¿‡ç¨‹ä¸­æ¯ä¸ªæ ·æœ¬ç‚¹åªæœ‰ä¸€æ¬¡è¢«åˆ’å…¥è®­ç»ƒé›†æˆ–æµ‹è¯•é›†çš„æœºä¼šã€‚

![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy85NjM3NzQyLWMzMzNmMWFkNDhhMDgxMWEucG5n?x-oss-process=image/format,png)
![](https://img-blog.csdn.net/20180205102310918?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvQ2hlblZhc3Q=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
![](https://img-blog.csdn.net/20180205102314995?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvQ2hlblZhc3Q=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


æ›´å¤šå‚è€ƒèµ„æ–™ï¼š[å‡ ç§äº¤å‰éªŒè¯ï¼ˆcross validationï¼‰æ–¹å¼çš„æ¯”è¾ƒ
](https://www.cnblogs.com/ysugyl/p/8707887.html)ã€[kæŠ˜äº¤å‰éªŒè¯](https://blog.csdn.net/tianguiyuyu/article/details/80697223)


- ä¸‹é¢è°ƒç”¨`sklearn.model_selection`çš„`cross_val_score`è¿›è¡Œäº¤å‰éªŒè¯


```python
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_absolute_error,  make_scorer
```

## 3ï¸âƒ£.1ï¸âƒ£ `cross_val_score`ç›¸åº”å‡½æ•°çš„åº”ç”¨


```python
def log_transfer(func):
    def wrapper(y, yhat):
        result = func(np.log(y), np.nan_to_num(np.log(yhat)))
        return result
    return wrapper
```

- ä¸Šé¢çš„`log_transfer`æ˜¯æä¾›è£…é¥°å™¨åŠŸèƒ½ï¼Œæ˜¯ä¸ºäº†å°†ä¸‹é¢çš„`cross_val_score`çš„`make_scorer`çš„`mean_absolute_error`ï¼ˆå®ƒçš„å…¬å¼åœ¨ä¸‹é¢ï¼‰çš„è¾“å…¥å‚æ•°åšå¯¹æ•°å¤„ç†ï¼Œå…¶ä¸­`np.nan_to_num`é¡ºä¾¿å°†`nan`è½¬å˜ä¸º0ã€‚
$$
MAE=\frac{\sum\limits_{i=1}^{n}\left|y_{i}-\hat{y}_{i}\right|}{n}
$$
- `cross_val_score`æ˜¯`sklearn`ç”¨äºäº¤å‰éªŒè¯è¯„ä¼°åˆ†æ•°çš„å‡½æ•°ï¼Œå‰é¢å‡ ä¸ªå‚æ•°å¾ˆæ˜æœ—ï¼Œåé¢å‡ ä¸ªå‚æ•°éœ€è¦è§£é‡Šä¸€ä¸‹ã€‚
    - `verbose`ï¼šè¯¦ç»†ç¨‹åº¦ï¼Œä¹Ÿå°±æ˜¯æ˜¯å¦è¾“å‡ºè¿›åº¦ä¿¡æ¯
    - `cv`ï¼šäº¤å‰éªŒè¯ç”Ÿæˆå™¨æˆ–å¯è¿­ä»£çš„æ¬¡æ•°
    - `scoring`ï¼šè°ƒç”¨ç”¨æ¥è¯„ä»·çš„æ–¹æ³•ï¼Œæ˜¯scoreè¶Šå¤§çº¦å¥½ï¼Œè¿˜æ˜¯lossè¶Šå°è¶Šå¥½ï¼Œé»˜è®¤æ˜¯lossã€‚è¿™é‡Œè°ƒç”¨äº†`mean_absolute_error`ï¼Œåªæ˜¯åœ¨è°ƒç”¨ä¹‹å‰å…ˆè¿›è¡Œäº†`log_transfer`çš„è£…é¥°ï¼Œç„¶åè°ƒç”¨çš„`y`å’Œ`yhat`ï¼Œä¼šè‡ªåŠ¨å°†`cross_val_score`å¾—åˆ°çš„`X`å’Œ`y`ä»£å…¥ã€‚
        - `make_scorer`ï¼šæ„å»ºä¸€ä¸ªå®Œæ•´çš„å®šåˆ¶scorerå‡½æ•°ï¼Œå¯é€‰å‚æ•°`greater_is_better`ï¼Œé»˜è®¤ä¸º`False`ï¼Œä¹Ÿå°±æ˜¯lossè¶Šå°è¶Šå¥½

- ä¸‹é¢æ˜¯å¯¹æœªè¿›è¡Œå¯¹æ•°å¤„ç†çš„åŸç‰¹å¾æ•°æ®è¿›è¡Œäº”æŠ˜äº¤å‰éªŒè¯


```python
scores = cross_val_score(model, X=train_X, y=train_y, verbose=1, cv = 5, scoring=make_scorer(log_transfer(mean_absolute_error)))
```

    [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
    [Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.2s finished
    


```python
print('AVG:', np.mean(scores))
```

    AVG: 0.7533845471636889
    


```python
scores = pd.DataFrame(scores.reshape(1,-1)) # è½¬åŒ–æˆä¸€è¡Œï¼Œ(-1,1)ä¸ºä¸€åˆ—
scores.columns = ['cv' + str(x) for x in range(1, 6)]
scores.index = ['MAE']
scores
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>cv1</th>
      <th>cv2</th>
      <th>cv3</th>
      <th>cv4</th>
      <th>cv5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>MAE</td>
      <td>0.727867</td>
      <td>0.759451</td>
      <td>0.781238</td>
      <td>0.750681</td>
      <td>0.747686</td>
    </tr>
  </tbody>
</table>
</div>



ä½¿ç”¨çº¿æ€§å›å½’æ¨¡å‹ï¼Œå¯¹è¿›è¡Œè¿‡å¯¹æ•°å¤„ç†çš„åŸç‰¹å¾æ•°æ®è¿›è¡Œäº”æŠ˜äº¤å‰éªŒè¯


```python
scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=1, cv = 5, scoring=make_scorer(mean_absolute_error))
```

    [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
    [Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.1s finished
    


```python
print('AVG:', np.mean(scores))
```

    AVG: 0.2124134663602803
    


```python
scores = pd.DataFrame(scores.reshape(1,-1))
scores.columns = ['cv' + str(x) for x in range(1, 6)]
scores.index = ['MAE']
scores
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>cv1</th>
      <th>cv2</th>
      <th>cv3</th>
      <th>cv4</th>
      <th>cv5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>MAE</td>
      <td>0.208238</td>
      <td>0.212408</td>
      <td>0.215933</td>
      <td>0.210742</td>
      <td>0.214747</td>
    </tr>
  </tbody>
</table>
</div>



å¯ä»¥çœ‹å‡ºè¿›è¡Œå¯¹æ•°å¤„ç†åï¼Œäº”æŠ˜äº¤å‰éªŒè¯çš„lossæ˜¾è‘—é™ä½ã€‚

## 3ï¸âƒ£.2ï¸âƒ£ è€ƒè™‘çœŸå®ä¸–ç•Œé™åˆ¶
&emsp;&emsp;ä¾‹å¦‚ï¼šé€šè¿‡2018å¹´çš„äºŒæ‰‹è½¦ä»·æ ¼é¢„æµ‹2017å¹´çš„äºŒæ‰‹è½¦ä»·æ ¼ï¼Œè¿™æ˜¾ç„¶æ˜¯ä¸åˆç†çš„ï¼Œå› æ­¤æˆ‘ä»¬è¿˜å¯ä»¥é‡‡ç”¨æ—¶é—´é¡ºåºå¯¹æ•°æ®é›†è¿›è¡Œåˆ†éš”ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬é€‰ç”¨é å‰æ—¶é—´çš„4/5æ ·æœ¬å½“ä½œè®­ç»ƒé›†ï¼Œé åæ—¶é—´çš„1/5å½“ä½œéªŒè¯é›†ï¼Œæœ€ç»ˆç»“æœä¸äº”æŠ˜äº¤å‰éªŒè¯å·®è·ä¸å¤§ã€‚


```python
import datetime
sample_feature = sample_feature.reset_index(drop=True)
split_point = len(sample_feature) // 5 * 4

train = sample_feature.loc[:split_point].dropna()
val = sample_feature.loc[split_point:].dropna()

train_X = train[continuous_feature_names]
train_y_ln = np.log(train['price'])
val_X = val[continuous_feature_names]
val_y_ln = np.log(val['price'])
```


```python
model = model.fit(train_X, train_y_ln)
```


```python
mean_absolute_error(val_y_ln, model.predict(val_X))
```




    0.21498301182417004



## 3ï¸âƒ£.3ï¸âƒ£ ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿ä¸éªŒè¯æ›²çº¿Â¶

&emsp;&emsp;å­¦ä¹ æ›²çº¿æ˜¯ä¸€ç§ç”¨æ¥åˆ¤æ–­è®­ç»ƒæ¨¡å‹çš„ä¸€ç§æ–¹æ³•ï¼Œå®ƒä¼šè‡ªåŠ¨æŠŠè®­ç»ƒæ ·æœ¬çš„æ•°é‡æŒ‰ç…§é¢„å®šçš„è§„åˆ™é€æ¸å¢åŠ ï¼Œç„¶åç”»å‡ºä¸åŒè®­ç»ƒæ ·æœ¬æ•°é‡æ—¶çš„æ¨¡å‹å‡†ç¡®åº¦ã€‚

&emsp;&emsp;æˆ‘ä»¬å¯ä»¥æŠŠ$J_{train}(\theta)$å’Œ$J_{test}(\theta)$ä½œä¸ºçºµåæ ‡ï¼Œç”»å‡ºä¸è®­ç»ƒé›†æ•°æ®é›†$m$çš„å¤§å°å…³ç³»ï¼Œè¿™å°±æ˜¯å­¦ä¹ æ›²çº¿ã€‚é€šè¿‡å­¦ä¹ æ›²çº¿ï¼Œå¯ä»¥ç›´è§‚åœ°è§‚å¯Ÿåˆ°æ¨¡å‹çš„å‡†ç¡®æ€§å’Œè®­ç»ƒæ•°æ®å¤§å°çš„å…³ç³»ã€‚ æˆ‘ä»¬å¯ä»¥æ¯”è¾ƒç›´è§‚çš„äº†è§£åˆ°æˆ‘ä»¬çš„æ¨¡å‹å¤„äºä¸€ä¸ªä»€ä¹ˆæ ·çš„çŠ¶æ€ï¼Œå¦‚ï¼šè¿‡æ‹Ÿåˆï¼ˆoverfittingï¼‰æˆ–æ¬ æ‹Ÿåˆï¼ˆunderfittingï¼‰

&emsp;&emsp;å¦‚æœæ•°æ®é›†çš„å¤§å°ä¸º$m$ï¼Œåˆ™é€šè¿‡ä¸‹é¢çš„æµç¨‹å³å¯ç”»å‡ºå­¦ä¹ æ›²çº¿ï¼š

- 1.æŠŠæ•°æ®é›†åˆ†æˆè®­ç»ƒæ•°æ®é›†å’Œäº¤å‰éªŒè¯é›†ï¼ˆå¯ä»¥çœ‹ä½œæµ‹è¯•é›†ï¼‰ï¼›

- 2.å–è®­ç»ƒæ•°æ®é›†çš„20%ä½œä¸ºè®­ç»ƒæ ·æœ¬ï¼Œè®­ç»ƒå‡ºæ¨¡å‹å‚æ•°ï¼›

- 3.ä½¿ç”¨äº¤å‰éªŒè¯é›†æ¥è®¡ç®—è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹çš„å‡†ç¡®æ€§ï¼›

- 4.ä»¥è®­ç»ƒé›†çš„scoreå’Œäº¤å‰éªŒè¯é›†scoreä¸ºçºµåæ ‡(è¿™é‡Œçš„scoreå–å†³äºä½ ä½¿ç”¨çš„`make_score`æ–¹æ³•ï¼Œä¾‹å¦‚MAE)ï¼Œè®­ç»ƒé›†çš„ä¸ªæ•°ä½œä¸ºæ¨ªåæ ‡ï¼Œåœ¨åæ ‡è½´ä¸Šç”»å‡ºä¸Šè¿°æ­¥éª¤è®¡ç®—å‡ºæ¥çš„æ¨¡å‹å‡†ç¡®æ€§ï¼›

- 5.è®­ç»ƒæ•°æ®é›†å¢åŠ 10%ï¼Œè°ƒåˆ°æ­¥éª¤2ï¼Œç»§ç»­æ‰§è¡Œï¼ŒçŸ¥é“è®­ç»ƒæ•°æ®é›†å¤§å°ä¸º100%ã€‚

`learning_curve()`ï¼šè¿™ä¸ªå‡½æ•°ä¸»è¦æ˜¯ç”¨æ¥åˆ¤æ–­ï¼ˆå¯è§†åŒ–ï¼‰æ¨¡å‹æ˜¯å¦è¿‡æ‹Ÿåˆçš„ã€‚ä¸‹é¢æ˜¯ä¸€äº›å‚æ•°çš„è§£é‡Šï¼š

- `X`ï¼šæ˜¯ä¸€ä¸ªm*nçš„çŸ©é˜µï¼Œm:æ•°æ®æ•°é‡ï¼Œn:ç‰¹å¾æ•°é‡ï¼›
- `y`ï¼šæ˜¯ä¸€ä¸ªm*1çš„çŸ©é˜µï¼Œm:æ•°æ®æ•°é‡ï¼Œç›¸å¯¹äº`X`çš„ç›®æ ‡è¿›è¡Œåˆ†ç±»æˆ–å›å½’ï¼›
- `groups`ï¼šå°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒ/æµ‹è¯•é›†æ—¶ä½¿ç”¨çš„æ ·æœ¬çš„æ ‡ç­¾åˆ†ç»„ã€‚**[å¯é€‰]**ï¼›
- `train_sizes`ï¼šæŒ‡å®šè®­ç»ƒæ ·å“æ•°é‡çš„å˜åŒ–è§„åˆ™ã€‚æ¯”å¦‚ï¼šnp.linspace(0.1, 1.0, 5)è¡¨ç¤ºæŠŠè®­ç»ƒæ ·å“æ•°é‡ä»0.1-1åˆ†æˆ5ç­‰åˆ†ï¼Œç”Ÿæˆ[0.1, 0.325,0.55,0.75,1]çš„åºåˆ—ï¼Œä»åºåˆ—ä¸­å–å‡ºè®­ç»ƒæ ·å“æ•°é‡ç™¾åˆ†æ¯”ï¼Œé€ä¸ªè®¡ç®—åœ¨å½“å‰è®­ç»ƒæ ·æœ¬æ•°é‡æƒ…å†µä¸‹è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹å‡†ç¡®æ€§ã€‚
- `cv`ï¼š`None`ï¼Œè¦ä½¿ç”¨é»˜è®¤çš„ä¸‰æŠ˜äº¤å‰éªŒè¯ï¼ˆv0.22ç‰ˆæœ¬ä¸­å°†æ”¹ä¸ºäº”æŠ˜ï¼‰ï¼›
- `n_jobs`ï¼šè¦å¹¶è¡Œè¿è¡Œçš„ä½œä¸šæ•°ã€‚Noneè¡¨ç¤º1ã€‚ -1è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰å¤„ç†å™¨ï¼›
- `pre_dispatch`ï¼šå¹¶è¡Œæ‰§è¡Œçš„é¢„è°ƒåº¦ä½œä¸šæ•°ï¼ˆé»˜è®¤ä¸ºå…¨éƒ¨ï¼‰ã€‚è¯¥é€‰é¡¹å¯ä»¥å‡å°‘åˆ†é…çš„å†…å­˜ã€‚è¯¥å­—ç¬¦ä¸²å¯ä»¥æ˜¯â€œ 2 * n_jobsâ€ä¹‹ç±»çš„è¡¨è¾¾å¼ï¼›
- `shuffle`ï¼š`bool`ï¼Œæ˜¯å¦åœ¨åŸºäº`train_sizes`ä¸ºå‰ç¼€ä¹‹å‰å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œæ´—ç‰Œï¼›


```python
from sklearn.model_selection import learning_curve, validation_curve
```

`plt.fill_between()`ç”¨æ¥å¡«å……ä¸¤æ¡çº¿é—´åŒºåŸŸï¼Œå…¶ä»–å¥½åƒæ²¡ä»€ä¹ˆå¥½è§£é‡Šçš„äº†ã€‚


```python
def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,n_jobs=1, train_size=np.linspace(.1, 1.0, 5 )):  
    plt.figure()  
    plt.title(title)  
    if ylim is not None:  
        plt.ylim(*ylim)  
    plt.xlabel('Training example')  
    plt.ylabel('score')  
    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_size, scoring = make_scorer(mean_absolute_error))  
    train_scores_mean = np.mean(train_scores, axis=1)  
    train_scores_std = np.std(train_scores, axis=1)  
    test_scores_mean = np.mean(test_scores, axis=1)  
    test_scores_std = np.std(test_scores, axis=1)  
    plt.grid()#åŒºåŸŸ  
    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,  
                     train_scores_mean + train_scores_std, alpha=0.1,  
                     color="r")  
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,  
                     test_scores_mean + test_scores_std, alpha=0.1,  
                     color="g")  
    plt.plot(train_sizes, train_scores_mean, 'o-', color='r',  
             label="Training score")  
    plt.plot(train_sizes, test_scores_mean,'o-',color="g",  
             label="Cross-validation score")  
    plt.legend(loc="best")  
    return plt  
```


```python
plot_learning_curve(LinearRegression(), 'Liner_model', train_X[:], train_y_ln[:], ylim=(0.0, 0.5), cv=5, n_jobs=-1)  
```




    <module 'matplotlib.pyplot' from 'D:\\Software\\Anaconda\\lib\\site-packages\\matplotlib\\pyplot.py'>




![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200505184932891.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0V4Y2FsaWJ1clVsaW1pdGVk,size_16,color_FFFFFF,t_70#pic_center)



è®­ç»ƒè¯¯å·®ä¸éªŒè¯è¯¯å·®é€æ¸ä¸€è‡´ï¼Œå‡†ç¡®ç‡ä¹ŸæŒºé«˜ï¼ˆè¿™é‡Œçš„scoreæ˜¯MAEï¼Œæ‰€ä»¥æ˜¯lossè¶‹è¿‘äº0.2ï¼Œå‡†ç¡®ç‡è¶‹è¿‘äº0.8ï¼‰ï¼Œä½†æ˜¯è®­ç»ƒè¯¯å·®å‡ ä¹æ²¡å˜è¿‡ï¼Œæ‰€ä»¥å±äºè¿‡æ‹Ÿåˆã€‚è¿™é‡Œç»™å‡ºä¸€ä¸‹é«˜åå·®æ¬ æ‹Ÿåˆ(bias)ä»¥åŠé«˜æ–¹å·®è¿‡æ‹Ÿåˆ(variance)çš„æ¨¡æ ·ï¼š

![](https://img-blog.csdn.net/20180909113753737?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NTIzODM5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMyLnpoaW1nLmNvbS84MC9hZmEwMzRkNTI5NjI2ODFkYjA5YjRkYzEwNjBmODA3NV83MjB3LmpwZw?x-oss-process=image/format,png)

æ›´å½¢è±¡ä¸€ç‚¹ï¼š

Dataï¼š

![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMyLnpoaW1nLmNvbS84MC9iZTBkNGQ0MzhjNGU0ODc1ZDUyYmEzZWFkZWFkNjhmM183MjB3LmpwZw?x-oss-process=image/format,png)

Normal fitting:

![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMzLnpoaW1nLmNvbS84MC9hMDkzNzEwZjk5MzY0MTg1NjYyYTRkZTFlZWFhMTU3M183MjB3LmpwZw?x-oss-process=image/format,png)

overfitting:

![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWM0LnpoaW1nLmNvbS84MC9hNDY1YTk4NjE3ODczMmI1YjdlMTc0YjAxODM2Zjg0Yl83MjB3LmpwZw?x-oss-process=image/format,png)

serious overfitting:

![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWM0LnpoaW1nLmNvbS84MC9mMmNiOWZkNzY4M2QzNWJmMjkwZDU3ZTJhZGMwMGYzMV83MjB3LmpwZw?x-oss-process=image/format,png)

# 4ï¸âƒ£ å¤šç§æ¨¡å‹å¯¹æ¯”


```python
train = sample_feature[continuous_feature_names + ['price']].dropna()

train_X = train[continuous_feature_names]
train_y = train['price']
train_y_ln = np.log(train_y + 1)
```

## 4ï¸âƒ£.1ï¸âƒ£ çº¿æ€§æ¨¡å‹ & åµŒå…¥å¼ç‰¹å¾é€‰æ‹©
&emsp;&emsp;æœ‰ä¸€äº›å‰å™çŸ¥è¯†éœ€è¦è¡¥å…¨ã€‚å…¶ä¸­å…³äºæ­£åˆ™åŒ–çš„çŸ¥è¯†ï¼š
- åˆ†åˆ«ä¸ºL1æ­£åˆ™åŒ–ä¸L2æ­£åˆ™åŒ–ï¼›
- L1æ­£åˆ™åŒ–çš„æ¨¡å‹å»ºå«åšLassoå›å½’ï¼Œä½¿ç”¨L2æ­£åˆ™åŒ–çš„æ¨¡å‹å«åšRidgeå›å½’ï¼ˆå²­å›å½’ï¼‰ï¼›
- L1æ­£åˆ™åŒ–æ˜¯æŒ‡æƒå€¼å‘é‡wä¸­å„ä¸ªå…ƒç´ çš„ç»å¯¹å€¼ä¹‹å’Œï¼Œé€šå¸¸è¡¨ç¤ºä¸º$\left \| w \right \| _{1} $ï¼›
- L2æ­£åˆ™åŒ–æ˜¯æŒ‡æƒå€¼å‘é‡wä¸­å„ä¸ªå…ƒç´ çš„å¹³æ–¹å’Œç„¶åå†æ±‚å¹³æ–¹æ ¹ï¼ˆå¯ä»¥çœ‹åˆ°Ridgeå›å½’çš„L2æ­£åˆ™åŒ–é¡¹æœ‰å¹³æ–¹ç¬¦å·ï¼‰ï¼Œé€šå¸¸è¡¨ç¤ºä¸º$\left \| w \right \| _{2} $
- L1æ­£åˆ™åŒ–å¯ä»¥äº§ç”Ÿç¨€ç–æƒå€¼çŸ©é˜µï¼Œå³äº§ç”Ÿä¸€ä¸ªç¨€ç–æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºç‰¹å¾é€‰æ‹©ï¼›
- L2æ­£åˆ™åŒ–å¯ä»¥é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆï¼ˆoverfittingï¼‰ï¼Œä¸€å®šç¨‹åº¦ä¸Šï¼ŒL1ä¹Ÿå¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼›

æ›´å¤šå…¶ä»–çŸ¥è¯†å¯ä»¥çœ‹è¿™ç¯‡æ–‡ç« ï¼š[æœºå™¨å­¦ä¹ ä¸­æ­£åˆ™åŒ–é¡¹L1å’ŒL2çš„ç›´è§‚ç†è§£](https://blog.csdn.net/jinping_shi/article/details/52433975)

&emsp;&emsp;åœ¨è¿‡æ»¤å¼å’ŒåŒ…è£¹å¼ç‰¹å¾é€‰æ‹©æ–¹æ³•ä¸­ï¼Œç‰¹å¾é€‰æ‹©è¿‡ç¨‹ä¸å­¦ä¹ å™¨è®­ç»ƒè¿‡ç¨‹æœ‰æ˜æ˜¾çš„åˆ†åˆ«ã€‚è€ŒåµŒå…¥å¼ç‰¹å¾é€‰æ‹©åœ¨å­¦ä¹ å™¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨åœ°è¿›è¡Œç‰¹å¾é€‰æ‹©ã€‚åµŒå…¥å¼é€‰æ‹©æœ€å¸¸ç”¨çš„æ˜¯L1æ­£åˆ™åŒ–ä¸L2æ­£åˆ™åŒ–ã€‚åœ¨å¯¹çº¿æ€§å›å½’æ¨¡å‹åŠ å…¥ä¸¤ç§æ­£åˆ™åŒ–æ–¹æ³•åï¼Œä»–ä»¬åˆ†åˆ«å˜æˆäº†å²­å›å½’ä¸Lassoå›å½’ã€‚

### 4ï¸âƒ£.1ï¸âƒ£.1ï¸âƒ£ `LinearRegression`ï¼Œ`Ridge`ï¼Œ`Lasso`æ–¹æ³•çš„è¿è¡Œ


```python
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
```


```python
models = [LinearRegression(),
          Ridge(),
          Lasso()]
```


```python
result = dict()
for model in models:
    model_name = str(model).split('(')[0]
    scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))
    result[model_name] = scores
    print(model_name + ' is finished')
```

    LinearRegression is finished
    Ridge is finished
    

    D:\Software\Anaconda\lib\site-packages\sklearn\linear_model\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.
      ConvergenceWarning)
    D:\Software\Anaconda\lib\site-packages\sklearn\linear_model\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.
      ConvergenceWarning)
    D:\Software\Anaconda\lib\site-packages\sklearn\linear_model\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.
      ConvergenceWarning)
    D:\Software\Anaconda\lib\site-packages\sklearn\linear_model\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.
      ConvergenceWarning)
    

    Lasso is finished
    

    D:\Software\Anaconda\lib\site-packages\sklearn\linear_model\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.
      ConvergenceWarning)
    

### 4ï¸âƒ£.1ï¸âƒ£.2ï¸âƒ£ ä¸‰ç§æ–¹æ³•çš„å¯¹æ¯”


```python
result = pd.DataFrame(result)
result.index = ['cv' + str(x) for x in range(1, 6)]
result
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>LinearRegression</th>
      <th>Ridge</th>
      <th>Lasso</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>cv1</td>
      <td>0.208238</td>
      <td>0.213319</td>
      <td>0.394868</td>
    </tr>
    <tr>
      <td>cv2</td>
      <td>0.212408</td>
      <td>0.216857</td>
      <td>0.387564</td>
    </tr>
    <tr>
      <td>cv3</td>
      <td>0.215933</td>
      <td>0.220840</td>
      <td>0.402278</td>
    </tr>
    <tr>
      <td>cv4</td>
      <td>0.210742</td>
      <td>0.215001</td>
      <td>0.396664</td>
    </tr>
    <tr>
      <td>cv5</td>
      <td>0.214747</td>
      <td>0.220031</td>
      <td>0.397400</td>
    </tr>
  </tbody>
</table>
</div>



1.çº¯`LinearRegression`æ–¹æ³•çš„æƒ…å†µï¼š`.intercept_`æ˜¯æˆªè·ï¼ˆä¸yè½´çš„äº¤ç‚¹ï¼‰å³$\theta_0$ï¼Œ`.coef_`æ˜¯æ¨¡å‹çš„æ–œç‡å³$\theta_1 - \theta_n$


```python
model = LinearRegression().fit(train_X, train_y_ln)
print('intercept:'+ str(model.intercept_)) # æˆªè·ï¼ˆä¸yè½´çš„äº¤ç‚¹ï¼‰
sns.barplot(abs(model.coef_), continuous_feature_names) 
```

    intercept:22.23769348625359
    




    <matplotlib.axes._subplots.AxesSubplot at 0x210418e4d68>




![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200505184953854.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0V4Y2FsaWJ1clVsaW1pdGVk,size_16,color_FFFFFF,t_70#pic_center)


çº¯`LinearRegression`å›å½’å¯ä»¥å‘ç°ï¼Œå¾—åˆ°çš„å‚æ•°åˆ—è¡¨æ˜¯æ¯”è¾ƒç¨€ç–çš„ã€‚


```python
model.coef_
```




    array([-3.73489972e-09, -6.10060860e-08,  7.40515349e-03, -1.88182450e-03,
           -1.24570527e-04,  3.63911807e-04, -3.32722751e-02, -2.75710825e-01,
           -1.43048695e-03, -3.28514719e-03,  8.57926933e-02,  5.66930260e+00,
            8.27635812e-01,  4.09620867e-01, -6.72467882e-01,  4.24497013e+00,
           -3.15038152e+02, -1.17801777e+00, -8.30861129e+01,  6.01215351e-01,
            9.25141289e-01, -1.32345773e+00,  1.20182089e+00,  1.10218030e+00,
           -2.59470516e-02,  8.88178420e-13, -2.92746484e-05, -3.63331132e-03,
           -1.63354329e-07,  5.68181101e-05,  1.29502381e-04, -2.97497182e-05,
            2.21512681e-09,  4.26377388e-05, -1.18112552e-04,  1.35814944e-02])



2.`Lasso`æ–¹æ³•å³L1æ­£åˆ™åŒ–çš„æƒ…å†µï¼š


```python
model = Lasso().fit(train_X, train_y_ln)
print('intercept:'+ str(model.intercept_))
sns.barplot(abs(model.coef_), continuous_feature_names)
```

    intercept:7.946156528722565
    

    D:\Software\Anaconda\lib\site-packages\sklearn\linear_model\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.
      ConvergenceWarning)
    




    <matplotlib.axes._subplots.AxesSubplot at 0x210405debe0>



![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200505185004284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0V4Y2FsaWJ1clVsaW1pdGVk,size_16,color_FFFFFF,t_70#pic_center)



L1æ­£åˆ™åŒ–æœ‰åŠ©äºç”Ÿæˆä¸€ä¸ªç¨€ç–æƒå€¼çŸ©é˜µï¼Œè¿›è€Œå¯ä»¥ç”¨äºç‰¹å¾é€‰æ‹©ã€‚å¦‚ä¸Šå›¾ï¼Œæˆ‘ä»¬å‘ç°powerä¸userd_timeç‰¹å¾éå¸¸é‡è¦ã€‚

3.`Ridge`æ–¹æ³•å³L2æ­£åˆ™åŒ–çš„æƒ…å†µï¼š


```python
model = Ridge().fit(train_X, train_y_ln)
print('intercept:'+ str(model.intercept_))
sns.barplot(abs(model.coef_), continuous_feature_names)
```

    intercept:2.7820015512913994
    




    <matplotlib.axes._subplots.AxesSubplot at 0x2103fdd99b0>




![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200505185010853.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0V4Y2FsaWJ1clVsaW1pdGVk,size_16,color_FFFFFF,t_70#pic_center)



ä»ä¸Šå›¾å¯ä»¥çœ‹åˆ°æœ‰å¾ˆå¤šå‚æ•°ç¦»0è¾ƒè¿œï¼Œå¾ˆå¤šä¸º0ã€‚

åŸå› åœ¨äºL2æ­£åˆ™åŒ–åœ¨æ‹Ÿåˆè¿‡ç¨‹ä¸­é€šå¸¸éƒ½å€¾å‘äºè®©æƒå€¼å°½å¯èƒ½å°ï¼Œæœ€åæ„é€ ä¸€ä¸ªæ‰€æœ‰å‚æ•°éƒ½æ¯”è¾ƒå°çš„æ¨¡å‹ã€‚å› ä¸ºä¸€èˆ¬è®¤ä¸ºå‚æ•°å€¼å°çš„æ¨¡å‹æ¯”è¾ƒç®€å•ï¼Œèƒ½é€‚åº”ä¸åŒçš„æ•°æ®é›†ï¼Œä¹Ÿåœ¨ä¸€å®šç¨‹åº¦ä¸Šé¿å…äº†è¿‡æ‹Ÿåˆç°è±¡ã€‚

å¯ä»¥è®¾æƒ³ä¸€ä¸‹å¯¹äºä¸€ä¸ªçº¿æ€§å›å½’æ–¹ç¨‹ï¼Œè‹¥å‚æ•°å¾ˆå¤§ï¼Œé‚£ä¹ˆåªè¦æ•°æ®åç§»ä¸€ç‚¹ç‚¹ï¼Œå°±ä¼šå¯¹ç»“æœé€ æˆå¾ˆå¤§çš„å½±å“ï¼›ä½†å¦‚æœå‚æ•°è¶³å¤Ÿå°ï¼Œæ•°æ®åç§»å¾—å¤šä¸€ç‚¹ä¹Ÿä¸ä¼šå¯¹ç»“æœé€ æˆä»€ä¹ˆå½±å“ï¼Œä¸“ä¸šä¸€ç‚¹çš„è¯´æ³•æ˜¯ã€æŠ—æ‰°åŠ¨èƒ½åŠ›å¼ºã€

é™¤æ­¤ä¹‹å¤–ï¼Œå†³ç­–æ ‘é€šè¿‡ä¿¡æ¯ç†µæˆ–GINIæŒ‡æ•°é€‰æ‹©åˆ†è£‚èŠ‚ç‚¹æ—¶ï¼Œä¼˜å…ˆé€‰æ‹©çš„åˆ†è£‚ç‰¹å¾ä¹Ÿæ›´åŠ é‡è¦ï¼Œè¿™åŒæ ·æ˜¯ä¸€ç§ç‰¹å¾é€‰æ‹©çš„æ–¹æ³•ã€‚XGBoostä¸LightGBMæ¨¡å‹ä¸­çš„model_importanceæŒ‡æ ‡æ­£æ˜¯åŸºäºæ­¤è®¡ç®—çš„

## 4ï¸âƒ£.2ï¸âƒ£ éçº¿æ€§æ¨¡å‹
&emsp;&emsp;æ”¯æŒå‘é‡æœºï¼Œå†³ç­–æ ‘ï¼Œéšæœºæ£®æ—ï¼Œæ¢¯åº¦æå‡æ ‘(GBDT)ï¼Œå¤šå±‚æ„ŸçŸ¥æœº(MLP)ï¼ŒXGBoostï¼ŒLightGBMç­‰


```python
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
from xgboost.sklearn import XGBRegressor
from lightgbm.sklearn import LGBMRegressor
```

å®šä¹‰æ¨¡å‹é›†åˆ


```python
models = [LinearRegression(),
          DecisionTreeRegressor(),
          RandomForestRegressor(),
          GradientBoostingRegressor(),
          MLPRegressor(solver='lbfgs', max_iter=100), 
          XGBRegressor(n_estimators = 100, objective='reg:squarederror'), 
          LGBMRegressor(n_estimators = 100)]
```

ç”¨æ•°æ®ä¸€ä¸€å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒ


```python
result = dict()
for model in models:
    model_name = str(model).split('(')[0]
    scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))
    result[model_name] = scores
    print(model_name + ' is finished')
```

    LinearRegression is finished
    DecisionTreeRegressor is finished
    RandomForestRegressor is finished
    GradientBoostingRegressor is finished
    MLPRegressor is finished
    XGBRegressor is finished
    LGBMRegressor is finished
    


```python
result = pd.DataFrame(result)
result.index = ['cv' + str(x) for x in range(1, 6)]
result
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>LinearRegression</th>
      <th>DecisionTreeRegressor</th>
      <th>RandomForestRegressor</th>
      <th>GradientBoostingRegressor</th>
      <th>MLPRegressor</th>
      <th>XGBRegressor</th>
      <th>LGBMRegressor</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>cv1</td>
      <td>0.208238</td>
      <td>0.224863</td>
      <td>0.163196</td>
      <td>0.179385</td>
      <td>581.596878</td>
      <td>0.155881</td>
      <td>0.153942</td>
    </tr>
    <tr>
      <td>cv2</td>
      <td>0.212408</td>
      <td>0.218795</td>
      <td>0.164292</td>
      <td>0.183759</td>
      <td>182.180288</td>
      <td>0.158566</td>
      <td>0.160262</td>
    </tr>
    <tr>
      <td>cv3</td>
      <td>0.215933</td>
      <td>0.216482</td>
      <td>0.164849</td>
      <td>0.185005</td>
      <td>250.668763</td>
      <td>0.158520</td>
      <td>0.159943</td>
    </tr>
    <tr>
      <td>cv4</td>
      <td>0.210742</td>
      <td>0.220903</td>
      <td>0.160878</td>
      <td>0.181660</td>
      <td>139.101476</td>
      <td>0.156608</td>
      <td>0.157528</td>
    </tr>
    <tr>
      <td>cv5</td>
      <td>0.214747</td>
      <td>0.226087</td>
      <td>0.164713</td>
      <td>0.183704</td>
      <td>108.664261</td>
      <td>0.173250</td>
      <td>0.157149</td>
    </tr>
  </tbody>
</table>
</div>



å¯ä»¥çœ‹åˆ°éšæœºæ£®æ—æ¨¡å‹åœ¨æ¯ä¸€ä¸ªfoldä¸­å‡å–å¾—äº†æ›´å¥½çš„æ•ˆæœ


```python
np.mean(result['RandomForestRegressor'])
```




    0.16358568277026037



## 4ï¸âƒ£.3ï¸âƒ£ æ¨¡å‹è°ƒå‚

&emsp;&emsp;ä¸‰ç§å¸¸ç”¨çš„è°ƒå‚æ–¹æ³•å¦‚ä¸‹ï¼š

è´ªå¿ƒç®—æ³• https://www.jianshu.com/p/ab89df9759c8<br>
ç½‘æ ¼è°ƒå‚ https://blog.csdn.net/weixin_43172660/article/details/83032029<br>
è´å¶æ–¯è°ƒå‚ https://blog.csdn.net/linxid/article/details/81189154<br>


```python
## LGBçš„å‚æ•°é›†åˆï¼š

objective = ['regression', 'regression_l1', 'mape', 'huber', 'fair']

num_leaves = [3,5,10,15,20,40, 55]
max_depth = [3,5,10,15,20,40, 55]
bagging_fraction = []
feature_fraction = []
drop_rate = []
```

### 4ï¸âƒ£.3ï¸âƒ£.1ï¸âƒ£ è´ªå¿ƒè°ƒå‚


```python
best_obj = dict()
for obj in objective:
    model = LGBMRegressor(objective=obj)
    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))
    best_obj[obj] = score
    
best_leaves = dict()
for leaves in num_leaves:
    model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0], num_leaves=leaves)
    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))
    best_leaves[leaves] = score
    
best_depth = dict()
for depth in max_depth:
    model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0],
                          num_leaves=min(best_leaves.items(), key=lambda x:x[1])[0],
                          max_depth=depth)
    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))
    best_depth[depth] = score
```


```python
sns.lineplot(x=['0_initial','1_turning_obj','2_turning_leaves','3_turning_depth'], y=[0.143 ,min(best_obj.values()), min(best_leaves.values()), min(best_depth.values())])
```




    <matplotlib.axes._subplots.AxesSubplot at 0x21041776128>




![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200505185018549.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0V4Y2FsaWJ1clVsaW1pdGVk,size_16,color_FFFFFF,t_70#pic_center)



### 4ï¸âƒ£.3ï¸âƒ£.2ï¸âƒ£ Grid Search ç½‘æ ¼è°ƒå‚


```python
from sklearn.model_selection import GridSearchCV
```


```python
parameters = {'objective': objective , 'num_leaves': num_leaves, 'max_depth': max_depth}
model = LGBMRegressor()
clf = GridSearchCV(model, parameters, cv=5)
clf = clf.fit(train_X, train_y)
```


```python
clf.best_params_
```




    {'max_depth': 10, 'num_leaves': 55, 'objective': 'regression'}




```python
model = LGBMRegressor(objective='regression',
                          num_leaves=55,
                          max_depth=10)
```


```python
np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))
```




    0.1526351038235066



### 4ï¸âƒ£.3ï¸âƒ£.3ï¸âƒ£ è´å¶æ–¯è°ƒå‚


```python
!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple bayesian-optimization
from bayes_opt import BayesianOptimization
```

    Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
    Collecting bayesian-optimization
      Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b5/26/9842333adbb8f17bcb3d699400a8b1ccde0af0b6de8d07224e183728acdf/bayesian_optimization-1.1.0-py3-none-any.whl
    Requirement already satisfied: scikit-learn>=0.18.0 in d:\software\anaconda\lib\site-packages (from bayesian-optimization) (0.20.3)
    Requirement already satisfied: scipy>=0.14.0 in d:\software\anaconda\lib\site-packages (from bayesian-optimization) (1.2.1)
    Requirement already satisfied: numpy>=1.9.0 in d:\software\anaconda\lib\site-packages (from bayesian-optimization) (1.16.2)
    Installing collected packages: bayesian-optimization
    Successfully installed bayesian-optimization-1.1.0
    


```python
def rf_cv(num_leaves, max_depth, subsample, min_child_samples):
    val = cross_val_score(
        LGBMRegressor(objective = 'regression_l1',
            num_leaves=int(num_leaves),
            max_depth=int(max_depth),
            subsample = subsample,
            min_child_samples = int(min_child_samples)
        ),
        X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)
    ).mean()
    return 1 - val # è´å¶æ–¯è°ƒå‚ç›®æ ‡æ˜¯æ±‚æœ€å¤§å€¼ï¼Œæ‰€ä»¥ç”¨1å‡å»è¯¯å·®
```


```python
rf_bo = BayesianOptimization(
    rf_cv,
    {
    'num_leaves': (2, 100),
    'max_depth': (2, 100),
    'subsample': (0.1, 1),
    'min_child_samples' : (2, 100)
    }
)
```


```python
rf_bo.maximize()
```

    |   iter    |  target   | max_depth | min_ch... | num_le... | subsample |
    -------------------------------------------------------------------------
    | [0m 1       [0m | [0m 0.8493  [0m | [0m 80.61   [0m | [0m 97.58   [0m | [0m 44.92   [0m | [0m 0.881   [0m |
    | [95m 2       [0m | [95m 0.8514  [0m | [95m 35.87   [0m | [95m 66.92   [0m | [95m 57.68   [0m | [95m 0.7878  [0m |
    | [95m 3       [0m | [95m 0.8522  [0m | [95m 49.75   [0m | [95m 68.95   [0m | [95m 64.99   [0m | [95m 0.1726  [0m |
    | [0m 4       [0m | [0m 0.8504  [0m | [0m 35.58   [0m | [0m 10.83   [0m | [0m 53.8    [0m | [0m 0.1306  [0m |
    | [0m 5       [0m | [0m 0.7942  [0m | [0m 63.37   [0m | [0m 32.21   [0m | [0m 3.143   [0m | [0m 0.4555  [0m |
    | [0m 6       [0m | [0m 0.7997  [0m | [0m 2.437   [0m | [0m 4.362   [0m | [0m 97.26   [0m | [0m 0.9957  [0m |
    | [95m 7       [0m | [95m 0.8526  [0m | [95m 47.85   [0m | [95m 69.39   [0m | [95m 68.02   [0m | [95m 0.8833  [0m |
    | [95m 8       [0m | [95m 0.8537  [0m | [95m 96.87   [0m | [95m 4.285   [0m | [95m 99.53   [0m | [95m 0.9389  [0m |
    | [95m 9       [0m | [95m 0.8546  [0m | [95m 96.06   [0m | [95m 97.85   [0m | [95m 98.82   [0m | [95m 0.8874  [0m |
    | [0m 10      [0m | [0m 0.7942  [0m | [0m 8.165   [0m | [0m 99.06   [0m | [0m 3.93    [0m | [0m 0.2049  [0m |
    | [0m 11      [0m | [0m 0.7993  [0m | [0m 2.77    [0m | [0m 99.47   [0m | [0m 91.16   [0m | [0m 0.2523  [0m |
    | [0m 12      [0m | [0m 0.852   [0m | [0m 99.3    [0m | [0m 43.04   [0m | [0m 62.67   [0m | [0m 0.9897  [0m |
    | [0m 13      [0m | [0m 0.8507  [0m | [0m 96.57   [0m | [0m 2.749   [0m | [0m 55.2    [0m | [0m 0.6727  [0m |
    | [0m 14      [0m | [0m 0.8168  [0m | [0m 3.076   [0m | [0m 3.269   [0m | [0m 33.78   [0m | [0m 0.5982  [0m |
    | [0m 15      [0m | [0m 0.8527  [0m | [0m 71.88   [0m | [0m 7.624   [0m | [0m 76.49   [0m | [0m 0.9536  [0m |
    | [0m 16      [0m | [0m 0.8528  [0m | [0m 99.44   [0m | [0m 99.28   [0m | [0m 69.58   [0m | [0m 0.7682  [0m |
    | [0m 17      [0m | [0m 0.8543  [0m | [0m 99.93   [0m | [0m 45.95   [0m | [0m 97.54   [0m | [0m 0.5095  [0m |
    | [0m 18      [0m | [0m 0.8518  [0m | [0m 60.87   [0m | [0m 99.67   [0m | [0m 61.3    [0m | [0m 0.7369  [0m |
    | [0m 19      [0m | [0m 0.8535  [0m | [0m 99.69   [0m | [0m 16.58   [0m | [0m 84.31   [0m | [0m 0.1025  [0m |
    | [0m 20      [0m | [0m 0.8507  [0m | [0m 54.68   [0m | [0m 38.11   [0m | [0m 54.65   [0m | [0m 0.9796  [0m |
    | [0m 21      [0m | [0m 0.8538  [0m | [0m 99.1    [0m | [0m 81.79   [0m | [0m 84.03   [0m | [0m 0.9823  [0m |
    | [0m 22      [0m | [0m 0.8529  [0m | [0m 99.28   [0m | [0m 3.373   [0m | [0m 83.48   [0m | [0m 0.7243  [0m |
    | [0m 23      [0m | [0m 0.8512  [0m | [0m 52.67   [0m | [0m 2.614   [0m | [0m 59.65   [0m | [0m 0.5286  [0m |
    | [95m 24      [0m | [95m 0.8546  [0m | [95m 75.81   [0m | [95m 61.62   [0m | [95m 99.78   [0m | [95m 0.9956  [0m |
    | [0m 25      [0m | [0m 0.853   [0m | [0m 45.9    [0m | [0m 33.68   [0m | [0m 74.59   [0m | [0m 0.73    [0m |
    | [0m 26      [0m | [0m 0.8532  [0m | [0m 82.58   [0m | [0m 63.9    [0m | [0m 78.61   [0m | [0m 0.1014  [0m |
    | [0m 27      [0m | [0m 0.8544  [0m | [0m 76.15   [0m | [0m 97.58   [0m | [0m 95.07   [0m | [0m 0.9995  [0m |
    | [0m 28      [0m | [0m 0.8545  [0m | [0m 95.75   [0m | [0m 74.96   [0m | [0m 99.45   [0m | [0m 0.7263  [0m |
    | [0m 29      [0m | [0m 0.8532  [0m | [0m 80.84   [0m | [0m 89.28   [0m | [0m 77.31   [0m | [0m 0.9389  [0m |
    | [0m 30      [0m | [0m 0.8545  [0m | [0m 82.92   [0m | [0m 35.46   [0m | [0m 96.66   [0m | [0m 0.969   [0m |
    =========================================================================
    


```python
rf_bo.max
```




    {'target': 0.8545792238909576,
     'params': {'max_depth': 75.80893509302794,
      'min_child_samples': 61.62267920507557,
      'num_leaves': 99.77501502667806,
      'subsample': 0.9955706357612557}}




```python
1 - rf_bo.max['target']
```




    0.14542077610904236



# 5ï¸âƒ£ æ€»ç»“
&emsp;&emsp;åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å®Œæˆäº†å»ºæ¨¡ä¸è°ƒå‚çš„å·¥ä½œï¼Œå¹¶å¯¹æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œäº†éªŒè¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨äº†ä¸€äº›åŸºæœ¬æ–¹æ³•æ¥æé«˜é¢„æµ‹çš„ç²¾åº¦ï¼Œæå‡å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚


```python
plt.figure(figsize=(13,5))
sns.lineplot(x=['0_origin','1_log_transfer','2_L1_&_L2','3_change_model','4_parameter_turning'], y=[1.36 ,0.19, 0.19, 0.16, 0.15])
```




    <matplotlib.axes._subplots.AxesSubplot at 0x21041688208>




![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200505185026643.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0V4Y2FsaWJ1clVsaW1pdGVk,size_16,color_FFFFFF,t_70#pic_center)

