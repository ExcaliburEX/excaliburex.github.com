<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[在本地VMware的Ubuntu，腾讯云CVM以及阿里云的ECS中搭建IPFS私有网络]]></title>
    <url>%2F%E5%9C%A8%E6%9C%AC%E5%9C%B0VMware%E7%9A%84Ubuntu%EF%BC%8C%E8%85%BE%E8%AE%AF%E4%BA%91CVM%E4%BB%A5%E5%8F%8A%E9%98%BF%E9%87%8C%E4%BA%91%E7%9A%84ECS%E4%B8%AD%E6%90%AD%E5%BB%BAIPFS%E7%A7%81%E6%9C%89%E7%BD%91%E7%BB%9C.html</url>
    <content type="text"><![CDATA[IPFS三部曲，之三。 0️⃣ 前言 连接腾讯云CVM，以及阿里云ECS可以用FinalShell或者Xshell，用Xshell的教程在这里保姆级教程——Xshell连接虚拟机中的Ubuntu并通过Xftp传输文件，连接本地Ubuntu和云端服务器步骤是一样的，只是ip输入的是公网ip。 在连接完毕后就可以进行后面的操作。 查看三个节点机器的IP地址： VMware中Ubuntu18.04的IP，即运行 1ifconfig 得到 这里的192.168.3.105便是。 腾讯云CVM 需要用到的就是这里的公网ip：129.211.103.82。 阿里云ECS 需要用到的就是这里的公网ip：47.96.189.80。 1️⃣ 安装IPFS具体请看这篇文章：一文完全解决——Ubuntu20.04下源码构建安装IPFS环境在最后运行： 1ipfs init 注意一下输出信息：也就是这里生成的.ipfs文件在什么位置，不记得可以再运行一遍ipfs init即可，这个位置后面要用到。 2️⃣ 生成共享Key 因为我们要组建的是私有网络，所有节点需要使用相同的私有key来加入网络中，我们使用go-ipfs-swarm-key-gen工具来生成共享key。我准备把本地的VMware的Ubuntu作为主运行节点，所以在这台Ubuntu上运行如下命令： 1234567#编译工具go get github.com/Kubuxu/go-ipfs-swarm-key-gen/ipfs-swarm-key-gencd $GOPATHcd src/github.com/Kubuxu/go-ipfs-swarm-key-gen/ipfs-swarm-key-gen/go build# 生成key./ipfs-swarm-key-gen > /home/excalibur/.ipfs/swarm.key 然后分别运行： 对腾讯云服务器： 12# 将本地生成的key拷贝到腾讯云服务器上的相同目录下scp /home/excalibur/.ipfs/swarm.key 192.168.3.105:/home/ubuntu/.ipfs/ 对阿里云服务器： 12# 将本地生成的key拷贝到阿里云服务器上的相同目录下scp /home/excalibur/.ipfs/swarm.key 47.96.189.80:/root/.ipfs/ 这里有三点需要注意： /home/excalibur/.ipfs/swarm.key这里面的/home/excalibur/.ipfs/是我的ipfs配置文件夹，你应该根据自己的位置修改，也就是之前提到的那个目录。 47.96.189.80:/root/.ipfs/，这里面前面的ip地址要根据你服务器的修改，并且后面的/root/.ipfs/也要根据你服务器上的ipfs文件夹修改，可以运行ipfs init进行查看。 如果遇到密码输入正确，然而出现Permissioned denied的情况，就输入su进入管理员模式，重新运行上面两个scp命令。 3️⃣ 移除默认的boostrap节点 因为要运行在私有网络上，不进入公网，必须删除其他启动节点信息。在三个节点上分别运行如下命令： 1ipfs bootstrap rm --all 4️⃣ 添加启动boostrap节点信息 这里以本地Ubuntu为启动节点，首先在本地节点运行如下命令： 1ipfs id 得到： 我们需要这里的hash值：QmTADgGT4MaCd3aTpD4vweGLQdWhr8oH8sue43DDioWBXA，然后再加上之前的本地节点的ip地址：192.168.3.105，就得到了所有需要的bootstrap信息，然后分别在两台云服务器上运行如下命令： 1ipfs bootstrap add /ip4/192.168.3.105/tcp/4001/ipfs/QmTADgGT4MaCd3aTpD4vweGLQdWhr8oH8sue43DDioWBXA 即可将本地节点作为它们的启动节点，自动加入ipfs网络。 5️⃣ 查看启动状态 分别在三个节点上运行： 1ipfs daemon 然后在任意节点上运行： 1ipfs swarm peers 将看到其他网络内节点的运行信息，我这里是在本地Ubuntu上运行的命令，可以看到腾讯云服务器的节点信息，但是阿里云不在😅。原因在于ECS的安全组设置：打开阿里云服务器设置，首先网络与安全组，然后安全组配置，然后配置规则，手动添加三个端口，分别是4001,5001，以及8080，最后ip地址可以是本地Ubuntu地址，或者直接设置成0.0.0.0/0。 也可以运行： 1ipfs stats bitswap 可以看到这里的partners字段为1，说明当前网络有2个节点。 6️⃣ 上传下载测试 在本地节点上传一个文件： 12echo helloworld > hello.txtipfs add hello.txt 得到： 可以在云服务器节点查看下载这个文件： 12ipfs cat QmUU2HcUBVSXkfWPUc3WUSeCMrWWeEJTuAgR9uyWBhh9Nfipfs get QmUU2HcUBVSXkfWPUc3WUSeCMrWWeEJTuAgR9uyWBhh9Nf 可见，就很纳爱斯！😁😁😁 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Ubuntu</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Linux</tag>
        <tag>Xshell</tag>
        <tag>虚拟机</tag>
        <tag>IPFS</tag>
        <tag>CVM</tag>
        <tag>ECS</tag>
        <tag>私有网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一文完全解决Ubuntu20.04下源码构建安装IPFS环境]]></title>
    <url>%2F%E4%B8%80%E6%96%87%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3Ubuntu20-04%E4%B8%8B%E6%BA%90%E7%A0%81%E6%9E%84%E5%BB%BA%E5%AE%89%E8%A3%85IPFS%E7%8E%AF%E5%A2%83.html</url>
    <content type="text"><![CDATA[IPFS三部曲，之二。 0⃣ 前言如果是新装的Ubuntu系统，运行sudo命令输入密码，可能会不成功，所以需要先运行： 1sudo passwd 重置密码，即可。 1⃣ 安装配置Go语言1⃣.1⃣ 下载GoIPFS是基于Go语言的项目，环境要求go version 1.14+。在Go的官方网站下载最新的版本即可https://golang.org/dl/。可以用以下命令： 12wget https://golang.org/dl/go1.14.6.linux-amd64.tar.gztar -C /usr/local -xzf go1.14.6.linux-amd64.tar.gz Tips: 如果wget失败可以到官网https://golang.org/dl/go1.14.6.linux-amd64.tar.gz下载镜像，然后在那个目录下打开终端执行上面的命令的第二句。 如果虚拟机下载失败，可以在主机中下载完成后，用Xftp连接虚拟机，将文件拖过去，至于如何连接，请看这篇文章保姆级教程——Xshell连接虚拟机中的Ubuntu并通过Xftp传输文件，Xshell和Xftp连接过程相同。1⃣.2⃣ 配置Go环境 在进入用户主目录，并新建名为go的文件夹，在go的文件夹中建立三个子目录(名字必须为src、pkg和bin)。创建目录过程如下: 12345678910cd ~mkdir gocd gosudo mkdir srcsudo mkdir pkgsudo mkdir binsudo chmod 777 srcsudo chmod 777 pkgsudo chmod 777 binls -l 配置环境变量，首先输入： 1vi ~/.profile 打开用户的环境变量，在最后添加如下内容： 1234export PATH=$PATH:/usr/local/go/bin export GOROOT=/usr/local/go export GOPATH=$HOME/go export PATH=$PATH:$HOME/go/bin 然后按Esc退出，接着输入:wq，然后输入回车就可以保存退出。 激活配置文件，即输入如下命令： 1source ~/.profile 最后验证一下是否成功，输入如下命令查看结果： 12go versiongo env 但是这样关掉终端，配置就会失效，所以在~/.bashrc中也设置一下： 1gedit ~/.bashrc 然后在最后面添加：1234export PATH=$PATH:/usr/local/go/bin export GOROOT=/usr/local/go export GOPATH=$HOME/go export PATH=$PATH:$HOME/go/bin 最后再：1source ~/.bashrc 2⃣ 安装配置IPFS环境2⃣.1⃣ 更新apt-get 并安装 git 在terminal执行以下语句: 12sudo apt-get updatesudo apt-get install git 2⃣.2⃣ 下载go-ipfs源码 因为go get国内基本上下载不了，加上镜像的话例如：123go env -w GO111MODULE=ongo env -w GOPROXY=https://goproxy.cn,directgo get -u github.com/ipfs/go-ipfs 虽然可以很快地下载，但却下载到了/go/pkg/mod/的目录下，感觉很难受，所以不推荐这种下载方法。 可以采取直接git clone的方法，但是如果直接clone的是github上的源码还是很慢，所以我采取的方法是，先将源码fork到自己的仓库，然后再导入到码云，然后再从码云上clone下来，速度简直快的飞起，可以直接用我的码云上的源码库，版本为ipfs 0.6.0，操作如下： 1234567cd ~cd go/srcmkdir github.comcd github.commkdir ipfscd ipfsgit clone https://gitee.com/ExcaliburAias/go-ipfs.git 当然，也不用非得clone到go/src/github.com/ipfs/go-ipfs下面，直接clone到桌面也可以。 2⃣.3⃣ 编译go-ipfs源码· 首先安装make工具，然后安装gcc，最后授予文件权限以及更改go get的源，操作如下： 12345678910cd ~cd go/src/github.com/ipfs/go-ipfssudo apt updatesudo apt install makesudo apt install build-essentialsudo chmod 777 /usr/local/go/binsudo chmod 777 /plugin/loader/preload.gogo env -w GO111MODULE=ongo env -w GOPROXY=https://goproxy.cn,directmake install · 测试： 1ipfs version 最后建议设置回去，也就是： 1go env -w GO111MODULE=off 最后，除此之外，也可以不用install，直接build，即生成的ipfs.exe不加入系统环境，而是生成在go/src/github.com/ipfs/go-ipfs/cmd/ipfs/ipfs.exe这里。实现方法就是将最后的： 1make install 改为1make build 3⃣ IPFS的初始化和连接 初始化IPFS节点： 1ipfs init 按提示输入 1ipfs cat /ipfs/QmQPeNsJPyVWPFDVHb77w8G42Fvo15z4bG2X8D2GhfbSXc/readme 查看已经存储的readme文件 启动守护进程并连接到IPFS网络：ipfs daemon可以在浏览器中输入：http://127.0.0.1:5001/webui 打开webui界面 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Ubuntu</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Linux</tag>
        <tag>Xshell</tag>
        <tag>虚拟机</tag>
        <tag>IPFS</tag>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[保姆级教程Xshell连接虚拟机中的Ubuntu并通过Xftp传输文件]]></title>
    <url>%2F%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%95%99%E7%A8%8BXshell%E8%BF%9E%E6%8E%A5%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B8%AD%E7%9A%84Ubuntu%E5%B9%B6%E9%80%9A%E8%BF%87Xftp%E4%BC%A0%E8%BE%93%E6%96%87%E4%BB%B6.html</url>
    <content type="text"><![CDATA[IPFS三部曲，之一。 1⃣ 虚拟机设置虚拟机——> 设置 ——>网络适配器——> 桥接模式 2⃣ 查看ip地址首先安装net-tools(如已安装忽略)，然后用ifconfig查看ip地址。命令如下： 12sudo apt install net-toolsifconfig 下图红框中就是我们后面需要的ip地址： 3⃣ 查看ssh状态首先检测ssh的状态： 1ps -e | grep ssh 没有看到sshd就说明未启动，选择下面的一种方式手动启动就好了: 12sudo service sshd startsudo /etc/init.d/ssh start 如果没安装，就直接安装，安装完毕会自动启动： 1sudo apt install openssh-server 4⃣ 启动Xshell进行连接 首先输入名称和前文的inet后面的ip地址： 然后在用户身份验证中，输入用户名和密码，用户名的获取方法是： 1who 我这里，就是：excalibur，密码就是sudo的密码。 设置完成后，直接连接即可。5⃣ 大功告成右键连接，用Xftp打开，可以通过Xftp进行文件传输：直接把左面的主机的文件往虚拟机里面拖就行了。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Ubuntu</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Linux</tag>
        <tag>Xshell</tag>
        <tag>Xftp</tag>
        <tag>虚拟机</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[地铁大小交路优化模型的遗传算法求解]]></title>
    <url>%2F%E5%9C%B0%E9%93%81%E5%A4%A7%E5%B0%8F%E4%BA%A4%E8%B7%AF%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E6%B1%82%E8%A7%A3.html</url>
    <content type="text"><![CDATA[学妹的请求，小试牛刀，一种尝试。 1️⃣ 问题陈述1️⃣.1️⃣ 需要解决的问题 目前地铁一般采用如下的单一交路：目前，我国绝大多数城市都采用这种交路形式，但是当断面客流量分布不均匀时容易造成线路运能浪费，客流拥挤。 替代方案就是用大小交路：使用遗传算法程序就是在既定的OD矩阵下找到最优的大小交路的往返站$S_a,S_b$以及相应的大小交路的发车频率$f_1,f_2$，也就是在遗传算法每次运行中，根据不同的大小交路折返站的设置，划分预定的OD出行矩阵，然后计算目标函数，判断是否达到最优。 1️⃣.2️⃣ 变量定义 $Q_1$——出行$O$点或$D$点位于小交路覆盖区段外出行以及$OD$均位于小交路覆盖区段外的客流量，人； $Q_2$——出行$OD$位于小交路覆盖区段的客流量，人； $t_{1d}、t_{2d}$——$Q_1、Q_2$对应乘客的平均候车时间，$s$; $q_{od}$——在车站$o$上车，在车站$d$下车的客流量，人； $i$——列车交路的集合，$i=\{1,2\}$，$1$代表大交路，$2$代表小交路； $f_i$——大小交路运行方式下的交路i的发车频率，对/小时； $f$——单一交路运行方式下的发车频率，对/小时； $f_{min}$——最小发车频率，设置为$12$对/小时； $T_{1周}、T_{2周}$——大小交路列车周转时间，$s$; $t_{运,j}$——列车在区间$j$的纯运行时间，$s$； $t_{停,j}$——列车在车站$h$的停站时间，$s$，设置为$30s$； $t_{折}$——列车在终点站、中间站的最小折返间隔时间，$s$，设置为$120s$; $C_z$——列车定员，即标准载客人数，人，设置为$1460$； $\alpha$——列车满载率上限，设置为100%； $I_o$——列车最小追踪间隔，$s$，设置为$120s$;1️⃣.3️⃣ 目标函数\min _{Z}=Q_{1} \cdot t_{1 d}+Q_{2} \cdot t_{2 d}其中Q_{1}=\sum_{d=1}^{n} \sum_{o=1}^{n} q_{o d}-Q_{2}\\Q_{2=} \sum_{d=o+1}^{b} \sum_{o=a}^{b-1} q_{o, d}+\sum_{d=a}^{o-1} \sum_{o=a+1}^{b} q_{o, d}t_{2 d}=\frac{1}{2} \cdot \frac{60}{f_{1}+f_{2}}\\ \\t_{1 d}=\frac{1}{2} \cdot \frac{60}{f} 1️⃣.4️⃣ 约束条件 列车数量 \left[\frac{T_{\text {周1}}}{60} \cdot f_{1}\right]+\left[\frac{T_{\text {周} 2}}{60} \cdot f_{2}\right] \leq\left[\frac{T_{\text {周} 1}}{60} \cdot f\right]其中 T_{\text {周} 1}=2 \cdot\left(\sum_{j=1}^{n-1} t_{\text {运}},_{j}+\sum_{h=1}^{n} t_{\text {停, } h}+\sum t_{\text {折}}\right)\\T_{\text {周} 2}=2 \cdot\left(\sum_{j=a}^{b-1} t_{\text {运}},_{j}+\sum_{h=a}^{b} t_{\text {停, } h}+\sum t^{'}_{\text {折}}\right) 满载率约束 \max _{j}\left(\sum_{d=j+1}^{n} \sum_{o=1}^{j} q_{o d} / \sum_{i=1}^{2} f_{i} \cdot \sum_{d=1}^{j} \sum_{o=j+1}^{n} q_{o d} / \sum_{i=1}^{2} f_{i}\right) \leq \alpha \cdot C_z 满足最小追踪间隔 f_{1}+f_{2} \leq \frac{3600}{I_{0}} 折返站折返能力 f_1\le\frac{3600}{t_{折}}\\f_2\le\frac{3600}{t_折} 满足最小发车频率 f_1>=f_{min} 其他约束 f_i\in N\\ 1\le a 35000 ; if sum(sum(N)) < 40000; xlswrite('test.xlsx',N,'Sheet1') endend 2️⃣.3️⃣ m_InitPop.m——初始化种群1234567891011function pop=m_InitPop(numpop,irange_l,irange_r)%% 初始化种群% 输入：numpop--种群大小；% [irange_l,irange_r]--初始种群所在的区间pop=[];for j = 1:numpop for i=1:4 % 因为a,b,f1,f2要求整数，所以生成随机整数 pop(i,j)= round(irange_l+(irange_r-irange_l)*rand); endend 2️⃣.4️⃣ m_Select.m——选择123456789101112131415161718function parentPop=m_Select(matrixFitness,pop,SELECTRATE)%% 选择% 输入：matrixFitness--适应度矩阵% pop--初始种群% SELECTRATE--选择率sumFitness=sum(matrixFitness(:));%计算所有种群的适应度accP=cumsum(matrixFitness/sumFitness);%累积概率%轮盘赌选择算法for n=1:round(SELECTRATE*size(pop,2)) matrix=find(accP>rand); %找到比随机数大的累积概率 if isempty(matrix) continue end parentPop(:,n)=pop(:,matrix(1));%将首个比随机数大的累积概率的位置的个体遗传下去endend 2️⃣.5️⃣ Crossover.m——交叉12345678910111213141516171819202122232425262728%% 子函数%%题 目：Crossover%%%%输 入：% parentsPop 上一代种群% NUMPOP 种群大小% CROSSOVERRATE 交叉率%输 出：% kidsPop 下一代种群%%% function kidsPop = Crossover(parentsPop,NUMPOP,CROSSOVERRATE)kidsPop = {[]};n = 1;while size(kidsPop,2) 0 fitness(n) = 1/1000000000; continue; end%% 4) 满载率约束 % constraint2 = [];% for j = 2:33% constraint2(j) = (sum(sum(OD(1:j, j+1:35)))/(f1+f2)) * (sum(sum(OD(j+1:35,1:j)))/(f1+f2));% end% if max(constraint2) > 1 * 1460% fitness(n) = 1/1000000000;% continue;% end%% 5) 最小追踪间隔 if f1 + f2 > 30 fitness(n) = 1/1000000000; continue; end %% 5) 最小发车间隔 if f1 < 12 fitness(n) = 1/1000000000; continue; end%% 主要适应度函数，设置为目标函数的倒数，即目标函数要求最小，那么越小，适应度就越大 fitness(n)= 1/m_Fx(pop(:,n), OD);end 2️⃣.🔟 m_Fx.m——目标函数(重要)12345function y=m_Fx(x, OD)%% 要求解的函数%% Z = Q1 * t1d + Q2 * t2d y = (sum(sum(OD)) - sum(sum(OD(x(1):x(2),x(1):x(2))))) * (30/x(3)) + sum(sum(OD(x(1):x(2),x(1):x(2)))) * (30/(x(3)+x(4)));end 3️⃣ 运行结果123最优解：71335.4762分钟最优解对应的各参数：4,32,14,4最大适应度：1.4018e-05 即设置第$4$和第$32$个站点为大小交路折返站，大交路发车频率为$14$列/小时，小交路发车频率为$4$列/小时，最低平均等待时间为$71335$分钟。 图像结果： document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>MATLAB</tag>
        <tag>遗传算法</tag>
        <tag>数学建模</tag>
        <tag>地铁线路优化</tag>
        <tag>目标函数</tag>
        <tag>约束条件</tag>
        <tag>二进制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单地打造一个搜索工具--爬取所有网页并创造单词的反向索引]]></title>
    <url>%2F%E7%AE%80%E5%8D%95%E5%9C%B0%E6%89%93%E9%80%A0%E4%B8%80%E4%B8%AA%E6%90%9C%E7%B4%A2%E5%B7%A5%E5%85%B7-%E7%88%AC%E5%8F%96%E6%89%80%E6%9C%89%E7%BD%91%E9%A1%B5%E5%B9%B6%E5%88%9B%E9%80%A0%E5%8D%95%E8%AF%8D%E7%9A%84%E5%8F%8D%E5%90%91%E7%B4%A2%E5%BC%95.html</url>
    <content type="text"><![CDATA[接单，简单的爬虫。 1️⃣ 任务要求 爬取既定网站的所有网页，本次爬取的是http://example.webscraping.com/，该网站包含所有国家的相关信息； 为每次单词创造包含出现位置以及词频的反向索引； 为用户提供$print$，$find$等命令。 2️⃣ 命令功能 $build$ 此命令指示搜索工具对网站进行爬取、生成反向索引并保存结果。索引存到文件系统中。为了简单起见，将整个索引保存在一个文件中。 $load$ 此命令从文件系统加载索引。显然，这个命令只有在，索引以前是使用$build$命令创建的。 $print$ 此命令打印一个单词的反向索引信息，例如： $print\quad Peso$ $find$ 此命令用于在反向索引中查找某个查询短语，并返回所有查询短语的列表，包含此短语的页面，例如： $find \quad Dinar$，返回所有包含Dinar的页面； $find \quad Area \quad Afghanistan$，返回所有同时包含这两个单词的网页列表。3️⃣ 具体程序 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171import timeimport reimport osimport jsonimport requestsimport pandas as pdfrom bs4 import BeautifulSoupfrom alive_progress import alive_barurl_index_prefix = "http://example.webscraping.com/places/default/index/"url_info_prefix = "http://example.webscraping.com"inverted_index_dict = {}length = 252header = { 'Accept': "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9", 'Accept-Encoding': "gzip, deflate", 'Accept-Language': "zh-CN,zh;q=0.9,en;q=0.8", 'Cache-Control': "max-age=0", 'Cookie': "session_id_places=True; session_data_places=\"586ad5c755d830e432c6e80f8b9a822a:xLrqTGkuTTFaRdOtQTpde-UcgSMy7nwOrXyEeyRafNjWT8t7J\ bHjZGf1cYO6bcnIVhwOHVNpJiMnr32rtSCF2_RSOUfBX4gRmU09KTNfMczD2vc4aaloPAvNE6gLStboj-EBBnFkWhVP3uCd8woSyXnTQwYi39HKoujz4iX1tJA5O4dr7z3VCn22mvev_\ MZaNSW4TT1jTJUZoF_3hyqtoN8rTL_Mjpu02ACJscaG6lRfQmIOBZ-BloR7aT4s-it19e0JYkbpynKb-an8f72IRhiN-thhyXeYbo6SCX0LzAra6Il1zM4Zpw9GkQFU2yha", 'Host': "example.webscraping.com", 'Proxy-Connection': "keep-alive", 'Referer': "http://example.webscraping.com/places/default/index/1", 'Upgrade-Insecure-Requests': "1", 'User-Agent': "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36"}def SplitList2Words(WordList): words = [] for phrase in WordList: phrase = str(phrase) if " " in phrase: for word in phrase.split(" "): if word != '>': # del '>' from words words.append(word) elif "." in phrase: words.append(phrase.split(".")[1]) # del '.' from words elif "," in phrase: for word in phrase.split(","): # del ',' from words words.append(word) else: words.append(phrase) return wordsdef GenerateInvertedIndex(words, inverted_index_dict, url): for w in words: if w in inverted_index_dict.keys(): if url in inverted_index_dict[w].keys(): # add to the number of occurences inverted_index_dict[w][url] += 1 else: # add this page link to the word dict value inverted_index_dict[w][url] = 1 else: # did not encounter this word before inverted_index_dict[w] = {url: 1} js = json.dumps(inverted_index_dict) file = open('InvertedIndex.txt', 'w') file.write(js) file.close() return inverted_index_dictdef build(): inverted_index_dict = {} with alive_bar(length) as bar: for page in range(26): header['Referer'] = url_index_prefix while True: try: # Set timeout to 10 seconds r = requests.get(url_index_prefix + str(page), headers=header, timeout=10) break except: print("TimeOut Error, reconnecting...") time.sleep(2) soup_main = BeautifulSoup(r.text, 'lxml') main_word_list = [str(w.text).strip() for w in soup_main.find_all('a')] main_word_list.append(str(soup_main.h1.text).strip()) words = SplitList2Words(main_word_list) inverted_index_dict = GenerateInvertedIndex( words, inverted_index_dict, url_index_prefix + str(page)) url_suffix = re.findall(r'/places/default/view/[A-Za-z]+\S+[A-Za-z]+[-]+[0-9]+', r.text) print("Start to crawl page %d !" % (page)) time.sleep(2) for info_url in url_suffix: bar() info_word_list = [] crawl_url = url_info_prefix + info_url header['Referer'] = url_index_prefix + str(page) while True: try: r_info = requests.get(crawl_url, headers=header, timeout=10) break except: print("TimeOut Error, reconnecting...") time.sleep(2) soup_info = BeautifulSoup(r_info.text, 'lxml') info_word_list = [str(w.text).strip() for w in soup_info.find_all('a')] info_word_list.append(str(soup_info.h1.text).strip()) title_info = SplitList2Words(info_word_list) inverted_index_dict = GenerateInvertedIndex( title_info, inverted_index_dict, crawl_url) country = pd.read_html(r_info.text) country_title = [x.split(":")[0] for x in country[0][0]] country_info = [info for info in country[0][1]] inverted_index_dict = GenerateInvertedIndex( country_title, inverted_index_dict, crawl_url) country_info = SplitList2Words(country_info) inverted_index_dict = GenerateInvertedIndex( country_info, inverted_index_dict, crawl_url) print("Country \"%s\" was crawled！" % (country[0][1][4])) time.sleep(5) print("Finished crawling page %d" % (page))def load(): file = open('InvertedIndex.txt', 'r') js = file.read() dic = json.loads(js) file.close() return dicif __name__ == "__main__": while True: command = input() if command == 'build': build() elif command == 'load': if not os.path.exists('InvertedIndex.txt'): print("Can not find the Inverted Index File, please 'build' first!") else: inverted_index_dict = load() print("Load file 'InvertedIndex.txt' successfully!") elif command.split(" ")[0] == 'print': try: beautiful_format = json.dumps(inverted_index_dict[command.split(" ")[1]], indent=4, ensure_ascii=False) print(beautiful_format) except: print("The index \'%s\' doesn't exist!" % (command.split(" ")[1])) elif command.split(" ")[0] == 'find': try: if len(command.split(" ")) == 2: for i in list(inverted_index_dict[command.split(" ")[1]].keys()): print(i) except: print("The index \'%s\' doesn't exist!" % (command[5:])) else: try: url_list = [] for word in command.split(" ")[1:]: url_list.append(list(inverted_index_dict[word].keys())) for i in set(url_list[0]).intersection(*url_list[1:]): print(i) except: print("The intersection of \'%s\' doesn't exist!" % (command[5:])) else: print("Please input the right command!") document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Crawler</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>pandas</tag>
        <tag>requests</tag>
        <tag>反向索引</tag>
        <tag>BeautifulSoup</tag>
        <tag>alive_progress</tag>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天池_二手车价格预测_Task4_建模调参]]></title>
    <url>%2F%E5%A4%A9%E6%B1%A0-%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B-Task4-%E5%BB%BA%E6%A8%A1%E8%B0%83%E5%8F%82.html</url>
    <content type="text"><![CDATA[天池数据挖掘，组对学习系列，系统地各种算法的杂糅。 0️⃣ 前言 本章思维导图： 0️⃣.1️⃣ 赛题重述 这是一道来自于天池的新手练习题目，用数据分析、机器学习等手段进行 二手车售卖价格预测 的回归问题。赛题本身的思路清晰明了，即对给定的数据集进行分析探讨，然后设计模型运用数据进行训练，测试模型，最终给出选手的预测结果。前面我们已经进行过EDA分析在这里天池_二手车价格预测_Task1-2_赛题理解与数据分析 以及天池_二手车价格预测_Task3_特征工程 0️⃣.2️⃣ 数据集概述 赛题官方给出了来自Ebay Kleinanzeigen的二手车交易记录，总数据量超过40w，包含31列变量信息，其中15列为匿名变量，即v0至v15。并从中抽取15万条作为训练集，5万条作为测试集A，5万条作为测试集B，同时对name、model、brand和regionCode等信息进行脱敏。具体的数据表如下图： Field Description SaleID 交易ID，唯一编码 name 汽车交易名称，已脱敏 regDate 汽车注册日期，例如20160101，2016年01月01日 model 车型编码，已脱敏 brand 汽车品牌，已脱敏 bodyType 车身类型：豪华轿车：0，微型车：1，厢型车：2，大巴车：3，敞篷车：4，双门汽车：5，商务车：6，搅拌车：7 fuelType 燃油类型：汽油：0，柴油：1，液化石油气：2，天然气：3，混合动力：4，其他：5，电动：6 gearbox 变速箱：手动：0，自动：1 power 发动机功率：范围 [ 0, 600 ] kilometer 汽车已行驶公里，单位万km notRepairedDamage 汽车有尚未修复的损坏：是：0，否：1 regionCode 地区编码，已脱敏 seller 销售方：个体：0，非个体：1 offerType 报价类型：提供：0，请求：1 creatDate 汽车上线时间，即开始售卖时间 price 二手车交易价格（预测目标） v系列特征 匿名特征，包含v0-14在内15个匿名特征 1️⃣ 数据处理 为了后面处理数据提高性能，所以需要对其进行内存优化。 导入相关的库 1234import pandas as pdimport numpy as npimport warningswarnings.filterwarnings('ignore') 通过调整数据类型，帮助我们减少数据在内存中占用的空间 1234567891011121314151617181920212223242526272829303132333435def reduce_mem_usage(df): """ 迭代dataframe的所有列，修改数据类型来减少内存的占用 """ start_mem = df.memory_usage().sum() print('Memory usage of dataframe is {:.2f} MB'.format(start_mem)) for col in df.columns: col_type = df[col].dtype if col_type != object: c_min = df[col].min() c_max = df[col].max() if str(col_type)[:3] == 'int': # 判断可以用哪种整型就可以表示，就转换到那个整型去 if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max: df[col] = df[col].astype(np.int8) elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max: df[col] = df[col].astype(np.int16) elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max: df[col] = df[col].astype(np.int32) elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max: df[col] = df[col].astype(np.int64) else: if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max: df[col] = df[col].astype(np.float16) elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max: df[col] = df[col].astype(np.float32) else: df[col] = df[col].astype(np.float64) else: df[col] = df[col].astype('category') end_mem = df.memory_usage().sum() print('Memory usage after optimization is: {:.2f} MB'.format(end_mem)) print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem)) return df 1sample_feature = reduce_mem_usage(pd.read_csv('../excel/data_for_tree.csv')) Memory usage of dataframe is 35249888.00 MB Memory usage after optimization is: 8925652.00 MB Decreased by 74.7% 1continuous_feature_names = [x for x in sample_feature.columns if x not in ['price','brand','model']] 1sample_feature.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SaleID name model brand bodyType fuelType gearbox power kilometer notRepairedDamage ... used_time city brand_amount brand_price_max brand_price_median brand_price_min brand_price_sum brand_price_std brand_price_average power_bin 0 1 2262 40.0 1 2.0 0.0 0.0 0 15.0 - ... 4756.0 4.0 4940.0 9504.0 3000.0 149.0 17934852.0 2538.0 3630.0 NaN 1 5 137642 24.0 10 0.0 1.0 0.0 109 10.0 0.0 ... 2482.0 3.0 3556.0 9504.0 2490.0 200.0 10936962.0 2180.0 3074.0 10.0 2 7 165346 26.0 14 1.0 0.0 0.0 101 15.0 0.0 ... 6108.0 4.0 8784.0 9504.0 1350.0 13.0 17445064.0 1798.0 1986.0 10.0 3 10 18961 19.0 9 3.0 1.0 0.0 101 15.0 0.0 ... 3874.0 1.0 4488.0 9504.0 1250.0 55.0 7867901.0 1557.0 1753.0 10.0 4 13 8129 65.0 1 0.0 0.0 0.0 150 15.0 1.0 ... 4152.0 3.0 4940.0 9504.0 3000.0 149.0 17934852.0 2538.0 3630.0 14.0 5 rows × 39 columns 1continuous_feature_names ['SaleID', 'name', 'bodyType', 'fuelType', 'gearbox', 'power', 'kilometer', 'notRepairedDamage', 'seller', 'offerType', 'v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13', 'v_14', 'train', 'used_time', 'city', 'brand_amount', 'brand_price_max', 'brand_price_median', 'brand_price_min', 'brand_price_sum', 'brand_price_std', 'brand_price_average', 'power_bin'] 2️⃣ 线性回归2️⃣.1️⃣ 简单建模 设置训练集的自变量train_X与因变量train_y 123456sample_feature = sample_feature.dropna().replace('-', 0).reset_index(drop=True)sample_feature['notRepairedDamage'] = sample_feature['notRepairedDamage'].astype(np.float32)train = sample_feature[continuous_feature_names + ['price']]train_X = train[continuous_feature_names]train_y = train['price'] 从sklearn.linear_model库调用线性回归函数 1from sklearn.linear_model import LinearRegression 训练模型，normalize设置为True则输入的样本数据将\frac{(X-X_{ave})}{||X||} 12model = LinearRegression(normalize=True)model = model.fit(train_X, train_y) 查看训练的线性回归模型的截距（intercept）与权重(coef)，其中zip先将特征与权重拼成元组，再用dict.items()将元组变成列表，lambda里面取元组的第2个元素，也就是按照权重排序。 123print('intercept:'+ str(model.intercept_))sorted(dict(zip(continuous_feature_names, model.coef_)).items(), key=lambda x:x[1], reverse=True) intercept:-74792.9734982533 [('v_6', 1409712.605060366), ('v_8', 610234.5713666412), ('v_2', 14000.150601494915), ('v_10', 11566.15879987477), ('v_7', 4359.400479384727), ('v_3', 734.1594753553514), ('v_13', 429.31597053081543), ('v_14', 113.51097451363385), ('bodyType', 53.59225499923475), ('fuelType', 28.70033988480179), ('power', 14.063521207625223), ('city', 11.214497244626225), ('brand_price_std', 0.26064581249034796), ('brand_price_median', 0.2236946027016186), ('brand_price_min', 0.14223892840381142), ('brand_price_max', 0.06288317241689621), ('brand_amount', 0.031481415743174694), ('name', 2.866003063271253e-05), ('SaleID', 1.5357186544049832e-05), ('gearbox', 8.527422323822975e-07), ('train', -3.026798367500305e-08), ('offerType', -2.0873267203569412e-07), ('seller', -8.426140993833542e-07), ('brand_price_sum', -4.1644253886318015e-06), ('brand_price_average', -0.10601622599106471), ('used_time', -0.11019174518618283), ('power_bin', -64.74445582883024), ('kilometer', -122.96508938774225), ('v_0', -317.8572907738245), ('notRepairedDamage', -412.1984812088826), ('v_4', -1239.4804712396635), ('v_1', -2389.3641453624136), ('v_12', -12326.513672033445), ('v_11', -16921.982011390297), ('v_5', -25554.951071390704), ('v_9', -26077.95662717417)] 2️⃣.2️⃣ 处理长尾分布 长尾分布是尾巴很长的分布。那么尾巴很长很厚的分布有什么特殊的呢？有两方面：一方面，这种分布会使得你的采样不准，估值不准，因为尾部占了很大部分。另一方面，尾部的数据少，人们对它的了解就少，那么如果它是有害的，那么它的破坏力就非常大，因为人们对它的预防措施和经验比较少。实际上，在稳定分布家族中，除了正态分布，其他均为长尾分布。 随机找个特征，用随机下标选取一定的数观测预测值与真实值之间的差别 12345678910from matplotlib import pyplot as pltsubsample_index = np.random.randint(low=0, high=len(train_y), size=50)plt.scatter(train_X['v_6'][subsample_index], train_y[subsample_index], color='black')plt.scatter(train_X['v_6'][subsample_index], model.predict(train_X.loc[subsample_index]), color='red')plt.xlabel('v_6')plt.ylabel('price')plt.legend(['True Price','Predicted Price'],loc='upper right')print('真实价格与预测价格差距过大！')plt.show() 真实价格与预测价格差距过大！ 绘制特征v_6的值与标签的散点图，图片发现模型的预测结果（红色点）与真实标签（黑色点）的分布差异较大，且部分预测值出现了小于0的情况，说明我们的模型存在一些问题。下面可以通过作图我们看看数据的标签（price）的分布情况 123456import seaborn as snsplt.figure(figsize=(15,5))plt.subplot(1,2,1)sns.distplot(train_y)plt.subplot(1,2,2)sns.distplot(train_y[train_y < np.quantile(train_y, 0.9)])# 去掉尾部10%的数再画一次，依然是呈现长尾分布 从这两个频率分布直方图来看，price呈现长尾分布，不利于我们的建模预测，原因是很多模型都假设数据误差项符合正态分布，而长尾分布的数据违背了这一假设。 在这里我们对train_y进行了$log(x+1)$变换，使标签贴近于正态分布 123456train_y_ln = np.log(train_y + 1)plt.figure(figsize=(15,5))plt.subplot(1,2,1)sns.distplot(train_y_ln)plt.subplot(1,2,2)sns.distplot(train_y_ln[train_y_ln < np.quantile(train_y_ln, 0.9)]) 可以看出经过对数处理后，长尾分布的效果减弱了。再进行一次线性回归： 1234model = model.fit(train_X, train_y_ln)print('intercept:'+ str(model.intercept_))sorted(dict(zip(continuous_feature_names, model.coef_)).items(), key=lambda x:x[1], reverse=True) intercept:22.237755141260187 [('v_1', 5.669305855573455), ('v_5', 4.244663233260515), ('v_12', 1.2018270333465797), ('v_13', 1.1021805892566767), ('v_10', 0.9251453991435046), ('v_2', 0.8276319426702504), ('v_9', 0.6011701859510072), ('v_3', 0.4096252333799574), ('v_0', 0.08579322268709569), ('power_bin', 0.013581489882378468), ('bodyType', 0.007405158753814581), ('power', 0.0003639122482301998), ('brand_price_median', 0.0001295023112073966), ('brand_price_max', 5.681812615719255e-05), ('brand_price_std', 4.2637652140444604e-05), ('brand_price_sum', 2.215129563552113e-09), ('gearbox', 7.094911325111752e-10), ('seller', 2.715054847612919e-10), ('offerType', 1.0291500984749291e-10), ('train', -2.2282620193436742e-11), ('SaleID', -3.7349069125800904e-09), ('name', -6.100613320903764e-08), ('brand_amount', -1.63362003323235e-07), ('used_time', -2.9274637535648837e-05), ('brand_price_min', -2.97497751376125e-05), ('brand_price_average', -0.0001181124521449396), ('fuelType', -0.0018817210167693563), ('city', -0.003633315365347111), ('v_14', -0.02594698320698149), ('kilometer', -0.03327227857575015), ('notRepairedDamage', -0.27571086049472), ('v_4', -0.6724689959780609), ('v_7', -1.178076244244115), ('v_11', -1.3234586342526309), ('v_8', -83.08615946716786), ('v_6', -315.0380673447196)] 再一次画出预测与真实值的散点对比图： 123456plt.scatter(train_X['v_6'][subsample_index], train_y[subsample_index], color='black')plt.scatter(train_X['v_6'][subsample_index], np.exp(model.predict(train_X.loc[subsample_index])), color='blue')plt.xlabel('v_6')plt.ylabel('price')plt.legend(['True Price','Predicted Price'],loc='upper right')plt.show() 效果稍微好了一点，但毕竟是线性回归，拟合得还是不够好。 3️⃣ 五折交叉验证¶（cross_val_score） 在使用训练集对参数进行训练的时候，经常会发现人们通常会将一整个训练集分为三个部分（比如mnist手写训练集）。一般分为：训练集（train_set），评估集（valid_set），测试集（test_set）这三个部分。这其实是为了保证训练效果而特意设置的。其中测试集很好理解，其实就是完全不参与训练的数据，仅仅用来观测测试效果的数据。而训练集和评估集则牵涉到下面的知识了。 因为在实际的训练中，训练的结果对于训练集的拟合程度通常还是挺好的（初始条件敏感），但是对于训练集之外的数据的拟合程度通常就不那么令人满意了。因此我们通常并不会把所有的数据集都拿来训练，而是分出一部分来（这一部分不参加训练）对训练集生成的参数进行测试，相对客观的判断这些参数对训练集之外的数据的符合程度。这种思想就称为交叉验证（Cross Validation）。 直观的类比就是训练集是上课，评估集是平时的作业，而测试集是最后的期末考试。😏 Cross Validation：简言之，就是进行多次train_test_split划分；每次划分时，在不同的数据集上进行训练、测试评估，从而得出一个评价结果；如果是5折交叉验证，意思就是在原始数据集上，进行5次划分，每次划分进行一次训练、评估，最后得到5次划分后的评估结果，一般在这几次评估结果上取平均得到最后的评分。k-fold cross-validation ，其中，k一般取5或10。 一般情况将K折交叉验证用于模型调优，找到使得模型泛化性能最优的超参值。找到后，在全部训练集上重新训练模型，并使用独立测试集对模型性能做出最终评价。K折交叉验证使用了无重复抽样技术的好处：每次迭代过程中每个样本点只有一次被划入训练集或测试集的机会。 更多参考资料：几种交叉验证（cross validation）方式的比较 、k折交叉验证 下面调用sklearn.model_selection的cross_val_score进行交叉验证 12from sklearn.model_selection import cross_val_scorefrom sklearn.metrics import mean_absolute_error, make_scorer 3️⃣.1️⃣ cross_val_score相应函数的应用12345def log_transfer(func): def wrapper(y, yhat): result = func(np.log(y), np.nan_to_num(np.log(yhat))) return result return wrapper 上面的log_transfer是提供装饰器功能，是为了将下面的cross_val_score的make_scorer的mean_absolute_error（它的公式在下面）的输入参数做对数处理，其中np.nan_to_num顺便将nan转变为0。 MAE=\frac{\sum\limits_{i=1}^{n}\left|y_{i}-\hat{y}_{i}\right|}{n} cross_val_score是sklearn用于交叉验证评估分数的函数，前面几个参数很明朗，后面几个参数需要解释一下。 verbose：详细程度，也就是是否输出进度信息 cv：交叉验证生成器或可迭代的次数 scoring：调用用来评价的方法，是score越大约好，还是loss越小越好，默认是loss。这里调用了mean_absolute_error，只是在调用之前先进行了log_transfer的装饰，然后调用的y和yhat，会自动将cross_val_score得到的X和y代入。 make_scorer：构建一个完整的定制scorer函数，可选参数greater_is_better，默认为False，也就是loss越小越好 下面是对未进行对数处理的原特征数据进行五折交叉验证 1scores = cross_val_score(model, X=train_X, y=train_y, verbose=1, cv = 5, scoring=make_scorer(log_transfer(mean_absolute_error))) [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=1)]: Done 5 out of 5 | elapsed: 0.2s finished 1print('AVG:', np.mean(scores)) AVG: 0.7533845471636889 1234scores = pd.DataFrame(scores.reshape(1,-1)) # 转化成一行，(-1,1)为一列scores.columns = ['cv' + str(x) for x in range(1, 6)]scores.index = ['MAE']scores .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cv1 cv2 cv3 cv4 cv5 MAE 0.727867 0.759451 0.781238 0.750681 0.747686 使用线性回归模型，对进行过对数处理的原特征数据进行五折交叉验证 1scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=1, cv = 5, scoring=make_scorer(mean_absolute_error)) [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=1)]: Done 5 out of 5 | elapsed: 0.1s finished 1print('AVG:', np.mean(scores)) AVG: 0.2124134663602803 1234scores = pd.DataFrame(scores.reshape(1,-1))scores.columns = ['cv' + str(x) for x in range(1, 6)]scores.index = ['MAE']scores .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cv1 cv2 cv3 cv4 cv5 MAE 0.208238 0.212408 0.215933 0.210742 0.214747 可以看出进行对数处理后，五折交叉验证的loss显著降低。 3️⃣.2️⃣ 考虑真实世界限制 例如：通过2018年的二手车价格预测2017年的二手车价格，这显然是不合理的，因此我们还可以采用时间顺序对数据集进行分隔。在本例中，我们选用靠前时间的4/5样本当作训练集，靠后时间的1/5当作验证集，最终结果与五折交叉验证差距不大。 1234567891011import datetimesample_feature = sample_feature.reset_index(drop=True)split_point = len(sample_feature) // 5 * 4train = sample_feature.loc[:split_point].dropna()val = sample_feature.loc[split_point:].dropna()train_X = train[continuous_feature_names]train_y_ln = np.log(train['price'])val_X = val[continuous_feature_names]val_y_ln = np.log(val['price']) 1model = model.fit(train_X, train_y_ln) 1mean_absolute_error(val_y_ln, model.predict(val_X)) 0.21498301182417004 3️⃣.3️⃣ 绘制学习率曲线与验证曲线¶ 学习曲线是一种用来判断训练模型的一种方法，它会自动把训练样本的数量按照预定的规则逐渐增加，然后画出不同训练样本数量时的模型准确度。 我们可以把$J_{train}(\theta)$和$J_{test}(\theta)$作为纵坐标，画出与训练集数据集$m$的大小关系，这就是学习曲线。通过学习曲线，可以直观地观察到模型的准确性和训练数据大小的关系。 我们可以比较直观的了解到我们的模型处于一个什么样的状态，如：过拟合（overfitting）或欠拟合（underfitting） 如果数据集的大小为$m$，则通过下面的流程即可画出学习曲线： 1.把数据集分成训练数据集和交叉验证集（可以看作测试集）； 2.取训练数据集的20%作为训练样本，训练出模型参数； 3.使用交叉验证集来计算训练出来的模型的准确性； 4.以训练集的score和交叉验证集score为纵坐标(这里的score取决于你使用的make_score方法，例如MAE)，训练集的个数作为横坐标，在坐标轴上画出上述步骤计算出来的模型准确性； 5.训练数据集增加10%，调到步骤2，继续执行，知道训练数据集大小为100%。 learning_curve()：这个函数主要是用来判断（可视化）模型是否过拟合的。下面是一些参数的解释： X：是一个m*n的矩阵，m:数据数量，n:特征数量； y：是一个m*1的矩阵，m:数据数量，相对于X的目标进行分类或回归； groups：将数据集拆分为训练/测试集时使用的样本的标签分组。[可选]； train_sizes：指定训练样品数量的变化规则。比如：np.linspace(0.1, 1.0, 5)表示把训练样品数量从0.1-1分成5等分，生成[0.1, 0.325,0.55,0.75,1]的序列，从序列中取出训练样品数量百分比，逐个计算在当前训练样本数量情况下训练出来的模型准确性。 cv：None，要使用默认的三折交叉验证（v0.22版本中将改为五折）； n_jobs：要并行运行的作业数。None表示1。 -1表示使用所有处理器； pre_dispatch：并行执行的预调度作业数（默认为全部）。该选项可以减少分配的内存。该字符串可以是“ 2 * n_jobs”之类的表达式； shuffle：bool，是否在基于train_sizes为前缀之前对训练数据进行洗牌； 1from sklearn.model_selection import learning_curve, validation_curve plt.fill_between()用来填充两条线间区域，其他好像没什么好解释的了。 12345678910111213141516171819202122232425def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,n_jobs=1, train_size=np.linspace(.1, 1.0, 5 )): plt.figure() plt.title(title) if ylim is not None: plt.ylim(*ylim) plt.xlabel('Training example') plt.ylabel('score') train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_size, scoring = make_scorer(mean_absolute_error)) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.grid()#区域 plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="r") plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color="g") plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label="Training score") plt.plot(train_sizes, test_scores_mean,'o-',color="g", label="Cross-validation score") plt.legend(loc="best") return plt 1plot_learning_curve(LinearRegression(), 'Liner_model', train_X[:], train_y_ln[:], ylim=(0.0, 0.5), cv=5, n_jobs=-1) 训练误差与验证误差逐渐一致，准确率也挺高（这里的score是MAE，所以是loss趋近于0.2，准确率趋近于0.8），但是训练误差几乎没变过，所以属于过拟合。这里给出一下高偏差欠拟合(bias)以及高方差过拟合(variance)的模样： 更形象一点： Data： Normal fitting: overfitting: serious overfitting: 4️⃣ 多种模型对比12345train = sample_feature[continuous_feature_names + ['price']].dropna()train_X = train[continuous_feature_names]train_y = train['price']train_y_ln = np.log(train_y + 1) 4️⃣.1️⃣ 线性模型 & 嵌入式特征选择 有一些前叙知识需要补全。其中关于正则化的知识： 分别为L1正则化与L2正则化； L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）； L1正则化是指权值向量w中各个元素的绝对值之和，通常表示为$\left | w \right | _{1} $； L2正则化是指权值向量w中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为$\left | w \right | _{2} $ L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择； L2正则化可以防止模型过拟合（overfitting），一定程度上，L1也可以防止过拟合； 更多其他知识可以看这篇文章：机器学习中正则化项L1和L2的直观理解 在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别。而嵌入式特征选择在学习器训练过程中自动地进行特征选择。嵌入式选择最常用的是L1正则化与L2正则化。在对线性回归模型加入两种正则化方法后，他们分别变成了岭回归与Lasso回归。 4️⃣.1️⃣.1️⃣ LinearRegression，Ridge，Lasso方法的运行123from sklearn.linear_model import LinearRegressionfrom sklearn.linear_model import Ridgefrom sklearn.linear_model import Lasso 123models = [LinearRegression(), Ridge(), Lasso()] 123456result = dict()for model in models: model_name = str(model).split('(')[0] scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)) result[model_name] = scores print(model_name + ' is finished') LinearRegression is finished Ridge is finished D:\Software\Anaconda\lib\site-packages\sklearn\linear_model\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems. ConvergenceWarning) D:\Software\Anaconda\lib\site-packages\sklearn\linear_model\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems. ConvergenceWarning) D:\Software\Anaconda\lib\site-packages\sklearn\linear_model\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems. ConvergenceWarning) D:\Software\Anaconda\lib\site-packages\sklearn\linear_model\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems. ConvergenceWarning) Lasso is finished D:\Software\Anaconda\lib\site-packages\sklearn\linear_model\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems. ConvergenceWarning) 4️⃣.1️⃣.2️⃣ 三种方法的对比123result = pd.DataFrame(result)result.index = ['cv' + str(x) for x in range(1, 6)]result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } LinearRegression Ridge Lasso cv1 0.208238 0.213319 0.394868 cv2 0.212408 0.216857 0.387564 cv3 0.215933 0.220840 0.402278 cv4 0.210742 0.215001 0.396664 cv5 0.214747 0.220031 0.397400 1.纯LinearRegression方法的情况：.intercept_是截距（与y轴的交点）即$\theta_0$，.coef_是模型的斜率即$\theta_1 - \theta_n$ 123model = LinearRegression().fit(train_X, train_y_ln)print('intercept:'+ str(model.intercept_)) # 截距（与y轴的交点）sns.barplot(abs(model.coef_), continuous_feature_names) intercept:22.23769348625359 纯LinearRegression回归可以发现，得到的参数列表是比较稀疏的。 1model.coef_ array([-3.73489972e-09, -6.10060860e-08, 7.40515349e-03, -1.88182450e-03, -1.24570527e-04, 3.63911807e-04, -3.32722751e-02, -2.75710825e-01, -1.43048695e-03, -3.28514719e-03, 8.57926933e-02, 5.66930260e+00, 8.27635812e-01, 4.09620867e-01, -6.72467882e-01, 4.24497013e+00, -3.15038152e+02, -1.17801777e+00, -8.30861129e+01, 6.01215351e-01, 9.25141289e-01, -1.32345773e+00, 1.20182089e+00, 1.10218030e+00, -2.59470516e-02, 8.88178420e-13, -2.92746484e-05, -3.63331132e-03, -1.63354329e-07, 5.68181101e-05, 1.29502381e-04, -2.97497182e-05, 2.21512681e-09, 4.26377388e-05, -1.18112552e-04, 1.35814944e-02]) 2.Lasso方法即L1正则化的情况： 123model = Lasso().fit(train_X, train_y_ln)print('intercept:'+ str(model.intercept_))sns.barplot(abs(model.coef_), continuous_feature_names) intercept:7.946156528722565 D:\Software\Anaconda\lib\site-packages\sklearn\linear_model\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems. ConvergenceWarning) L1正则化有助于生成一个稀疏权值矩阵，进而可以用于特征选择。如上图，我们发现power与userd_time特征非常重要。 3.Ridge方法即L2正则化的情况： 123model = Ridge().fit(train_X, train_y_ln)print('intercept:'+ str(model.intercept_))sns.barplot(abs(model.coef_), continuous_feature_names) intercept:2.7820015512913994 从上图可以看到有很多参数离0较远，很多为0。 原因在于L2正则化在拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。 可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』 除此之外，决策树通过信息熵或GINI指数选择分裂节点时，优先选择的分裂特征也更加重要，这同样是一种特征选择的方法。XGBoost与LightGBM模型中的model_importance指标正是基于此计算的 4️⃣.2️⃣ 非线性模型 支持向量机，决策树，随机森林，梯度提升树(GBDT)，多层感知机(MLP)，XGBoost，LightGBM等 12345678from sklearn.linear_model import LinearRegressionfrom sklearn.svm import SVCfrom sklearn.tree import DecisionTreeRegressorfrom sklearn.ensemble import RandomForestRegressorfrom sklearn.ensemble import GradientBoostingRegressorfrom sklearn.neural_network import MLPRegressorfrom xgboost.sklearn import XGBRegressorfrom lightgbm.sklearn import LGBMRegressor 定义模型集合 1234567models = [LinearRegression(), DecisionTreeRegressor(), RandomForestRegressor(), GradientBoostingRegressor(), MLPRegressor(solver='lbfgs', max_iter=100), XGBRegressor(n_estimators = 100, objective='reg:squarederror'), LGBMRegressor(n_estimators = 100)] 用数据一一对模型进行训练 123456result = dict()for model in models: model_name = str(model).split('(')[0] scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)) result[model_name] = scores print(model_name + ' is finished') LinearRegression is finished DecisionTreeRegressor is finished RandomForestRegressor is finished GradientBoostingRegressor is finished MLPRegressor is finished XGBRegressor is finished LGBMRegressor is finished 123result = pd.DataFrame(result)result.index = ['cv' + str(x) for x in range(1, 6)]result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } LinearRegression DecisionTreeRegressor RandomForestRegressor GradientBoostingRegressor MLPRegressor XGBRegressor LGBMRegressor cv1 0.208238 0.224863 0.163196 0.179385 581.596878 0.155881 0.153942 cv2 0.212408 0.218795 0.164292 0.183759 182.180288 0.158566 0.160262 cv3 0.215933 0.216482 0.164849 0.185005 250.668763 0.158520 0.159943 cv4 0.210742 0.220903 0.160878 0.181660 139.101476 0.156608 0.157528 cv5 0.214747 0.226087 0.164713 0.183704 108.664261 0.173250 0.157149 可以看到随机森林模型在每一个fold中均取得了更好的效果 1np.mean(result['RandomForestRegressor']) 0.16358568277026037 4️⃣.3️⃣ 模型调参 三种常用的调参方法如下： 贪心算法 https://www.jianshu.com/p/ab89df9759c8网格调参 https://blog.csdn.net/weixin_43172660/article/details/83032029贝叶斯调参 https://blog.csdn.net/linxid/article/details/81189154 123456789## LGB的参数集合：objective = ['regression', 'regression_l1', 'mape', 'huber', 'fair']num_leaves = [3,5,10,15,20,40, 55]max_depth = [3,5,10,15,20,40, 55]bagging_fraction = []feature_fraction = []drop_rate = [] 4️⃣.3️⃣.1️⃣ 贪心调参12345678910111213141516171819best_obj = dict()for obj in objective: model = LGBMRegressor(objective=obj) score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))) best_obj[obj] = score best_leaves = dict()for leaves in num_leaves: model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0], num_leaves=leaves) score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))) best_leaves[leaves] = score best_depth = dict()for depth in max_depth: model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0], num_leaves=min(best_leaves.items(), key=lambda x:x[1])[0], max_depth=depth) score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))) best_depth[depth] = score 1sns.lineplot(x=['0_initial','1_turning_obj','2_turning_leaves','3_turning_depth'], y=[0.143 ,min(best_obj.values()), min(best_leaves.values()), min(best_depth.values())]) 4️⃣.3️⃣.2️⃣ Grid Search 网格调参1from sklearn.model_selection import GridSearchCV 1234parameters = {'objective': objective , 'num_leaves': num_leaves, 'max_depth': max_depth}model = LGBMRegressor()clf = GridSearchCV(model, parameters, cv=5)clf = clf.fit(train_X, train_y) 1clf.best_params_ {'max_depth': 10, 'num_leaves': 55, 'objective': 'regression'} 123model = LGBMRegressor(objective='regression', num_leaves=55, max_depth=10) 1np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))) 0.1526351038235066 4️⃣.3️⃣.3️⃣ 贝叶斯调参12!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple bayesian-optimizationfrom bayes_opt import BayesianOptimization Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple Collecting bayesian-optimization Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b5/26/9842333adbb8f17bcb3d699400a8b1ccde0af0b6de8d07224e183728acdf/bayesian_optimization-1.1.0-py3-none-any.whl Requirement already satisfied: scikit-learn>=0.18.0 in d:\software\anaconda\lib\site-packages (from bayesian-optimization) (0.20.3) Requirement already satisfied: scipy>=0.14.0 in d:\software\anaconda\lib\site-packages (from bayesian-optimization) (1.2.1) Requirement already satisfied: numpy>=1.9.0 in d:\software\anaconda\lib\site-packages (from bayesian-optimization) (1.16.2) Installing collected packages: bayesian-optimization Successfully installed bayesian-optimization-1.1.0 1234567891011def rf_cv(num_leaves, max_depth, subsample, min_child_samples): val = cross_val_score( LGBMRegressor(objective = 'regression_l1', num_leaves=int(num_leaves), max_depth=int(max_depth), subsample = subsample, min_child_samples = int(min_child_samples) ), X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error) ).mean() return 1 - val # 贝叶斯调参目标是求最大值，所以用1减去误差 123456789rf_bo = BayesianOptimization( rf_cv, { 'num_leaves': (2, 100), 'max_depth': (2, 100), 'subsample': (0.1, 1), 'min_child_samples' : (2, 100) }) 1rf_bo.maximize() | iter | target | max_depth | min_ch... | num_le... | subsample | ------------------------------------------------------------------------- | [0m 1 [0m | [0m 0.8493 [0m | [0m 80.61 [0m | [0m 97.58 [0m | [0m 44.92 [0m | [0m 0.881 [0m | | [95m 2 [0m | [95m 0.8514 [0m | [95m 35.87 [0m | [95m 66.92 [0m | [95m 57.68 [0m | [95m 0.7878 [0m | | [95m 3 [0m | [95m 0.8522 [0m | [95m 49.75 [0m | [95m 68.95 [0m | [95m 64.99 [0m | [95m 0.1726 [0m | | [0m 4 [0m | [0m 0.8504 [0m | [0m 35.58 [0m | [0m 10.83 [0m | [0m 53.8 [0m | [0m 0.1306 [0m | | [0m 5 [0m | [0m 0.7942 [0m | [0m 63.37 [0m | [0m 32.21 [0m | [0m 3.143 [0m | [0m 0.4555 [0m | | [0m 6 [0m | [0m 0.7997 [0m | [0m 2.437 [0m | [0m 4.362 [0m | [0m 97.26 [0m | [0m 0.9957 [0m | | [95m 7 [0m | [95m 0.8526 [0m | [95m 47.85 [0m | [95m 69.39 [0m | [95m 68.02 [0m | [95m 0.8833 [0m | | [95m 8 [0m | [95m 0.8537 [0m | [95m 96.87 [0m | [95m 4.285 [0m | [95m 99.53 [0m | [95m 0.9389 [0m | | [95m 9 [0m | [95m 0.8546 [0m | [95m 96.06 [0m | [95m 97.85 [0m | [95m 98.82 [0m | [95m 0.8874 [0m | | [0m 10 [0m | [0m 0.7942 [0m | [0m 8.165 [0m | [0m 99.06 [0m | [0m 3.93 [0m | [0m 0.2049 [0m | | [0m 11 [0m | [0m 0.7993 [0m | [0m 2.77 [0m | [0m 99.47 [0m | [0m 91.16 [0m | [0m 0.2523 [0m | | [0m 12 [0m | [0m 0.852 [0m | [0m 99.3 [0m | [0m 43.04 [0m | [0m 62.67 [0m | [0m 0.9897 [0m | | [0m 13 [0m | [0m 0.8507 [0m | [0m 96.57 [0m | [0m 2.749 [0m | [0m 55.2 [0m | [0m 0.6727 [0m | | [0m 14 [0m | [0m 0.8168 [0m | [0m 3.076 [0m | [0m 3.269 [0m | [0m 33.78 [0m | [0m 0.5982 [0m | | [0m 15 [0m | [0m 0.8527 [0m | [0m 71.88 [0m | [0m 7.624 [0m | [0m 76.49 [0m | [0m 0.9536 [0m | | [0m 16 [0m | [0m 0.8528 [0m | [0m 99.44 [0m | [0m 99.28 [0m | [0m 69.58 [0m | [0m 0.7682 [0m | | [0m 17 [0m | [0m 0.8543 [0m | [0m 99.93 [0m | [0m 45.95 [0m | [0m 97.54 [0m | [0m 0.5095 [0m | | [0m 18 [0m | [0m 0.8518 [0m | [0m 60.87 [0m | [0m 99.67 [0m | [0m 61.3 [0m | [0m 0.7369 [0m | | [0m 19 [0m | [0m 0.8535 [0m | [0m 99.69 [0m | [0m 16.58 [0m | [0m 84.31 [0m | [0m 0.1025 [0m | | [0m 20 [0m | [0m 0.8507 [0m | [0m 54.68 [0m | [0m 38.11 [0m | [0m 54.65 [0m | [0m 0.9796 [0m | | [0m 21 [0m | [0m 0.8538 [0m | [0m 99.1 [0m | [0m 81.79 [0m | [0m 84.03 [0m | [0m 0.9823 [0m | | [0m 22 [0m | [0m 0.8529 [0m | [0m 99.28 [0m | [0m 3.373 [0m | [0m 83.48 [0m | [0m 0.7243 [0m | | [0m 23 [0m | [0m 0.8512 [0m | [0m 52.67 [0m | [0m 2.614 [0m | [0m 59.65 [0m | [0m 0.5286 [0m | | [95m 24 [0m | [95m 0.8546 [0m | [95m 75.81 [0m | [95m 61.62 [0m | [95m 99.78 [0m | [95m 0.9956 [0m | | [0m 25 [0m | [0m 0.853 [0m | [0m 45.9 [0m | [0m 33.68 [0m | [0m 74.59 [0m | [0m 0.73 [0m | | [0m 26 [0m | [0m 0.8532 [0m | [0m 82.58 [0m | [0m 63.9 [0m | [0m 78.61 [0m | [0m 0.1014 [0m | | [0m 27 [0m | [0m 0.8544 [0m | [0m 76.15 [0m | [0m 97.58 [0m | [0m 95.07 [0m | [0m 0.9995 [0m | | [0m 28 [0m | [0m 0.8545 [0m | [0m 95.75 [0m | [0m 74.96 [0m | [0m 99.45 [0m | [0m 0.7263 [0m | | [0m 29 [0m | [0m 0.8532 [0m | [0m 80.84 [0m | [0m 89.28 [0m | [0m 77.31 [0m | [0m 0.9389 [0m | | [0m 30 [0m | [0m 0.8545 [0m | [0m 82.92 [0m | [0m 35.46 [0m | [0m 96.66 [0m | [0m 0.969 [0m | ========================================================================= 1rf_bo.max {'target': 0.8545792238909576, 'params': {'max_depth': 75.80893509302794, 'min_child_samples': 61.62267920507557, 'num_leaves': 99.77501502667806, 'subsample': 0.9955706357612557}} 11 - rf_bo.max['target'] 0.14542077610904236 5️⃣ 总结 在本章中，我们完成了建模与调参的工作，并对我们的模型进行了验证。此外，我们还采用了一些基本方法来提高预测的精度，提升如下图所示。 12plt.figure(figsize=(13,5))sns.lineplot(x=['0_origin','1_log_transfer','2_L1_&_L2','3_change_model','4_parameter_turning'], y=[1.36 ,0.19, 0.19, 0.16, 0.15]) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>Editor</tag>
        <tag>DataMining</tag>
        <tag>Tianchi</tag>
        <tag>Study</tag>
        <tag>Jupyter</tag>
        <tag>seaborn</tag>
        <tag>内存优化</tag>
        <tag>五折交叉验证</tag>
        <tag>cross validation</tag>
        <tag>Linear Regression</tag>
        <tag>Ridge正则化</tag>
        <tag>Lasso正则化</tag>
        <tag>svm</tag>
        <tag>决策树</tag>
        <tag>随机森林</tag>
        <tag>梯度提升树(GBDT)</tag>
        <tag>多层感知机(MLP)</tag>
        <tag>XGBoost</tag>
        <tag>LightGBM</tag>
        <tag>贪心调参</tag>
        <tag>网格调参</tag>
        <tag>贝叶斯调参</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一文解决--matplotlib绘制极坐标热力图并插值优化]]></title>
    <url>%2F%E4%B8%80%E6%96%87%E8%A7%A3%E5%86%B3-matplotlib%E7%BB%98%E5%88%B6%E6%9E%81%E5%9D%90%E6%A0%87%E7%83%AD%E5%8A%9B%E5%9B%BE%E5%B9%B6%E6%8F%92%E5%80%BC%E4%BC%98%E5%8C%96.html</url>
    <content type="text"><![CDATA[图画的好看，文看的舒心。 0️⃣ 前言 又到了毕业季，学弟学妹们开始了毕设之旅，提到毕设想到了什么呢？对，没错，必备技巧就是绘制各种精美绝伦，举世无双的高清美图。这不，我刚炖了碗鲜美的极坐标热力图气象图汤。😢 如下： 1️⃣ 数据准备 数据可以是随机产生，或者放在csv文件中读。在csv中存储格式如下：| pos | 0 | 30 | 60 | 90 ||——-|———————|———————|———————|———————|| 0 | 1.101447148 | 1.308827831 | 1.526038083 | 1.603848713 || 30 | 1.101447148 | 1.279591136 | 1.49432297 | 1.577829862 || 60 | 1.101447148 | 1.204513965 | 1.435064241 | 1.52576792 || 90 | 1.101447148 | 1.108569817 | 1.404547306 | 1.499676995 || 120 | 1.101447148 | 1.204513965 | 1.435064241 | 1.52576792 || 150 | 1.101447148 | 1.279591136 | 1.49432297 | 1.577829862 || 180 | 1.101447148 | 1.308827831 | 1.526038083 | 1.603848713 || 210 | 1.101447148 | 1.279591136 | 1.49432297 | 1.577829862 || 240 | 1.101447148 | 1.204513965 | 1.435064241 | 1.52576792 || 270 | 1.101447148 | 1.108569817 | 1.404547306 | 1.499676995 || 300 | 1.101447148 | 1.204513965 | 1.435064241 | 1.52576792 || 330 | 1.101447148 | 1.279591136 | 1.49432297 | 1.577829862 || 360 | 1.101447148 | 1.308827831 | 1.526038083 | 1.603848713 | 因为要绘制的是极坐标图，所以列名代表的就是弧度，而行名代表的就是半径。csv文件下载：data.csv，下载后复制成四份，分别命名为data1.csv，data2.csv，data3.csv，data4.csv。 2️⃣ 代码2️⃣.1️⃣ 导入需要的包1234import numpy as npimport pandas as pdfrom scipy.interpolate import interp2d # 后面需要的插值库from matplotlib import pyplot as plt 2️⃣.2️⃣ 从csv文件中读取数据12345678data1 = pd.read_csv('data1.csv')data2 = pd.read_csv('data2.csv')data3 = pd.read_csv('data3.csv')data4 = pd.read_csv('data4.csv')data = [data1, data2, data3, data4]pos = np.array(data['pos']/180*np.pi)ind = np.array(data.columns[1:], dtype=np.int)values = np.array(data[ind.astype('str')]) 2️⃣.3️⃣ 随机产生数据123pos = np.radians(np.linspace(0, 360, 30))ind = np.arange(0, 90, 10)values = np.random.random((pos.size, ind.size)) 2️⃣.4️⃣ 全部代码(方便大家直接复制运行)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import numpy as npimport pandas as pdfrom scipy.interpolate import interp2dfrom matplotlib import pyplot as pltdata1 = pd.read_csv('data1.csv')data2 = pd.read_csv('data2.csv')data3 = pd.read_csv('data3.csv')data4 = pd.read_csv('data4.csv')data = [data1, data2, data3, data4]def plot_weather_heatmap(dataList, title): plt.figure(figsize=(25, 25)) for i in range(len(dataList)): data = dataList[i] ''' 方法一：从csv文件中读取数据 ''' # pos = np.array(data['pos']/180*np.pi) # ind = np.array(data.columns[1:], dtype=np.int) # values = np.array(data[ind.astype('str')]) ''' 方法二：随机产生数据 ''' pos = np.radians(np.linspace(0, 360, 30)) ind = np.arange(0, 90, 10) values = np.random.random((pos.size, ind.size)) #计算插值函数 func = interp2d(pos, ind, values.T, kind='cubic') tnew = np.linspace(0, 2*np.pi, 200) # theta #绘图数据点 rnew = np.linspace(0, 90, 100) # r vnew = func(tnew, rnew) tnew, rnew = np.meshgrid(tnew, rnew) ax = plt.subplot(2, 2, i+1, projection='polar') plt.pcolor(tnew, rnew, vnew, cmap='jet') plt.grid(c='black') plt.colorbar() ax.set_theta_zero_location("N") ax.set_theta_direction(-1) plt.title(title[i], fontsize=20) #设置坐标标签标注和字体大小 plt.xlabel(' ', fontsize=15) plt.ylabel(' ', fontsize=15) #设置坐标刻度字体大小 plt.xticks(fontsize=15, rotation=90) plt.yticks(fontsize=15) # cb.set_label("Pixel reflectance")title = ['Spring', 'Summer', 'Autumn', 'Winter']plot_weather_heatmap(data, title)plt.savefig("pic.png", dpi=300)plt.show() 3️⃣ cmap参数，为了更好看 关于下面这句中的jet参数是指定图的色域，可以更换。 1plt.pcolor(tnew, rnew, vnew, cmap='jet') 可选值如下 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>pandas</tag>
        <tag>matplotlib</tag>
        <tag>热力图</tag>
        <tag>极坐标</tag>
        <tag>插值</tag>
        <tag>画图</tag>
        <tag>气象图</tag>
        <tag>毕业论文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用Selenium秒填朋友圈各种问卷星调查问卷]]></title>
    <url>%2F%E5%88%A9%E7%94%A8Selenium%E7%A7%92%E5%A1%AB%E6%9C%8B%E5%8F%8B%E5%9C%88%E5%90%84%E7%A7%8D%E9%97%AE%E5%8D%B7%E6%98%9F%E8%B0%83%E6%9F%A5%E9%97%AE%E5%8D%B7.html</url>
    <content type="text"><![CDATA[这时代没人愿意填调查问卷，纯属无聊。 0️⃣ 前言毕业季到了，要开始写论文了，朋友圈各种同学的各种课题的调查问卷，但几乎没什么人填，想帮他们随机填一填。 1️⃣ 实现功能 目前还不算完善，只能填电脑端，手机端需要重新写逻辑，也就是链接里面要改成jq而不是m； 默认自动填10份，可以在主函数的times里面修改数值； 问卷链接在FillTheQuestionaire函数里面修改； 写了代理更换，但没钱换代理，换了代理可以更加真实一点，不然老是一个地区的人填不好。2️⃣ 代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495from selenium import webdriverimport timeimport reimport osfrom bs4 import BeautifulSoupfrom lxml import etreeimport randomimport pandas as pddef ChangeIP(): page = random.randint(1,4055) url = 'https://www.xicidaili.com/nn/' # url = 'https://www.kuaidaili.com/free/' driverIP = webdriver.Chrome() driverIP.get(url) content = driverIP.page_source.encode('utf-8') html = etree.HTML(content) t = html.xpath("//div[@class='bar']/div[@class='bar_inner fast']") flow = ['99%','98%'] for i in t: if i.attrib['style'].split(":")[1] in flow: index = t.index(i) break driverIP.quit() data = pd.read_html(content) ipinfo = data[0].values[index] # ipinfo = random.choice(data[0].values) ip = str(ipinfo[5]).lower() + "://" + str(ipinfo[1]) + ":" + str(ipinfo[2]) # ip = str(ipinfo[3]).lower() + "://" + str(ipinfo[0]) + ":" + str(ipinfo[1]) return ipdef FillTheQuestionaire(times): url = 'https://www.wjx.cn/jq/74385885.aspx' for t in range(times): mobileEmulation = {'deviceName': 'iPhone X'} options = webdriver.ChromeOptions() options.add_experimental_option('mobileEmulation', mobileEmulation) # options = webdriver.ChromeOptions() # ip = ChangeIP() # print(ip) # options.add_argument("--proxy-server=" + ip) # driver = webdriver.Chrome(chrome_options=options) # if t % 2 == 0: driver = webdriver.Chrome() # else: # driver = webdriver.Chrome(chrome_options=options) driver.get(url) content = driver.page_source.encode('utf-8') html = etree.HTML(content) soup = BeautifulSoup(content, 'lxml') NumOfQuestions = len(driver.find_elements_by_xpath( "//div[@class='div_question']")) for quiz in range(NumOfQuestions): try: question = driver.find_elements_by_xpath("//div[@id='divquestion" + str( quiz + 1) + "']//ul[@class='ulradiocheck']//li//a[@class='jqRadio']") random.choice(question).click() except: pass try: tr = driver.find_elements_by_xpath( "//div[@id='divquestion" + str(quiz + 1) + "']/table/tbody/tr") for t in range(len(tr)): button = driver.find_elements_by_xpath("//div[@id='divquestion" + str( quiz + 1) + "']/table/tbody/tr[" + str(t + 1) + "]/td/a[@class='jqRadio']") try: random.choice(button).click() except: pass except: pass try: checkbox = driver.find_elements_by_xpath("//div[@id='divquestion" + str( quiz + 1) + "']//ul[@class='ulradiocheck']//li//a[@class='jqCheckbox']") YorN = [x for x in range(2)] checkbox[0].click() for i in range(len(checkbox) - 2): if random.choice(YorN) == 1: print("是") try: checkbox[i+1].click() except: pass except: pass time.sleep(3) driver.find_elements_by_xpath("//input[@id='submit_button']")[0].click() time.sleep(2) driver.quit()if __name__ == "__main__": times = 10 FillTheQuestionaire(times) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Crawler</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>pandas</tag>
        <tag>Selenium</tag>
        <tag>朋友圈</tag>
        <tag>问卷星</tag>
        <tag>调查问卷</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零实现朴素贝叶斯分类器(离散情况)--以学生分班为例]]></title>
    <url>%2F%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8-%E7%A6%BB%E6%95%A3%E6%83%85%E5%86%B5-%E4%BB%A5%E5%AD%A6%E7%94%9F%E5%88%86%E7%8F%AD%E4%B8%BA%E4%BE%8B.html</url>
    <content type="text"><![CDATA[闲来无事，算法糖。 1️⃣ 任务要求1️⃣.1️⃣ 实现5个函数，分别为： load_data()：读取数据，并转换为可用的形式； split_data()：将数据集分为训练集和测试集； train()：从当前数据集中训练模型； predict()：用train()生成的模型，对测试集的学生进行分班； evaluate()：输出模型的准确率。1️⃣.2️⃣ train()和predict()不可以用第三方库；1️⃣.3️⃣ 数据集(下载链接：student.csv) 每个学生对应的情况，与最终分到的班级； 649行数据(instances)； 30个类别性特征； 6个班级，包括{A+，A，B，C，D，F}；1️⃣.4️⃣ 数据集解释：1 school - students school (binary: “GP” - Gabriel Pereira or “MS” - Mousinho da Silveira) 2 sex - students sex (binary: “F” - female or “M” - male) 3 address - students home address type (binary: “U” - urban or “R” - rural) 4 famsize - family size (binary: “LE3” - less or equal to 3 or “GT3” - greater than 3) 5 Pstatus - parents cohabitation status (binary: “T” - living together or “A” - apart) 6 Medu - mothers education (nominal: low, none, mid, high) 7 Fedu - fathers education (nominal: low, none, mid, high) 8 Mjob - mothers job (nominal: “teacher”, “health” care related, civil “services” (e.g. administrative or police), “at_home” or “other”) 9 Fjob - fathers job (nominal: “teacher”, “health” care related, civil “services” (e.g. administrative or police), “at_home” or “other”) 10 reason - reason to choose this school (nominal: close to “home”, school “reputation”, “course” preference or “other”) 11 guardian - students guardian (nominal: “mother”, “father” or “other”) 12 traveltime - home to school travel time (nominal: none, low, medium, high, very_high) 13 studytime - weekly study time (nominal: none, low, medium, high, very_high) 14 failures - number of past class failures (nominal: none, low, medium, high, very_high) 15 schoolsup - extra educational support (binary: yes or no)16 famsup - family educational support (binary: yes or no) 17 paid - extra paid classes within the course subject (binary: yes or no) 18 activities - extra-curricular activities (binary: yes or no) 19 nursery - attended nursery school (binary: yes or no) 20 higher - wants to take higher education (binary: yes or no) 21 internet - Internet access at home (binary: yes or no) 22 romantic - with a romantic relationship (binary: yes or no) 23 famrel - quality of family relationships (nominal: very_bad, bad, mediocre, good, excellent) 24 freetime - free time after school (nominal: very_low, low, mediocre, high, very_high) 25 goout - going out with friends (nominal: very_low, low, mediocre, high, very_high)26 Dalc - workday alcohol consumption (nominal: very_low, low, mediocre, high, very_high) 27 Walc - weekend alcohol consumption (nominal: very_low, low, mediocre, high, very_high) 28 health - current health status (nominal: very_bad, bad, mediocre, good, excellent) 29 absences - number of school absences (nominal: none, one_to_three, four_to_six, seven_to_ten, more_than_ten) 30 Grade - final grade (A+, A, B, C, D, F) 2️⃣ 代码2️⃣.1️⃣ load_data()12345# This function should open a data file in csv, and transform it into a usable format def load_data(): import pandas as pd data = pd.read_csv('student.csv', sep=',') return data 2️⃣.2️⃣ split_data()123456789101112131415161718192021# This function should split a data set into a training set and hold-out test setdef split_data(data, test_size): """ split the data into train set and test set :param data: Dtype from pd.read_csv :param test_size: float, define the position to split :return: """ import numpy as np X = data[list(data.columns[:-1])].values # get the instances matrix y = data['Grade'] # get the class vector index = np.arange(data.shape[0]) # get the number of the dataset np.random.shuffle(index) # shuffle the order of the data X = X[index] # reorder the instances matrix y = y[index] # reorder the class vector split_point = int(X.shape[0] * test_size) # define the position to split the data into train and test X_train, X_test = X[:split_point], X[split_point:] y_train, y_test = y[:split_point], y[split_point:] return X_train, X_test, y_train, y_test 2️⃣.3️⃣ train()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# This function should build a supervised NB modeldef train(X, y, alpha): """ train or generate the probability matrix of Naive Bayes Classifier :param X: Dtype from pd.read_csv, train set :param y: Dtype from pd.read_csv, train class :param alpha: Laplace smooth index :return: """ y_class_count = {} feature_dimension = len(X[1]) # number of feature # get the number of each labels for c in y: y_class_count[c] = y_class_count.get(c, 0) + 1 # generate the dict of class, e.g. {'A':'69',...} y_class_tuple = sorted(y_class_count.items(), reverse=False) # generate the tuple of class and sort it in terms of number, e.g. [('A','69'),...] K = len(y_class_tuple) # the specific number of class grade N = len(y) # the number of instances # get the prior probability prior_prob = {} for key in range(len(y_class_tuple)): prior_prob[y_class_tuple[key][0]] = (y_class_tuple[key][1] + alpha) / (N + K * alpha) # laplace smooth # get the value set of each feature feature_value = [] # feature with different value feature_value_number = [] # the number of unique values of each feature for feature in range(feature_dimension): unique_feature = list(set(X[:, feature])) # use `set` to get the unique value feature_value_number.append(len(unique_feature)) feature_value.append(unique_feature) # calculate the conditional probability conditional_prob = [] # calculate the count (x = a & y = c) for j in range(feature_dimension): count = [[0 for i in range(len(y_class_count))] for i in range(feature_value_number[j])] # use list comprehension to generate zero matrix, (feature_value_number[j] rows x y_class_count cols) for i in range(len(X[:, j])): for k in range(len(feature_value[j])): for t in range(len(y_class_count)): if X[:, j][i] == feature_value[j][k] and list(y)[i] == y_class_tuple[t][0]: # x = value and y = class, get the count count[k][t] += 1 # calculate the conditional probability for m in range(len(y_class_tuple)): for r in range(len(count)): count[r][m] = (count[r][m] + alpha) / (y_class_tuple[m][1] + alpha * feature_value_number[j]) # laplace smoothing conditional_prob.append(count) return y_class_tuple, prior_prob, feature_value, feature_value_number, conditional_prob 2️⃣.4️⃣ predict()1234567891011121314151617181920212223242526def classify(y_class_tuple, prior_prob, feature_value, conditional_prob, feature_value_number, alpha, instance): """ generate the answer of classification :param y_class_tuple: list, the tuple of class and sort it in terms of number :param prior_prob: float list, prior probability of class :param feature_value: list, feature value of all the attributes :param conditional_prob: float list, posterior probability :param feature_value_number: float list, number of different unique features :param alpha: float, Laplace smooth index default 1 :param instance: list, one row of test set :return: """ import math predict = {} for m in range(len(y_class_tuple)): # get the prior_probability of m-th label in y_class_tuple yhat = math.log(prior_prob[y_class_tuple[m][0]]) # use log-transformation to avoid float missing for n in range(len(instance)): if instance[n] in feature_value[n]: index = feature_value[n].index(instance[n]) # locate the feature in feature_value yhat = yhat + math.log(conditional_prob[n][index][m]) # accumulate the probability else: # if the value of feature is not in training set, return the laplace smoothing yhat = alpha / (feature_value_number[n] * alpha) predict[y_class_tuple[m][0]] = yhat return predict 1234567891011121314151617181920212223242526272829# This function should predict the class for an instance or a set of instances, based on a trained model def predict(y_class_tuple, prior_prob, feature_value, feature_value_number, conditional_prob, X, alpha, flag=0): """ predict the class for an instance or a set of instances, based on a trained model :param y_class_tuple: list, the tuple of class and sort it in terms of number :param prior_prob: float list, prior probability of class :param feature_value: list, feature value of all the attributes :param conditional_prob: float list, posterior probability :param feature_value_number: float list, number of different unique features :param alpha: float, Laplace smooth index default 1 :param X: Dtype from pd.read_csv, test set :param flag: set 1 return probability or set 0 return prediction, default 0 :return: """ import operator as op test_num = len(X) prediction = [0 for i in range(test_num)] probability = [0 for i in range(test_num)] for i in range(test_num): result = classify(y_class_tuple, prior_prob, feature_value, conditional_prob, feature_value_number, 1, X[i, :]) # result is the probability of each class result = sorted(result.items(), key=op.itemgetter(1), reverse=True) # the max probability is the predict class prediction[i] = result[0][0] # show the predict answer probability[i] = result[0][1] # show the predict probability if flag: return probability else: return prediction 2️⃣.5️⃣ evaluate()1234# This function should evaluate a set of predictions in terms of accuracydef evaluate(p, y_test): accuracy = sum(p == y_test)/len(y_test) return accuracy 2️⃣.6️⃣ 主函数12345data = load_data()X_train, X_test, y_train, y_test = split_data(data, 0.7)y_class_tuple, prior_prob, feature_value, feature_value_number, conditional_prob = train(X_train, y_train, 1)p = predict(y_class_tuple, prior_prob, feature_value, feature_value_number, conditional_prob, X_test, 1)evaluate(p, y_test) 3️⃣ 整合全部代码(方便大家复制后直接运行)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150# This function should open a data file in csv, and transform it into a usable format def load_data(): import pandas as pd data = pd.read_csv('student.csv', sep=',') return data# This function should split a data set into a training set and hold-out test setdef split_data(data, test_size): """ split the data into train set and test set :param data: Dtype from pd.read_csv :param test_size: float, define the position to split :return: """ import numpy as np X = data[list(data.columns[:-1])].values # get the instances matrix y = data['Grade'] # get the class vector index = np.arange(data.shape[0]) # get the number of the dataset np.random.shuffle(index) # shuffle the order of the data X = X[index] # reorder the instances matrix y = y[index] # reorder the class vector split_point = int(X.shape[0] * test_size) # define the position to split the data into train and test X_train, X_test = X[:split_point], X[split_point:] y_train, y_test = y[:split_point], y[split_point:] return X_train, X_test, y_train, y_test# This function should build a supervised NB modeldef train(X, y, alpha): """ train or generate the probability matrix of Naive Bayes Classifier :param X: Dtype from pd.read_csv, train set :param y: Dtype from pd.read_csv, train class :param alpha: Laplace smooth index :return: """ y_class_count = {} feature_dimension = len(X[1]) # number of feature # get the number of each labels for c in y: y_class_count[c] = y_class_count.get(c, 0) + 1 # generate the dict of class, e.g. {'A':'69',...} y_class_tuple = sorted(y_class_count.items(), reverse=False) # generate the tuple of class and sort it in terms of number, e.g. [('A','69'),...] K = len(y_class_tuple) # the specific number of class grade N = len(y) # the number of instances # get the prior probability prior_prob = {} for key in range(len(y_class_tuple)): prior_prob[y_class_tuple[key][0]] = (y_class_tuple[key][1] + alpha) / (N + K * alpha) # laplace smooth # get the value set of each feature feature_value = [] # feature with different value feature_value_number = [] # the number of unique values of each feature for feature in range(feature_dimension): unique_feature = list(set(X[:, feature])) # use `set` to get the unique value feature_value_number.append(len(unique_feature)) feature_value.append(unique_feature) # calculate the conditional probability conditional_prob = [] # calculate the count (x = a & y = c) for j in range(feature_dimension): count = [[0 for i in range(len(y_class_count))] for i in range(feature_value_number[j])] # use list comprehension to generate zero matrix, (feature_value_number[j] rows x y_class_count cols) for i in range(len(X[:, j])): for k in range(len(feature_value[j])): for t in range(len(y_class_count)): if X[:, j][i] == feature_value[j][k] and list(y)[i] == y_class_tuple[t][0]: # x = value and y = class, get the count count[k][t] += 1 # calculate the conditional probability for m in range(len(y_class_tuple)): for r in range(len(count)): count[r][m] = (count[r][m] + alpha) / (y_class_tuple[m][1] + alpha * feature_value_number[j]) # laplace smoothing conditional_prob.append(count) return y_class_tuple, prior_prob, feature_value, feature_value_number, conditional_probdef classify(y_class_tuple, prior_prob, feature_value, conditional_prob, feature_value_number, alpha, instance): """ generate the answer of classification :param y_class_tuple: list, the tuple of class and sort it in terms of number :param prior_prob: float list, prior probability of class :param feature_value: list, feature value of all the attributes :param conditional_prob: float list, posterior probability :param feature_value_number: float list, number of different unique features :param alpha: float, Laplace smooth index default 1 :param instance: list, one row of test set :return: """ import math predict = {} for m in range(len(y_class_tuple)): # get the prior_probability of m-th label in y_class_tuple yhat = math.log(prior_prob[y_class_tuple[m][0]]) # use log-transformation to avoid float missing for n in range(len(instance)): if instance[n] in feature_value[n]: index = feature_value[n].index(instance[n]) # locate the feature in feature_value yhat = yhat + math.log(conditional_prob[n][index][m]) # accumulate the probability else: # if the value of feature is not in training set, return the laplace smoothing yhat = alpha / (feature_value_number[n] * alpha) predict[y_class_tuple[m][0]] = yhat return predict# This function should predict the class for an instance or a set of instances, based on a trained model def predict(y_class_tuple, prior_prob, feature_value, feature_value_number, conditional_prob, X, alpha, flag=0): """ predict the class for an instance or a set of instances, based on a trained model :param y_class_tuple: list, the tuple of class and sort it in terms of number :param prior_prob: float list, prior probability of class :param feature_value: list, feature value of all the attributes :param conditional_prob: float list, posterior probability :param feature_value_number: float list, number of different unique features :param alpha: float, Laplace smooth index default 1 :param X: Dtype from pd.read_csv, test set :param flag: set 1 return probability or set 0 return prediction, default 0 :return: """ import operator as op test_num = len(X) prediction = [0 for i in range(test_num)] probability = [0 for i in range(test_num)] for i in range(test_num): result = classify(y_class_tuple, prior_prob, feature_value, conditional_prob, feature_value_number, 1, X[i, :]) # result is the probability of each class result = sorted(result.items(), key=op.itemgetter(1), reverse=True) # the max probability is the predict class prediction[i] = result[0][0] # show the predict answer probability[i] = result[0][1] # show the predict probability if flag: return probability else: return prediction# This function should evaluate a set of predictions in terms of accuracydef evaluate(p, y_test): accuracy = sum(p == y_test)/len(y_test) return accuracydata = load_data()X_train, X_test, y_train, y_test = split_data(data, 0.7)y_class_tuple, prior_prob, feature_value, feature_value_number, conditional_prob = train(X_train, y_train, 1)p = predict(y_class_tuple, prior_prob, feature_value, feature_value_number, conditional_prob, X_test, 1)evaluate(p, y_test) 10.4358974358974359 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Navie Bayes</tag>
        <tag>朴素贝叶斯</tag>
        <tag>python</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高性能分布式计算(HPC)作业3--节点通信，发布计算任务，并在计算任务中阻塞]]></title>
    <url>%2F%E9%AB%98%E6%80%A7%E8%83%BD%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97-HPC-%E4%BD%9C%E4%B8%9A3-%E8%8A%82%E7%82%B9%E9%80%9A%E4%BF%A1%EF%BC%8C%E5%8F%91%E5%B8%83%E8%AE%A1%E7%AE%97%E4%BB%BB%E5%8A%A1%EF%BC%8C%E5%B9%B6%E5%9C%A8%E8%AE%A1%E7%AE%97%E4%BB%BB%E5%8A%A1%E4%B8%AD%E9%98%BB%E5%A1%9E.html</url>
    <content type="text"><![CDATA[分布式计算作业3，socket实时通信，接收发送文件，在计算任务中阻塞。 1️⃣ 作业要求 建立两节点间的通信； 从节点 1 向节点 2 发送一个python语言编写的源程序 A，节点 2 执行程序 A，并向节点 1 返回计算结果； 程序 A 可在节点 2 上独立完成运行，无需其它条件支持； 额外功能Barrier 程序 A 中设计一个独立的函数Barrier； 在程序 A 执行过程中，执行 函数Barrier； 函数Barrier运行时，程序 A 阻塞在函数中。函数Barrier发起与节点 1 的通信。等待节点1 发送字符串GOON以后，函数Barrier返回，程序 A 继续执行直至结束。 2️⃣ 代码2️⃣.1️⃣ 代码1——节点通信，发布计算任务123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139import socketimport timeimport threadingimport osimport sysdef ReceiveFile(conn): while True: # 连接成功后一直使用当前连接，直到退出 with open("recv.py", "ab") as f: data = conn.recv(1024) if data == b'quit': break if data != b'success': f.write(data) conn.send("success".encode()) print("文件barrier.py已经接收！存储为recv.py") f.close()def SendAnswer(conn): while True: if os.path.exists("recv.py"): ans = os.popen("python recv.py") ansRead = ans.read() print("recv.py运行完毕，得到结果为%s" % (str(ansRead))) with open('output.txt', "w") as f: f.write(ansRead) f.close() print("将得到的结果写入output.txt") with open('output.txt', 'rb') as f: for i in f: conn.send(i) data = conn.recv(1024) if data != b'success': break conn.send('quit'.encode()) print("将output.txt发送完毕！") break conn.close()def SendPyFile(conn): with open('barrier.py', 'rb') as f: for i in f: conn.send(i) data = conn.recv(1024) if data != b'success': break print("文件barrier.py已经发送！") conn.send('quit'.encode())def ReceiveAnswer(conn): while True: with open("recv_output.txt", "ab") as f: data = conn.recv(1024) if data == b'quit': break if data != b'success': f.write(data) conn.send("success".encode()) print("结果接收完毕，存储在recv_output.txt！") f.close() conn.send('quit'.encode())def process(conn): print("等待5秒返回GOON！") for i in range(5): print(i+1) time.sleep(1) conn.send('GOON'.encode())def ClientBarrier(conn): while True: conn, addr = conn.accept() print("barrier函数阻塞，连接建立，地址为%s"%(str(addr))) t = threading.Thread(target=process, args=(conn,)) t.start() break# 改写线程类class msgThread(threading.Thread): def __init__(self, conn, flag): threading.Thread.__init__(self) self.conn = conn self.flag = flag def run(self): if self.flag == "send_file": SendPyFile(self.conn) elif self.flag == "receive_answer": ReceiveAnswer(self.conn) elif self.flag == "receive_file": ReceiveFile(self.conn) else: SendAnswer(self.conn)def Client(address): client = socket.socket(socket.AF_INET, socket.SOCK_STREAM) while True: try: client.connect((address, 6999)) # 建立一个链接，连接到本地的6999端口 break except: print("等待侦听！") time.sleep(1) Thread_receive = msgThread(client, "receive_file") Thread_send = msgThread(client, "send_answer") Thread_receive.start() Thread_send.start()def Server(): server = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server.bind(('127.0.0.1', 6999)) # 绑定要监听的端口 server.listen(5) # 开始监听 表示可以使用五个链接排队 conn, addr = server.accept() # 等待链接,多个链接的时候就会出现问题,其实返回了两个值 print("侦听器已启动！port：6999") print("连接建立，地址在%s"%(str(addr))) Thread_receive = msgThread(conn, "send_file") Thread_send = msgThread(conn, "receive_answer") Thread_barrier = threading.Thread(target=ClientBarrier, args=(server,)) Thread_receive.start() Thread_send.start() Thread_barrier.start()if __name__ == "__main__": BootMode = input("请选择启动方式(1(控制节点)或2(计算节点))：\n") if BootMode == '1': Server() else: port = input("请输入侦听服务器地址(默认127.0.0.1)：\n") Client(port) 2️⃣.2️⃣ 代码2——Barrier函数12345678910111213141516171819202122import randomimport socketdef barrier(): client = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client.connect(('127.0.0.1', 6999)) while True: data = client.recv(1024) if data == b'GOON': break client.send('quit'.encode()) client.close()def MaxMin(): a = [] for i in range(10): a.append(random.random() * 10) barrier() print(max(a), min(a))if __name__ == "__main__": MaxMin() document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>distributed system</tag>
        <tag>节点通信</tag>
        <tag>多线程</tag>
        <tag>multithreading</tag>
        <tag>socket</tag>
        <tag>分布式计算</tag>
        <tag>Barrier</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高性能分布式计算(HPC)作业2--节点通信，发布计算任务]]></title>
    <url>%2F%E9%AB%98%E6%80%A7%E8%83%BD%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97-HPC-%E4%BD%9C%E4%B8%9A2-%E8%8A%82%E7%82%B9%E9%80%9A%E4%BF%A1%EF%BC%8C%E5%8F%91%E5%B8%83%E8%AE%A1%E7%AE%97%E4%BB%BB%E5%8A%A1.html</url>
    <content type="text"><![CDATA[分布式计算作业2，socket实时通信，接收发送文件并阻塞。 1️⃣ 作业要求 建立两节点间的通信； 从节点 1 向节点 2 发送一个python语言编写的源程序 A，节点 2 执行程序 A，并向节点 1 返回计算结果； 程序 A 可在节点 2 上独立完成运行，无需其它条件支持。 2️⃣程序2️⃣.1️⃣ 程序1——实现通信123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119import socketimport timeimport threadingimport osimport sysdef ReceiveFile(conn): while True: # 连接成功后一直使用当前连接，直到退出 with open("recv.py", "ab") as f: data = conn.recv(1024) if data == b'quit': break if data != b'success': f.write(data) conn.send("success".encode()) print("文件test.py已经接收！存储为recv.py") f.close()def SendAnswer(conn): while True: if os.path.exists("recv.py"): ans = os.popen("python recv.py") ansRead = ans.read() print("recv.py运行完毕，得到结果为%s" % (str(ansRead))) with open('output.txt', "w") as f: f.write(ansRead) f.close() print("将得到的结果写入output.txt") with open('output.txt', 'rb') as f: for i in f: conn.send(i) data = conn.recv(1024) if data != b'success': break conn.send('quit'.encode()) print("将output.txt发送完毕！") break conn.close()def SendPyFile(conn): with open('test.py', 'rb') as f: for i in f: conn.send(i) data = conn.recv(1024) if data != b'success': break print("文件test.py已经发送！") conn.send('quit'.encode())def ReceiveAnswer(conn): while True: with open("recv_output.txt", "ab") as f: data = conn.recv(1024) if data == b'quit': break if data != b'success': f.write(data) conn.send("success".encode()) print("结果接收完毕，存储在recv_output.txt！") f.close() conn.send('quit'.encode())class msgThread(threading.Thread): def __init__(self, conn, flag): threading.Thread.__init__(self) self.conn = conn self.flag = flag def run(self): if self.flag == "send_file": SendPyFile(self.conn) elif self.flag == "receive_answer": ReceiveAnswer(self.conn) elif self.flag == "receive_file": ReceiveFile(self.conn) else: SendAnswer(self.conn)# 改写线程类def Client(address): client = socket.socket(socket.AF_INET, socket.SOCK_STREAM) while True: try: client.connect((address, 6999)) # 建立一个链接，连接到本地的6999端口 break except: print("等待侦听！") time.sleep(1) Thread_receive = msgThread(client, "receive_file") Thread_send = msgThread(client, "send_answer") Thread_receive.start() Thread_send.start()def Server(): server = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server.bind(('127.0.0.1', 6999)) # 绑定要监听的端口 server.listen(5) # 开始监听 表示可以使用五个链接排队 conn, addr = server.accept() # 等待链接,多个链接的时候就会出现问题,其实返回了两个值 print("侦听器已启动！port：6999") print("连接建立，地址在%s" % (str(addr))) Thread_receive = msgThread(conn, "send_file") Thread_send = msgThread(conn, "receive_answer") Thread_receive.start() Thread_send.start()if __name__ == "__main__": BootMode = input("请选择启动方式(1(控制节点)或2(计算节点))：\n") if BootMode == '1': Server() else: port = input("请输入侦听服务器地址(默认127.0.0.1)：\n") Client(port) 2️⃣.2️⃣ 程序2——计算任务函数123456789import randoma = []for i in range(10): a.append(random.random() * 10) print(max(a),min(a)) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>distributed system</tag>
        <tag>节点通信</tag>
        <tag>多线程</tag>
        <tag>multithreading</tag>
        <tag>socket</tag>
        <tag>分布式计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高性能分布式计算(HPC)作业1--节点实时通信]]></title>
    <url>%2F%E9%AB%98%E6%80%A7%E8%83%BD%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97-HPC-%E4%BD%9C%E4%B8%9A1-%E8%8A%82%E7%82%B9%E5%AE%9E%E6%97%B6%E9%80%9A%E4%BF%A1.html</url>
    <content type="text"><![CDATA[分布式计算作业1，socket实时通信。 1️⃣ 作业要求 通信双方使用同一个通信程序； 通信程序有两种启动方式。方式一：启动后创建侦听器，等待连接。方式二：启动是给定服务器（处于侦听状态的机器）地址； 首先按方式一启动侦听节点； 然后按方式二启动第二个通信节点，与侦听节点建立连接； 完成连接后，通信双方进入双向通信状态，可以互发文字消息； 任何一方发出“QUIT”消息（大小写不敏感）即终止通信，双方终止程序运行。2️⃣ 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108import socket # 客户端 发送一个数据，再接收一个数据import timeimport threadingimport ctypesimport inspectquit = 0# 终止线程def _async_raise(tid, exctype): """raises the exception, performs cleanup if needed""" tid = ctypes.c_long(tid) if not inspect.isclass(exctype): exctype = type(exctype) res = ctypes.pythonapi.PyThreadState_SetAsyncExc( tid, ctypes.py_object(exctype)) if res == 0: raise ValueError("invalid thread id") elif res != 1: # """if it returns a number greater than one, you're in trouble, # and you should call it again with exc=NULL to revert the effect""" ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, None) raise SystemError("PyThreadState_SetAsyncExc failed")def stop_thread(thread): _async_raise(thread.ident, SystemExit)def ReceiveMsg(conn): global quit while True: try: data = conn.recv(1024) except: print("连接结束") conn.close() break if str(data.decode()).upper() != 'QUIT': print('recive:', data.decode()) else: quit = 1 conn.close() breakdef SendMsg(conn): global quit while True: send = input("send:\n") try: conn.send(send.encode('utf-8')) except: print("连接结束") conn.close() break if str(send).upper() == 'QUIT': conn.close() break# 改写线程class msgThread(threading.Thread): def __init__(self, conn, flag): threading.Thread.__init__(self) self.conn = conn self.flag = flag def run(self): if self.flag == 1: ReceiveMsg(self.conn) else: SendMsg(self.conn)# 声明socket类型，同时生成链接对象def Client(address): client = socket.socket(socket.AF_INET, socket.SOCK_STREAM) while True: try: client.connect((address, 6999)) # 建立一个链接，连接到本地的6999端口 break except: print("等待侦听！") time.sleep(1) Thread_receive = msgThread(client, 1) Thread_send = msgThread(client, 2) Thread_receive.start() Thread_send.start()def Server(): server = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server.bind(('127.0.0.1', 6999)) # 绑定要监听的端口 server.listen(5) # 开始监听 表示可以使用五个链接排队 conn, addr = server.accept() # 等待链接,多个链接的时候就会出现问题,其实返回了两个值 print("侦听器已启动！port：6999") print(conn, addr) Thread_receive = msgThread(conn, 1) Thread_send = msgThread(conn, 2) Thread_receive.start() Thread_send.start()if __name__ == "__main__": BootMode = input("请选择启动方式(1或2)：\n") if BootMode == '1': Server() else: port = input("请输入侦听服务器地址(默认127.0.0.1)：\n") Client(port) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>分布式</tag>
        <tag>distributed system</tag>
        <tag>节点通信</tag>
        <tag>多线程</tag>
        <tag>multithreading</tag>
        <tag>socket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天池_二手车价格预测_Task_3_特征工程]]></title>
    <url>%2F%E5%A4%A9%E6%B1%A0-%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B-Task-3-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B.html</url>
    <content type="text"><![CDATA[接续 特征工程🃏 特征工程，是指用一系列工程化的方式从原始数据中筛选出更好的数据特征，以提升模型的训练效果。业内有一句广为流传的话是：数据和特征决定了机器学习的上限，而模型和算法是在逼近这个上限而已。由此可见，好的数据和特征是模型和算法发挥更大的作用的前提。特征工程通常包括数据预处理、特征选择、降维等环节。如下图所示： 0️⃣ 前言 我们经常在处理数据时，会面临以下问题： 收集的数据格式不对（如 SQL 数据库、JSON、CSV 等） 缺失值和异常值 标准化 减少数据集中存在的固有噪声（部分存储数据可能已损坏） 数据集中的某些功能可能无法收集任何信息以供分析 而减少统计分析期间要使用的特征的数量可能会带来一些好处，例如： 提高精度 降低过拟合风险 加快训练速度 改进数据可视化 增加我们模型的可解释性 事实上，统计上证明，当执行机器学习任务时，存在针对每个特定任务应该使用的最佳数量的特征（图 1）。如果添加的特征比必要的特征多，那么我们的模型性能将下降（因为添加了噪声）。真正的挑战是找出哪些特征是最佳的使用特征（这实际上取决于我们提供的数据量和我们正在努力实现的任务的复杂性）。这就是特征选择技术能够帮到我们的地方！ 0️⃣.1️⃣ 赛题重述 这是一道来自于天池的新手练习题目，用数据分析、机器学习等手段进行 二手车售卖价格预测 的回归问题。赛题本身的思路清晰明了，即对给定的数据集进行分析探讨，然后设计模型运用数据进行训练，测试模型，最终给出选手的预测结果。前面我们已经进行过EDA分析在这里天池_二手车价格预测_Task1-2_赛题理解与数据分析 0️⃣.2️⃣ 数据集概述 赛题官方给出了来自Ebay Kleinanzeigen的二手车交易记录，总数据量超过40w，包含31列变量信息，其中15列为匿名变量，即v0至v15。并从中抽取15万条作为训练集，5万条作为测试集A，5万条作为测试集B，同时对name、model、brand和regionCode等信息进行脱敏。具体的数据表如下图： Field Description SaleID 交易ID，唯一编码 name 汽车交易名称，已脱敏 regDate 汽车注册日期，例如20160101，2016年01月01日 model 车型编码，已脱敏 brand 汽车品牌，已脱敏 bodyType 车身类型：豪华轿车：0，微型车：1，厢型车：2，大巴车：3，敞篷车：4，双门汽车：5，商务车：6，搅拌车：7 fuelType 燃油类型：汽油：0，柴油：1，液化石油气：2，天然气：3，混合动力：4，其他：5，电动：6 gearbox 变速箱：手动：0，自动：1 power 发动机功率：范围 [ 0, 600 ] kilometer 汽车已行驶公里，单位万km notRepairedDamage 汽车有尚未修复的损坏：是：0，否：1 regionCode 地区编码，已脱敏 seller 销售方：个体：0，非个体：1 offerType 报价类型：提供：0，请求：1 creatDate 汽车上线时间，即开始售卖时间 price 二手车交易价格（预测目标） v系列特征 匿名特征，包含v0-14在内15个匿名特征 1️⃣ 异常缺失值删除1️⃣.1️⃣ 导入库与数据12345678import pandas as pdimport numpy as npimport matplotlibimport matplotlib.pyplot as pltimport seaborn as snsfrom operator import itemgetter%matplotlib inline 1234train = pd.read_csv('used_car_train_20200313.csv', sep=' ')test = pd.read_csv('used_car_testA_20200313.csv', sep=' ')print(train.shape)print(test.shape) (150000, 31) (50000, 30) 1train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SaleID name regDate model brand bodyType fuelType gearbox power kilometer ... v_5 v_6 v_7 v_8 v_9 v_10 v_11 v_12 v_13 v_14 0 0 736 20040402 30.0 6 1.0 0.0 0.0 60 12.5 ... 0.235676 0.101988 0.129549 0.022816 0.097462 -2.881803 2.804097 -2.420821 0.795292 0.914762 1 1 2262 20030301 40.0 1 2.0 0.0 0.0 0 15.0 ... 0.264777 0.121004 0.135731 0.026597 0.020582 -4.900482 2.096338 -1.030483 -1.722674 0.245522 2 2 14874 20040403 115.0 15 1.0 0.0 0.0 163 12.5 ... 0.251410 0.114912 0.165147 0.062173 0.027075 -4.846749 1.803559 1.565330 -0.832687 -0.229963 3 3 71865 19960908 109.0 10 0.0 0.0 1.0 193 15.0 ... 0.274293 0.110300 0.121964 0.033395 0.000000 -4.509599 1.285940 -0.501868 -2.438353 -0.478699 4 4 111080 20120103 110.0 5 1.0 0.0 0.0 68 5.0 ... 0.228036 0.073205 0.091880 0.078819 0.121534 -1.896240 0.910783 0.931110 2.834518 1.923482 5 rows × 31 columns 1train.columns Index(['SaleID', 'name', 'regDate', 'model', 'brand', 'bodyType', 'fuelType', 'gearbox', 'power', 'kilometer', 'notRepairedDamage', 'regionCode', 'seller', 'offerType', 'creatDate', 'price', 'v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13', 'v_14'], dtype='object') 1️⃣.2️⃣ 异常值删除 这里可以将箱型图中的超过上下限的那些值作为异常值删除。如下图所示，箱型图中间是一个箱体，也就是粉红色部分，箱体左边，中间，右边分别有一条线，左边是下分位数($Q1$)，右边是上四分位数($Q3$)，中间是中位数($Median$)，上下四分位数之差是四分位距$IQR（Interquartile Range$，用$Q1-1.5IQR$得到下边缘（最小值），$Q3+1.5IQR$得到上边缘（最大值）。在上边缘之外的数据就是极大异常值，在下边缘之外的数据就是极小异常值。 搞清楚原理那我们就构造一个实现上述功能的函数吧！ 123456789101112131415161718192021222324252627282930313233343536373839def drop_outliers(data, col_name, scale = 1.5): """ 用于清洗异常值，默认用 box_plot（scale=1.5）进行清洗 :param data: 接收 pandas 数据格式 :param col_name: pandas 列名 :param scale: 尺度 :return: """ data_n = data.copy() data_series = data_n[col_name] IQR = scale * (data_series.quantile(0.75) - data_series.quantile(0.25)) # quantile是pd内置的求四分位的函数 val_low = data_series.quantile(0.25) - IQR # 下边缘 val_up = data_series.quantile(0.75) + IQR # 上边缘 rule_low = (data_series < val_low) # 下边缘的极小异常值的下标列表 rule_up = (data_series > val_up) # 上边缘的极大异常值的下标列表 index = np.arange(data_series.shape[0])[rule_low | rule_up] # | 运算就是说只要rule_low和rule_up中只要有一个值为True，就把这个下标取出来 print(index) print("Delete number is: {}".format(len(index))) data_n = data_n.drop(index) # 删除index对应下标的元素 data_n.reset_index(drop=True, inplace=True) #下文有介绍 print("Now column number is: {}".format(data_n.shape[0])) index_low = np.arange(data_series.shape[0])[rule_low] # 下边缘的异常数据的描述统计量 outliers = data_series.iloc[index_low] print("Description of data less than the lower bound is:") print(pd.Series(outliers).describe()) index_up = np.arange(data_series.shape[0])[rule_up] # 上边缘的异常数据的描述统计量 outliers = data_series.iloc[index_up] print("Description of data larger than the upper bound is:") print(pd.Series(outliers).describe()) fig, ax = plt.subplots(1, 2, figsize = (10, 7)) sns.boxplot(y = data[col_name], data = data, palette = "Set1", ax = ax[0]) sns.boxplot(y = data_n[col_name], data = data_n, palette = "Set1", ax = ax[1]) return data_n 这里reset_index可以还原索引，重新变为默认的整型索引 DataFrame.reset_index(level=None, drop=False, inplace=False, col_level=0, col_fill=”) level：int、str、tuple或list，默认无，仅从索引中删除给定级别。默认情况下移除所有级别。控制了具体要还原的那个等级的索引 drop：drop为False则索引列会被还原为普通列，否则会丢失 inplace：默认为False，适当修改DataFrame(不要创建新对象) col_level：int或str，默认值为0，如果列有多个级别，则确定将标签插入到哪个级别。默认情况下，它将插入到第一级。 col_fill：对象，默认‘’，如果列有多个级别，则确定其他级别的命名方式。如果没有，则重复索引名 1drop_outliers(train, 'power', scale=1.5) [ 33 77 104 ... 149967 149981 149984] Delete number is: 4878 Now column number is: 145122 Description of data less than the lower bound is: count 0.0 mean NaN std NaN min NaN 25% NaN 50% NaN 75% NaN max NaN Name: power, dtype: float64 Description of data larger than the upper bound is: count 4878.000000 mean 410.132021 std 884.219933 min 264.000000 25% 286.000000 50% 306.000000 75% 349.000000 max 19312.000000 Name: power, dtype: float64 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SaleID name regDate model brand bodyType fuelType gearbox power kilometer ... v_5 v_6 v_7 v_8 v_9 v_10 v_11 v_12 v_13 v_14 0 0 736 20040402 30.0 6 1.0 0.0 0.0 60 12.5 ... 0.235676 0.101988 0.129549 0.022816 0.097462 -2.881803 2.804097 -2.420821 0.795292 0.914762 1 1 2262 20030301 40.0 1 2.0 0.0 0.0 0 15.0 ... 0.264777 0.121004 0.135731 0.026597 0.020582 -4.900482 2.096338 -1.030483 -1.722674 0.245522 2 2 14874 20040403 115.0 15 1.0 0.0 0.0 163 12.5 ... 0.251410 0.114912 0.165147 0.062173 0.027075 -4.846749 1.803559 1.565330 -0.832687 -0.229963 3 3 71865 19960908 109.0 10 0.0 0.0 1.0 193 15.0 ... 0.274293 0.110300 0.121964 0.033395 0.000000 -4.509599 1.285940 -0.501868 -2.438353 -0.478699 4 4 111080 20120103 110.0 5 1.0 0.0 0.0 68 5.0 ... 0.228036 0.073205 0.091880 0.078819 0.121534 -1.896240 0.910783 0.931110 2.834518 1.923482 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 145117 149995 163978 20000607 121.0 10 4.0 0.0 1.0 163 15.0 ... 0.280264 0.000310 0.048441 0.071158 0.019174 1.988114 -2.983973 0.589167 -1.304370 -0.302592 145118 149996 184535 20091102 116.0 11 0.0 0.0 0.0 125 10.0 ... 0.253217 0.000777 0.084079 0.099681 0.079371 1.839166 -2.774615 2.553994 0.924196 -0.272160 145119 149997 147587 20101003 60.0 11 1.0 1.0 0.0 90 6.0 ... 0.233353 0.000705 0.118872 0.100118 0.097914 2.439812 -1.630677 2.290197 1.891922 0.414931 145120 149998 45907 20060312 34.0 10 3.0 1.0 0.0 156 15.0 ... 0.256369 0.000252 0.081479 0.083558 0.081498 2.075380 -2.633719 1.414937 0.431981 -1.659014 145121 149999 177672 19990204 19.0 28 6.0 0.0 1.0 193 12.5 ... 0.284475 0.000000 0.040072 0.062543 0.025819 1.978453 -3.179913 0.031724 -1.483350 -0.342674 145122 rows × 31 columns 从这张删除异常值前后的箱型图对比可以看出，剔除异常值后，数据的分布就很均匀了。 下面我们就批量对所有的特征进行一次异常数据删除： 1234567891011121314151617181920212223242526272829303132333435363738def Bach_drop_outliers(data,scale=1.5): dataNew = data.copy() for fea in data.columns: try: IQR = scale * (dataNew[fea].quantile(0.75) - dataNew[fea].quantile(0.25)) # quantile是pd内置的求四分位的函数 except: continue val_low = dataNew[fea].quantile(0.25) - IQR # 下边缘 val_up = dataNew[fea].quantile(0.75) + IQR # 上边缘 rule_low = (dataNew[fea] < val_low) # 下边缘的极小异常值的下标列表 rule_up = (dataNew[fea] > val_up) # 上边缘的极大异常值的下标列表 index = np.arange(dataNew[fea].shape[0])[rule_low | rule_up] # | 运算就是说只要rule_low和rule_up中只要有一个值为True，就把这个下标取出来 print("feature %s deleted number is %d"%(fea, len(index))) dataNew = dataNew.drop(index)# 删除index对应下标的元素 dataNew.reset_index(drop=True, inplace=True) fig, ax = plt.subplots(5, 6, figsize = (20, 15)) x = 0 y = 0 for fea in dataNew.columns: try: sns.boxplot(y = dataNew[fea], data =dataNew, palette = "Set2", ax = ax[x][y]) y+=1 if y == 6: y = 0 x += 1 except: print(fea) y+=1 if y == 6: y = 0 x += 1 continue return dataNewtrain = Bach_drop_outliers(train) feature SaleID deleted number is 0 feature name deleted number is 0 feature regDate deleted number is 0 feature model deleted number is 9720 feature brand deleted number is 4032 feature bodyType deleted number is 5458 feature fuelType deleted number is 333 feature gearbox deleted number is 26829 feature power deleted number is 1506 feature kilometer deleted number is 15306 feature regionCode deleted number is 4 feature seller deleted number is 1 feature offerType deleted number is 0 feature creatDate deleted number is 13989 feature price deleted number is 4527 feature v_0 deleted number is 2558 feature v_1 deleted number is 0 feature v_2 deleted number is 487 feature v_3 deleted number is 173 feature v_4 deleted number is 61 feature v_5 deleted number is 0 feature v_6 deleted number is 0 feature v_7 deleted number is 64 feature v_8 deleted number is 0 feature v_9 deleted number is 24 feature v_10 deleted number is 0 feature v_11 deleted number is 0 feature v_12 deleted number is 4 feature v_13 deleted number is 0 feature v_14 deleted number is 1944 notRepairedDamage v_14 可以看出，经过箱型图异常值删除后，新数据的箱型图的数据几乎没有异常值了，甚至有些箱型图的数据是一条直线，当然那是因为数据本身就是种类非0即1。 2️⃣ 树模型的特征构造 训练集和测试集放在一起，方便构造特征 1234train['train'] = 1test['train'] = 0data = pd.concat([train, test], ignore_index=True, sort=False)data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SaleID name regDate model brand bodyType fuelType gearbox power kilometer ... v_6 v_7 v_8 v_9 v_10 v_11 v_12 v_13 v_14 train 0 1 2262 20030301 40.0 1 2.0 0.0 0.0 0 15.0 ... 0.121004 0.135731 0.026597 0.020582 -4.900482 2.096338 -1.030483 -1.722674 0.245522 1 1 5 137642 20090602 24.0 10 0.0 1.0 0.0 109 10.0 ... 0.000518 0.119838 0.090922 0.048769 1.885526 -2.721943 2.457660 -0.286973 0.206573 1 2 7 165346 19990706 26.0 14 1.0 0.0 0.0 101 15.0 ... 0.000000 0.122943 0.039839 0.082413 3.693829 -0.245014 -2.192810 0.236728 0.195567 1 3 10 18961 20050811 19.0 9 3.0 1.0 0.0 101 15.0 ... 0.105385 0.077271 0.042445 0.060794 -4.206000 1.060391 -0.647515 -0.191194 0.349187 1 4 13 8129 20041110 65.0 1 0.0 0.0 0.0 150 15.0 ... 0.106950 0.134945 0.050364 0.051359 -4.614692 0.821889 0.753490 -0.886425 -0.341562 1 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 112975 199995 20903 19960503 4.0 4 4.0 0.0 0.0 116 15.0 ... 0.130044 0.049833 0.028807 0.004616 -5.978511 1.303174 -1.207191 -1.981240 -0.357695 0 112976 199996 708 19991011 0.0 0 0.0 0.0 0.0 75 15.0 ... 0.108095 0.066039 0.025468 0.025971 -3.913825 1.759524 -2.075658 -1.154847 0.169073 0 112977 199997 6693 20040412 49.0 1 0.0 1.0 1.0 224 15.0 ... 0.105724 0.117652 0.057479 0.015669 -4.639065 0.654713 1.137756 -1.390531 0.254420 0 112978 199998 96900 20020008 27.0 1 0.0 0.0 1.0 334 15.0 ... 0.000490 0.137366 0.086216 0.051383 1.833504 -2.828687 2.465630 -0.911682 -2.057353 0 112979 199999 193384 20041109 166.0 6 1.0 NaN 1.0 68 9.0 ... 0.000300 0.103534 0.080625 0.124264 2.914571 -1.135270 0.547628 2.094057 -1.552150 0 112980 rows × 32 columns 2️⃣.1️⃣ 时间特征构造 使用时间：data['creatDate'] - data['regDate']，反应汽车使用时间，一般来说价格与使用时间成反比 不过要注意，数据里有时间出错的格式，所以我们需要 errors=’coerce’ 123data['used_time'] = (pd.to_datetime(data['creatDate'], format='%Y%m%d', errors='coerce') - pd.to_datetime(data['regDate'], format='%Y%m%d', errors='coerce')).dt.daysdata['used_time'] 0 4757.0 1 2482.0 2 6108.0 3 3874.0 4 4154.0 ... 112975 7261.0 112976 6014.0 112977 4345.0 112978 NaN 112979 4151.0 Name: used_time, Length: 112980, dtype: float64 看一下空数据，有 7.6k 个样本的时间是有问题的，我们可以选择删除，也可以选择放着。 但是这里不建议删除，因为删除缺失数据占总样本量过大，3.8% 我们可以先放着，因为如果我们 XGBoost 之类的决策树，其本身就能处理缺失值，所以可以不用管； 1data['used_time'].isnull().sum() 8591 1data.isnull().sum().sum() 70585 2️⃣.2️⃣ 城市信息特征提取 从邮编中提取城市信息，因为是德国的数据，所以参考德国的邮编，相当于加入了先验知识 12data['city'] = data['regionCode'].apply(lambda x : str(x)[:-3])data['city'] 0 4 1 3 2 4 3 1 4 3 .. 112975 3 112976 1 112977 3 112978 1 112979 3 Name: city, Length: 112980, dtype: object 2️⃣.3️⃣ 品牌特征提取 计算某品牌的销售统计量，这里要以 train 的数据计算统计量。 12345678910111213141516train_gb = train.groupby("brand")all_info = {}for kind, kind_data in train_gb: info = {} kind_data = kind_data[kind_data['price'] > 0] # kind_data['price'] > 0 返回的是下标再取一次列表就得到了数据 info['brand_amount'] = len(kind_data) info['brand_price_max'] = kind_data.price.max() info['brand_price_median'] = kind_data.price.median() info['brand_price_min'] = kind_data.price.min() info['brand_price_sum'] = kind_data.price.sum() info['brand_price_std'] = kind_data.price.std() info['brand_price_average'] = round(kind_data.price.sum() / (len(kind_data) + 1), 2) all_info[kind] = infobrand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={"index": "brand"})data = data.merge(brand_fe, how='left', on='brand')data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SaleID name regDate model brand bodyType fuelType gearbox power kilometer ... train used_time city brand_amount brand_price_max brand_price_median brand_price_min brand_price_sum brand_price_std brand_price_average 0 1 2262 20030301 40.0 1 2.0 0.0 0.0 0 15.0 ... 1 4757.0 4 4940.0 9500.0 2999.0 149.0 17934852.0 2537.956443 3629.80 1 5 137642 20090602 24.0 10 0.0 1.0 0.0 109 10.0 ... 1 2482.0 3 3557.0 9500.0 2490.0 200.0 10936962.0 2180.881827 3073.91 2 7 165346 19990706 26.0 14 1.0 0.0 0.0 101 15.0 ... 1 6108.0 4 8784.0 9500.0 1350.0 13.0 17445064.0 1797.704405 1985.78 3 10 18961 20050811 19.0 9 3.0 1.0 0.0 101 15.0 ... 1 3874.0 1 4487.0 9500.0 1250.0 55.0 7867901.0 1556.621159 1753.10 4 13 8129 20041110 65.0 1 0.0 0.0 0.0 150 15.0 ... 1 4154.0 3 4940.0 9500.0 2999.0 149.0 17934852.0 2537.956443 3629.80 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 112975 199995 20903 19960503 4.0 4 4.0 0.0 0.0 116 15.0 ... 0 7261.0 3 6368.0 9500.0 3000.0 150.0 24046576.0 2558.650243 3775.57 112976 199996 708 19991011 0.0 0 0.0 0.0 0.0 75 15.0 ... 0 6014.0 1 16371.0 9500.0 2150.0 50.0 46735356.0 2276.755156 2854.59 112977 199997 6693 20040412 49.0 1 0.0 1.0 1.0 224 15.0 ... 0 4345.0 3 4940.0 9500.0 2999.0 149.0 17934852.0 2537.956443 3629.80 112978 199998 96900 20020008 27.0 1 0.0 0.0 1.0 334 15.0 ... 0 NaN 1 4940.0 9500.0 2999.0 149.0 17934852.0 2537.956443 3629.80 112979 199999 193384 20041109 166.0 6 1.0 NaN 1.0 68 9.0 ... 0 4151.0 3 5778.0 9500.0 1400.0 50.0 11955982.0 1871.933447 2068.87 112980 rows × 41 columns 1brand_fe .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } brand brand_amount brand_price_max brand_price_median brand_price_min brand_price_sum brand_price_std brand_price_average 0 0 16371.0 9500.0 2150.0 50.0 46735356.0 2276.755156 2854.59 1 1 4940.0 9500.0 2999.0 149.0 17934852.0 2537.956443 3629.80 2 3 665.0 9500.0 2800.0 99.0 2158773.0 2058.532395 3241.40 3 4 6368.0 9500.0 3000.0 150.0 24046576.0 2558.650243 3775.57 4 5 2842.0 9500.0 1850.0 75.0 6562224.0 1738.415572 2308.20 5 6 5778.0 9500.0 1400.0 50.0 11955982.0 1871.933447 2068.87 6 7 1035.0 9500.0 1500.0 100.0 2372550.0 2071.320262 2290.11 7 8 705.0 9500.0 1100.0 125.0 1077211.0 1318.748474 1525.79 8 9 4487.0 9500.0 1250.0 55.0 7867901.0 1556.621159 1753.10 9 10 3557.0 9500.0 2490.0 200.0 10936962.0 2180.881827 3073.91 10 11 1390.0 9500.0 1750.0 50.0 3513591.0 2151.572044 2525.95 11 12 549.0 9500.0 1850.0 100.0 1413264.0 2091.218447 2569.57 12 13 1689.0 8950.0 1250.0 25.0 2832005.0 1363.018568 1675.74 13 14 8784.0 9500.0 1350.0 13.0 17445064.0 1797.704405 1985.78 14 15 389.0 9500.0 5700.0 1800.0 2247357.0 1795.404288 5762.45 15 16 291.0 8900.0 1950.0 300.0 636703.0 1223.490908 2180.49 16 17 542.0 9500.0 1970.0 150.0 1444129.0 2136.402905 2659.54 17 18 66.0 8990.0 1650.0 150.0 167360.0 2514.210817 2497.91 18 19 341.0 9100.0 1200.0 130.0 540335.0 1337.203100 1579.93 19 20 514.0 8150.0 1200.0 100.0 818973.0 1276.623577 1590.24 20 21 527.0 8900.0 1890.0 99.0 1285258.0 1832.524896 2434.20 21 22 222.0 9300.0 1925.0 190.0 592296.0 2118.280894 2656.04 22 23 68.0 9500.0 1194.5 100.0 110253.0 1754.883573 1597.87 23 24 4.0 8600.0 7550.0 5999.0 29699.0 1072.435041 5939.80 24 25 735.0 9500.0 1500.0 100.0 1725999.0 2152.726491 2345.11 25 26 121.0 9500.0 2699.0 300.0 417260.0 2563.586943 3420.16 3️⃣ 树模型的数据分桶 数据分箱（也称为离散分箱或分段）是一种数据预处理技术，用于减少次要观察误差的影响，是一种将多个连续值分组为较少数量的“分箱”的方法。例如我们有各个年龄的数据的统计值，可以分成某个段的年龄的值。 离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展； 离散后的特征对异常值更具鲁棒性，如 age>30 为 1 否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰； LR 属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达能力，加大拟合； 离散后特征可以进行特征交叉，提升表达能力，由 M+N 个变量变成 M*N 个变量，进一步引入非线形，提升了表达能力； 特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化 下面以power为例子，做一次数据分桶 123bin = [i*10 for i in range(31)]data['power_bin'] = pd.cut(data['power'], bin, labels=False)data[['power_bin', 'power']] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } power_bin power 0 NaN 0 1 10.0 109 2 10.0 101 3 10.0 101 4 14.0 150 ... ... ... 112975 11.0 116 112976 7.0 75 112977 22.0 224 112978 NaN 334 112979 6.0 68 112980 rows × 2 columns 可以看出这个分箱的作用就是将同一个区间段的功率值设为同样的值，比如101~109都设置为10.0。然后就可以删除掉原数据了： 1data = data.drop(['creatDate', 'regDate', 'regionCode'], axis=1) 12print(data.shape)data.columns (112980, 39) Index(['SaleID', 'name', 'model', 'brand', 'bodyType', 'fuelType', 'gearbox', 'power', 'kilometer', 'notRepairedDamage', 'seller', 'offerType', 'price', 'v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13', 'v_14', 'train', 'used_time', 'city', 'brand_amount', 'brand_price_max', 'brand_price_median', 'brand_price_min', 'brand_price_sum', 'brand_price_std', 'brand_price_average', 'power_bin'], dtype='object') 至此，可以导出给树模型用的数据 1data.to_csv('data_for_tree.csv', index=0) 4️⃣ LR与NN模型的特征构造 上面的步骤就是一次比较完备的特征构造，我们还可以为其他模型构造特征，主要是由于不用模型需要的数据输入是不同的。 4️⃣.1️⃣ $log$与归一化观察一下数据分布 1data['power'].plot.hist() 再看看train数据集的分布： 1train['power'].plot.hist() 我们对其取 log，再做归一化 123data['power'] = np.log(data['power'] + 1) data['power'] = ((data['power'] - np.min(data['power'])) / (np.max(data['power']) - np.min(data['power'])))data['power'].plot.hist() 看看行驶里程的情况，应该是原始数据已经分好了桶 1data['kilometer'].plot.hist() 归一化 123data['kilometer'] = ((data['kilometer'] - np.min(data['kilometer'])) / (np.max(data['kilometer']) - np.min(data['kilometer'])))data['kilometer'].plot.hist() 对刚刚构造的统计量进行归一化 1234def max_min(x): return (x - np.min(x)) / (np.max(x) - np.min(x))data.columns[-10:] Index(['used_time', 'city', 'brand_amount', 'brand_price_max', 'brand_price_median', 'brand_price_min', 'brand_price_sum', 'brand_price_std', 'brand_price_average', 'power_bin'], dtype='object') 123for i in data.columns[-10:]: if np.min(data[i]) != '': # 存在空值的情况 data[i] = max_min(data[i]) 4️⃣.2️⃣ $OneEncoder$编码 对类别特征进行$OneEncoder$在此之前先介绍一下$OneEncoder$编码:$one-hot$的基本思想，将离散型特征的每一种取值都看成一种状态，若你的这一特征中有$N$个不相同的取值，那么我们就可以将该特征抽象成$N$种不同的状态，$one-hot$编码保证了每一个取值只会使得一种状态处于“激活态”，也就是说这N种状态中只有一个状态位值为1，其他状态位都是0。举个例子，假设我们以学历为例，我们想要研究的类别为小学、中学、大学、硕士、博士五种类别，我们使用$one-hot$对其编码就会得到： $dummy \quad encoding$ 哑变量编码直观的解释就是任意的将一个状态位去除。还是拿上面的例子来说，我们用4个状态位就足够反应上述5个类别的信息，也就是我们仅仅使用前四个状态位 [0,0,0,0] 就可以表达博士了。只是因为对于一个我们研究的样本，他已不是小学生、也不是中学生、也不是大学生、又不是研究生，那么我们就可以默认他是博士，是不是。所以，我们用哑变量编码可以将上述5类表示成： 123data = pd.get_dummies(data, columns=['model', 'brand', 'bodyType', 'fuelType', 'gearbox', 'notRepairedDamage', 'power_bin'])data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SaleID name power kilometer seller offerType price v_0 v_1 v_2 ... power_bin_0.6896551724137931 power_bin_0.7241379310344828 power_bin_0.7586206896551724 power_bin_0.7931034482758621 power_bin_0.8275862068965517 power_bin_0.8620689655172413 power_bin_0.896551724137931 power_bin_0.9310344827586207 power_bin_0.9655172413793104 power_bin_1.0 0 1 2262 0.000000 1.000000 0 0 3600.0 45.305273 5.236112 0.137925 ... 0 0 0 0 0 0 0 0 0 0 1 5 137642 0.474626 0.655172 0 0 8000.0 46.323165 -3.229285 0.156615 ... 0 0 0 0 0 0 0 0 0 0 2 7 165346 0.467002 1.000000 0 0 1000.0 42.255586 -3.167771 -0.676693 ... 0 0 0 0 0 0 0 0 0 0 3 10 18961 0.467002 1.000000 0 0 3100.0 45.401241 4.195311 -0.370513 ... 0 0 0 0 0 0 0 0 0 0 4 13 8129 0.506615 1.000000 0 0 3100.0 46.844574 4.175332 0.490609 ... 0 0 0 0 0 0 0 0 0 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 112975 199995 20903 0.480856 1.000000 0 0 NaN 45.621391 5.958453 -0.918571 ... 0 0 0 0 0 0 0 0 0 0 112976 199996 708 0.437292 1.000000 0 0 NaN 43.935162 4.476841 -0.841710 ... 0 0 0 0 0 0 0 0 0 0 112977 199997 6693 0.546885 1.000000 0 0 NaN 46.537137 4.170806 0.388595 ... 0 0 1 0 0 0 0 0 0 0 112978 199998 96900 0.587076 1.000000 0 0 NaN 46.771359 -3.296814 0.243566 ... 0 0 0 0 0 0 0 0 0 0 112979 199999 193384 0.427535 0.586207 0 0 NaN 43.731010 -3.121867 0.027348 ... 0 0 0 0 0 0 0 0 0 0 112980 rows × 369 columns 将这份数据输出给LR模型使用 1data.to_csv('data_for_lr.csv', index=0) 5️⃣ 特征选择5️⃣.1️⃣ 过滤式(filter)相关性分析 123456print(data['power'].corr(data['price'], method='spearman'))print(data['kilometer'].corr(data['price'], method='spearman'))print(data['brand_amount'].corr(data['price'], method='spearman'))print(data['brand_price_average'].corr(data['price'], method='spearman'))print(data['brand_price_max'].corr(data['price'], method='spearman'))print(data['brand_price_median'].corr(data['price'], method='spearman')) 0.4698539569820024 -0.19974282513118508 0.04085800320025127 0.3135239590412946 0.07894119089254827 0.3138873049004745 可以看出power，brand_price_average，brand_price_median与price相关性比较高 1234567data_numeric = data[['power', 'kilometer', 'brand_amount', 'brand_price_average', 'brand_price_max', 'brand_price_median']]correlation = data_numeric.corr()f , ax = plt.subplots(figsize = (7, 7))plt.title('Correlation of Numeric Features with Price',y=1,size=30)sns.heatmap(correlation, square = True, cmap = 'PuBuGn', vmax=0.8) 看不出来啥。😛 5️⃣.2️⃣ 包裹式(wrapper)1!pip install mlxtend Collecting mlxtend Downloading https://files.pythonhosted.org/packages/64/e2/1610a86284029abcad0ac9bc86cb19f9787fe6448ede467188b2a5121bb4/mlxtend-0.17.2-py2.py3-none-any.whl (1.3MB) Requirement already satisfied: setuptools in d:\software\anaconda\lib\site-packages (from mlxtend) (40.8.0) Requirement already satisfied: pandas>=0.24.2 in d:\software\anaconda\lib\site-packages (from mlxtend) (0.25.1) Requirement already satisfied: scipy>=1.2.1 in d:\software\anaconda\lib\site-packages (from mlxtend) (1.2.1) Requirement already satisfied: matplotlib>=3.0.0 in d:\software\anaconda\lib\site-packages (from mlxtend) (3.0.3) Requirement already satisfied: numpy>=1.16.2 in d:\software\anaconda\lib\site-packages (from mlxtend) (1.16.2) Collecting joblib>=0.13.2 (from mlxtend) Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB) Requirement already satisfied: scikit-learn>=0.20.3 in d:\software\anaconda\lib\site-packages (from mlxtend) (0.20.3) Requirement already satisfied: pytz>=2017.2 in d:\software\anaconda\lib\site-packages (from pandas>=0.24.2->mlxtend) (2018.9) Requirement already satisfied: python-dateutil>=2.6.1 in d:\software\anaconda\lib\site-packages (from pandas>=0.24.2->mlxtend) (2.8.0) Requirement already satisfied: cycler>=0.10 in d:\software\anaconda\lib\site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0) Requirement already satisfied: kiwisolver>=1.0.1 in d:\software\anaconda\lib\site-packages (from matplotlib>=3.0.0->mlxtend) (1.0.1) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in d:\software\anaconda\lib\site-packages (from matplotlib>=3.0.0->mlxtend) (2.3.1) Requirement already satisfied: six>=1.5 in d:\software\anaconda\lib\site-packages (from python-dateutil>=2.6.1->pandas>=0.24.2->mlxtend) (1.12.0) Installing collected packages: joblib, mlxtend Successfully installed joblib-0.14.1 mlxtend-0.17.2 1x .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SaleID name power kilometer seller offerType v_0 v_1 v_2 v_3 ... power_bin_0.6896551724137931 power_bin_0.7241379310344828 power_bin_0.7586206896551724 power_bin_0.7931034482758621 power_bin_0.8275862068965517 power_bin_0.8620689655172413 power_bin_0.896551724137931 power_bin_0.9310344827586207 power_bin_0.9655172413793104 power_bin_1.0 0 1 2262 0.000000 1.000000 0 0 45.305273 5.236112 0.137925 1.380657 ... 0 0 0 0 0 0 0 0 0 0 1 5 137642 0.474626 0.655172 0 0 46.323165 -3.229285 0.156615 -1.727217 ... 0 0 0 0 0 0 0 0 0 0 2 7 165346 0.467002 1.000000 0 0 42.255586 -3.167771 -0.676693 1.942673 ... 0 0 0 0 0 0 0 0 0 0 3 10 18961 0.467002 1.000000 0 0 45.401241 4.195311 -0.370513 0.444251 ... 0 0 0 0 0 0 0 0 0 0 4 13 8129 0.506615 1.000000 0 0 46.844574 4.175332 0.490609 0.085718 ... 0 0 0 0 0 0 0 0 0 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 112975 199995 20903 0.480856 1.000000 0 0 45.621391 5.958453 -0.918571 0.774826 ... 0 0 0 0 0 0 0 0 0 0 112976 199996 708 0.437292 1.000000 0 0 43.935162 4.476841 -0.841710 1.328253 ... 0 0 0 0 0 0 0 0 0 0 112977 199997 6693 0.546885 1.000000 0 0 46.537137 4.170806 0.388595 -0.704689 ... 0 0 1 0 0 0 0 0 0 0 112978 199998 96900 0.587076 1.000000 0 0 46.771359 -3.296814 0.243566 -1.277411 ... 0 0 0 0 0 0 0 0 0 0 112979 199999 193384 0.427535 0.586207 0 0 43.731010 -3.121867 0.027348 -0.808914 ... 0 0 0 0 0 0 0 0 0 0 112980 rows × 368 columns 123456789101112131415from mlxtend.feature_selection import SequentialFeatureSelector as SFSfrom sklearn.linear_model import LinearRegressionsfs = SFS(LinearRegression(), k_features=10, forward=True, floating=False, scoring = 'r2', cv = 0)x = data.drop(['price'], axis=1)x = x.fillna(0)y = data['price']x.dropna(axis=0, how='any', inplace=True)y.dropna(axis=0, how='any', inplace=True)sfs.fit(x, y)sfs.k_feature_names_ 画出来，可以看到边际效益 12345from mlxtend.plotting import plot_sequential_feature_selection as plot_sfsimport matplotlib.pyplot as pltfig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev')plt.grid()plt.show() 5️⃣.3️⃣ 嵌入式（embedding）Lasso 回归和决策树可以完成嵌入式特征选择，大部分情况下都是用嵌入式做特征筛选。 下一步就是建模了。🤔 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>Editor</tag>
        <tag>DataMining</tag>
        <tag>Tianchi</tag>
        <tag>Study</tag>
        <tag>Jupyter</tag>
        <tag>seaborn</tag>
        <tag>Onehot</tag>
        <tag>BoxPlot</tag>
        <tag>IQR</tag>
        <tag>groupby</tag>
        <tag>过滤式(filter)</tag>
        <tag>包裹式(wrapper)</tag>
        <tag>嵌入式（embedding）</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[完美解决：Hexo Next主题本地可预览CSS，但部署到网站CSS失效问题!]]></title>
    <url>%2F%E5%AE%8C%E7%BE%8E%E8%A7%A3%E5%86%B3%EF%BC%9AHexo-Next%E4%B8%BB%E9%A2%98%E6%9C%AC%E5%9C%B0%E5%8F%AF%E9%A2%84%E8%A7%88CSS%EF%BC%8C%E4%BD%86%E9%83%A8%E7%BD%B2%E5%88%B0%E7%BD%91%E7%AB%99CSS%E5%A4%B1%E6%95%88%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[非常折磨人，好在我暂时解决它了。😂 code { color: #FF1493 !important; font-weight: 800 !important;} 🌟 前言 我的Hexo版本是3.9.0，Next主题版本是7.5版本，也就是移除了custom文件的神奇的跨时代的版本。 我的服务器不是github-page，而是阿里云的ECS服务器，关于如何将Hexo从github-page迁移到阿里云的ECS服务器请看这篇文章：将博客部署到阿里云服务器上。 当然一开始觉得这个版本好搓卡，本来想改改样式，不会CSS，上网搜搜就有很多改custom文件的文章，复制复制就可以改成很好看的样式，这下好了，一移除就全部失效了。 其实不然，在与Next的大坑中摸爬滚打了很久后， 发现，你可以在themes/next/source/css/main.styl中最后加上一句：1@import "_custom/custom"; 然后再在themes/next/source/css目录下新建_custom文件夹，再进去新建custom.styl文件，将网上搜罗到的Next主题的文件都粘贴进去，就可以在本地预览这些新添加的样式了。 🙆当然你也许不需要这么麻烦的操作，你甚至可以在themes/next/source/css文件夹中的任何一个.styl文件添加你想要的css样式代码都可以在本地预览中生效。 我想其中的原因在于：主题调用的文件主要来自于themes/next/source/css/main.styl，而这个文件里面全是import语句，即将所有的css文件import进来，也就意味着最后生成的整体的main.css文件不过是将所有的css分文件中的语句按顺序排列罢了，所以你加在哪个文件改变的不过是最后的main.css的语句顺序罢了，但是其提供的效果依然生效。但为了日后修改方便，还是建议找对应的位置添加。 1️⃣ 问题陈述 前言中我也提到了“本地预览生效”的话，意味着，你大可自己定义css样式，也可以将网上的内容复制粘贴，但有一点非常头疼，那就是大多数情况下，你只能成功地进行本地预览，而一旦deploy到服务器上要么就是完全无效，要么就是稀奇古怪，甚至有时候你即使将整个/css文件夹删除，发现deploy后的网站样式完全没有变化。😢 紧接着通过在部署后的页面以及本地预览的页面分别进行F12调试，逐一对比，终于发现了不一样的地方。本地预览时调试页面的/css的文件下的文件名为main.css就跟hexo g生成在/public文件夹下是一模一样的，但是到了部署页面中这个文件名就变为了main.css?v=7.3.0，这多出来的?v=7.3.0百思不得其解。再看看main.css中的文件内容跟我pc里面的/public/css/main.css里面的东西一模一样，但是main.css?v=7.3.0里面莫名其妙的少了几百行，原以为是hexo deploy命令部署的不全，漏了东西，但上阿里云的服务器文件夹里面一看内容跟我本地的一样。并且当我将main.css里面的东西复制到main.css?v=7.3.0时，我想要的部署的页面就跟我本地预览终于一样了，虽然刷新一下就没了，毕竟是网页调试。 那么问题就很清楚了，就是这个main.css?v=7.3.0并不放在服务器端，调用的源头也并不明朗，而且是无法更改的，所以得想办法让部署的页面加载main.css而不是main.css?v=7.3.0。 2️⃣ 问题解决 既然知道问题出在哪里就很简单了，费了一番功夫，终于发现在themes/next/layout/_partials/head/head.swig文件中，有一行语句是这样的： 1&#60link rel="stylesheet" href="{{ url_for(theme.css) }}/main.css?v={{ version }}"&#62 很明显之前多出来的?v=7.3.0就是出自于这里的?v=，所以就把这里的?v=删除，就可以了。 然后再hexo clean && hexo g && hexo d，查看部署端页面，样式齐全完美！问题解决。👍👍👍 3️⃣ 一些缺点 这个方法可以完美地解决问题本身，而且绝对不会再出现本地预览与部署端不一样的问题，但是会出现副作用。 当我以为终于可以愉快地肆无忌惮地玩耍时，又发现一个新的问题。就是我又再次改造了css样式，即在/_custom/custom文件中加入了一些样式，再hexo d发现样式没有变化。再调试发现问题，main.css文件没有变化。思考一下，猜测这个原因应该跟main.css?v=7.3.0问题是一样的，它本身是不可更改的，即使再hexo d新的css文件，其本身不会变化。 问题的解决方法就是将themes/next/layout/_partials/head/head.swig中的12- &#60link rel="stylesheet" href="{{ url_for(theme.css) }}/main.css"&#62+ &#60link rel="stylesheet" href="{{ url_for(theme.css) }}/main1.css"&#62 也就是改成其他名字main1也好main2也好，就是改成跟原来不用的名字。然后继续hexo g就会将生成的那些博客文章页面里面的引用的css文件名改为main1.css文件。 同时还要将/public/css中的main.css改为main1.css，最后hexo d，发现改动的css样式也生效了。 当然这也就意味着，以后每次改动css样式，都要将main.css改成新的名字，如main2.css、main3.css……。 建议hexo clean && hexo g && hexo d之前，先备份一下/public文件夹，保留可以回退版本的可能。 所以建议要么不改css，要么一次性改全。毕竟写Hexo博客，重要的不是好看，而是内容，不是嘛？😉 4️⃣ 总结 删除themes/next/layout/_partials/head/head.swig中的main.css?v=后面的?v= 每次修改css后，hexo d之前，改造themes/next/layout/_partials/head/head.swig中的main.css的名字，如main2.css、main3.css……。 建议hexo clean && hexo g && hexo d之前，先备份一下/public文件夹，保留可以回退版本的可能。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>css</tag>
        <tag>博客</tag>
        <tag>Next</tag>
        <tag>custom</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天池二手车价格预测Task1-2—赛题理解与数据分析]]></title>
    <url>%2F%E5%A4%A9%E6%B1%A0-%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B-Task1-2-%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[惊闻4.13日江苏省高校将要启动开学模式，我自岿然不动。山中何事？松花酿酒，春水煎茶，如是而已。 1️⃣ 赛题理解✍️1️⃣.1️⃣ 赛题重述 这是一道来自于天池的新手练习题目，用数据分析、机器学习等手段进行 二手车售卖价格预测 的回归问题。赛题本身的思路清晰明了，即对给定的数据集进行分析探讨，然后设计模型运用数据进行训练，测试模型，最终给出选手的预测结果。 1️⃣.2️⃣ 数据集概述 赛题官方给出了来自Ebay Kleinanzeigen的二手车交易记录，总数据量超过40w，包含31列变量信息，其中15列为匿名变量，即v0至v15。并从中抽取15万条作为训练集，5万条作为测试集A，5万条作为测试集B，同时对name、model、brand和regionCode等信息进行脱敏。具体的数据表如下图： Field Description SaleID 交易ID，唯一编码 name 汽车交易名称，已脱敏 regDate 汽车注册日期，例如20160101，2016年01月01日 model 车型编码，已脱敏 brand 汽车品牌，已脱敏 bodyType 车身类型：豪华轿车：0，微型车：1，厢型车：2，大巴车：3，敞篷车：4，双门汽车：5，商务车：6，搅拌车：7 fuelType 燃油类型：汽油：0，柴油：1，液化石油气：2，天然气：3，混合动力：4，其他：5，电动：6 gearbox 变速箱：手动：0，自动：1 power 发动机功率：范围 [ 0, 600 ] kilometer 汽车已行驶公里，单位万km notRepairedDamage 汽车有尚未修复的损坏：是：0，否：1 regionCode 地区编码，已脱敏 seller 销售方：个体：0，非个体：1 offerType 报价类型：提供：0，请求：1 creatDate 汽车上线时间，即开始售卖时间 price 二手车交易价格（预测目标） v系列特征 匿名特征，包含v0-14在内15个匿名特征 思考💭💡 指标重要性 数据集里面包含的很多维度的数据，对于人来说第一眼看上去就会产生直观的感觉，哪些指标对售价的影响大，哪些指标对售价的影响小，特别是对于一个长期从事二手车交易的人来说，更是如此。例如 kilometer(汽车已行驶公里)肯定是对于成交价格的影响是巨大的。但是如何让我设计的模型认知到这些先验知识是个棘手的问题，但我想这应该时一个很旧的问题，只是我还没有足够的知识去通晓它解决的肌理。确实，对于机器来说，这些数据只是一列列的向量，所以首要解决的就是向量的重要性。 简单思维 简单地假设（我相信所有人都会想到的easy思路🤪），所有的变量跟预测目标成交价格是simple的线性关系，列一个包含31个自变量的线性函数，用批量梯度下降法拟合出31个自变量系数，然后用正则化解决过拟合问题。 它的假设函数是这样的： h_{\theta}(x)=\theta^{T} X=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\ldots+\theta_{31} x_{31} 它的带有正则化的代价函数是这样的： J(\theta)=\frac{1}{62} \sum_{i=1}^{31}\left[\left(\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{31} \theta_{j}^{2}\right)\right] 1️⃣.3️⃣ 预测结果评价指标⚒️ 赛题的预测评估指标为$MAE(Mean Absolute Error)$ MAE=\frac{\sum\limits_{i=1}^{n}\left|y_{i}-\hat{y}_{i}\right|}{n} 可以看出，指标就一个，没有很多维度的评价框架，不那么劝退。🤔 2️⃣ 数据分析EDA📊 EDA的价值主要在于熟悉数据集，了解数据集，对数据集进行验证来确定所获得数据集可以用于接下来的机器学习或者深度学习使用。 当了解了数据集之后我们下一步就是要去了解变量间的相互关系以及变量与预测值之间的存在关系。 引导数据科学从业者进行数据处理以及特征工程的步骤,使数据集的结构和特征集让接下来的预测问题更加可靠。 完成对于数据的探索性分析，并对于数据进行一些图表或者文字总结并打卡。 当然这一步也要就解决我在 1️⃣.2️⃣ 中提出的第一个思考，能否通过探索性分析，发掘指标之间的关系，从而为模型内联性地定义出各指标的对成交价格的强弱相关性。但是EDA分析涉及的范围太大，可视化的东西很多，但是如果在后续的分析中不进行运用就是多余的工作，所以只需要挑选最重要的几个因素进行分析，具体如下： 数据总览，即describe()统计量以及info()数据类型 缺失值以及异常值检测 分析待预测的真实值的分布 特征之间的相关性分析 2️⃣.1️⃣ 数据总览2️⃣.1️⃣.1️⃣ 各种计算包的导入12345import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as sns # seabon是一个做可视化非常nice的包，它的别名sns是约定俗成的的东西，还有一段很有意思的故事import missingno as msno # 用来检测缺失值 2️⃣.1️⃣.1️⃣ 数据载入12Train_data = pd.read_csv('used_car_train_20200313.csv', sep=' ')Test_data = pd.read_csv('used_car_testA_20200313.csv', sep=' ') 2️⃣.1️⃣.2️⃣ 数据的基本形态 训练集的长相 12Train_data.head()Train_data.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SaleID name regDate model brand bodyType fuelType gearbox power kilometer ... v_5 v_6 v_7 v_8 v_9 v_10 v_11 v_12 v_13 v_14 149995 149995 163978 20000607 121.0 10 4.0 0.0 1.0 163 15.0 ... 0.280264 0.000310 0.048441 0.071158 0.019174 1.988114 -2.983973 0.589167 -1.304370 -0.302592 149996 149996 184535 20091102 116.0 11 0.0 0.0 0.0 125 10.0 ... 0.253217 0.000777 0.084079 0.099681 0.079371 1.839166 -2.774615 2.553994 0.924196 -0.272160 149997 149997 147587 20101003 60.0 11 1.0 1.0 0.0 90 6.0 ... 0.233353 0.000705 0.118872 0.100118 0.097914 2.439812 -1.630677 2.290197 1.891922 0.414931 149998 149998 45907 20060312 34.0 10 3.0 1.0 0.0 156 15.0 ... 0.256369 0.000252 0.081479 0.083558 0.081498 2.075380 -2.633719 1.414937 0.431981 -1.659014 149999 149999 177672 19990204 19.0 28 6.0 0.0 1.0 193 12.5 ... 0.284475 0.000000 0.040072 0.062543 0.025819 1.978453 -3.179913 0.031724 -1.483350 -0.342674 5 rows × 31 columns 1Train_data.shape (150000, 31) 测试集的长相 12Test_data.head()Test_data.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SaleID name regDate model brand bodyType fuelType gearbox power kilometer ... v_5 v_6 v_7 v_8 v_9 v_10 v_11 v_12 v_13 v_14 49995 199995 20903 19960503 4.0 4 4.0 0.0 0.0 116 15.0 ... 0.284664 0.130044 0.049833 0.028807 0.004616 -5.978511 1.303174 -1.207191 -1.981240 -0.357695 49996 199996 708 19991011 0.0 0 0.0 0.0 0.0 75 15.0 ... 0.268101 0.108095 0.066039 0.025468 0.025971 -3.913825 1.759524 -2.075658 -1.154847 0.169073 49997 199997 6693 20040412 49.0 1 0.0 1.0 1.0 224 15.0 ... 0.269432 0.105724 0.117652 0.057479 0.015669 -4.639065 0.654713 1.137756 -1.390531 0.254420 49998 199998 96900 20020008 27.0 1 0.0 0.0 1.0 334 15.0 ... 0.261152 0.000490 0.137366 0.086216 0.051383 1.833504 -2.828687 2.465630 -0.911682 -2.057353 49999 199999 193384 20041109 166.0 6 1.0 NaN 1.0 68 9.0 ... 0.228730 0.000300 0.103534 0.080625 0.124264 2.914571 -1.135270 0.547628 2.094057 -1.552150 5 rows × 30 columns 1Test_data.shape (50000, 30) 可以看出，数据的分散程度很大，有整型，有浮点，有正数，有负数，还有日期，当然可以当成是字符串。另外如果数据都换算成数值的话，数据间差距特别大，有些成千上万，有些几分几厘，这样在预测时就难以避免地会忽视某些值的作用，所以需要对其进行归一化。 shape的运用是也十分重要，对数据的大小要心中有数 用describe()来对数据进行基本统计量的分析，关于describe()的基本参数如下（且其默认只对数值型数据进行分析，如果有字符串，时间序列等的数据，会减少统计的项目）： count：一列的元素个数； mean：一列数据的平均值； std：一列数据的均方差；（方差的算术平方根，反映一个数据集的离散程度：越大，数据间的差异越大，数据集中数据的离散程度越高；越小，数据间的大小差异越小，数据集中的数据离散程度越低） min：一列数据中的最小值； max：一列数中的最大值； 25%：一列数据中，前 25% 的数据的平均值； 50%：一列数据中，前 50% 的数据的平均值； 75%：一列数据中，前 75% 的数据的平均值； 用info()来查看数据类型，并主要查看是否有异常数据 1Train_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SaleID name regDate model brand bodyType fuelType gearbox power kilometer ... v_5 v_6 v_7 v_8 v_9 v_10 v_11 v_12 v_13 v_14 count 150000.000000 150000.000000 1.500000e+05 149999.000000 150000.000000 145494.000000 141320.000000 144019.000000 150000.000000 150000.000000 ... 150000.000000 150000.000000 150000.000000 150000.000000 150000.000000 150000.000000 150000.000000 150000.000000 150000.000000 150000.000000 mean 74999.500000 68349.172873 2.003417e+07 47.129021 8.052733 1.792369 0.375842 0.224943 119.316547 12.597160 ... 0.248204 0.044923 0.124692 0.058144 0.061996 -0.001000 0.009035 0.004813 0.000313 -0.000688 std 43301.414527 61103.875095 5.364988e+04 49.536040 7.864956 1.760640 0.548677 0.417546 177.168419 3.919576 ... 0.045804 0.051743 0.201410 0.029186 0.035692 3.772386 3.286071 2.517478 1.288988 1.038685 min 0.000000 0.000000 1.991000e+07 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.500000 ... 0.000000 0.000000 0.000000 0.000000 0.000000 -9.168192 -5.558207 -9.639552 -4.153899 -6.546556 25% 37499.750000 11156.000000 1.999091e+07 10.000000 1.000000 0.000000 0.000000 0.000000 75.000000 12.500000 ... 0.243615 0.000038 0.062474 0.035334 0.033930 -3.722303 -1.951543 -1.871846 -1.057789 -0.437034 50% 74999.500000 51638.000000 2.003091e+07 30.000000 6.000000 1.000000 0.000000 0.000000 110.000000 15.000000 ... 0.257798 0.000812 0.095866 0.057014 0.058484 1.624076 -0.358053 -0.130753 -0.036245 0.141246 75% 112499.250000 118841.250000 2.007111e+07 66.000000 13.000000 3.000000 1.000000 0.000000 150.000000 15.000000 ... 0.265297 0.102009 0.125243 0.079382 0.087491 2.844357 1.255022 1.776933 0.942813 0.680378 max 149999.000000 196812.000000 2.015121e+07 247.000000 39.000000 7.000000 6.000000 1.000000 19312.000000 15.000000 ... 0.291838 0.151420 1.404936 0.160791 0.222787 12.357011 18.819042 13.847792 11.147669 8.658418 8 rows × 30 columns 1Test_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SaleID name regDate model brand bodyType fuelType gearbox power kilometer ... v_5 v_6 v_7 v_8 v_9 v_10 v_11 v_12 v_13 v_14 count 50000.000000 50000.000000 5.000000e+04 50000.000000 50000.000000 48587.000000 47107.000000 48090.000000 50000.000000 50000.000000 ... 50000.000000 50000.000000 50000.000000 50000.000000 50000.000000 50000.000000 50000.000000 50000.000000 50000.000000 50000.000000 mean 174999.500000 68542.223280 2.003393e+07 46.844520 8.056240 1.782185 0.373405 0.224350 119.883620 12.595580 ... 0.248669 0.045021 0.122744 0.057997 0.062000 -0.017855 -0.013742 -0.013554 -0.003147 0.001516 std 14433.901067 61052.808133 5.368870e+04 49.469548 7.819477 1.760736 0.546442 0.417158 185.097387 3.908979 ... 0.044601 0.051766 0.195972 0.029211 0.035653 3.747985 3.231258 2.515962 1.286597 1.027360 min 150000.000000 0.000000 1.991000e+07 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.500000 ... 0.000000 0.000000 0.000000 0.000000 0.000000 -9.160049 -5.411964 -8.916949 -4.123333 -6.112667 25% 162499.750000 11203.500000 1.999091e+07 10.000000 1.000000 0.000000 0.000000 0.000000 75.000000 12.500000 ... 0.243762 0.000044 0.062644 0.035084 0.033714 -3.700121 -1.971325 -1.876703 -1.060428 -0.437920 50% 174999.500000 52248.500000 2.003091e+07 29.000000 6.000000 1.000000 0.000000 0.000000 109.000000 15.000000 ... 0.257877 0.000815 0.095828 0.057084 0.058764 1.613212 -0.355843 -0.142779 -0.035956 0.138799 75% 187499.250000 118856.500000 2.007110e+07 65.000000 13.000000 3.000000 1.000000 0.000000 150.000000 15.000000 ... 0.265328 0.102025 0.125438 0.079077 0.087489 2.832708 1.262914 1.764335 0.941469 0.681163 max 199999.000000 196805.000000 2.015121e+07 246.000000 39.000000 7.000000 6.000000 1.000000 20000.000000 15.000000 ... 0.291618 0.153265 1.358813 0.156355 0.214775 12.338872 18.856218 12.950498 5.913273 2.624622 8 rows × 29 columns 1Train_data.info() RangeIndex: 150000 entries, 0 to 149999 Data columns (total 31 columns): SaleID 150000 non-null int64 name 150000 non-null int64 regDate 150000 non-null int64 model 149999 non-null float64 brand 150000 non-null int64 bodyType 145494 non-null float64 fuelType 141320 non-null float64 gearbox 144019 non-null float64 power 150000 non-null int64 kilometer 150000 non-null float64 notRepairedDamage 150000 non-null object regionCode 150000 non-null int64 seller 150000 non-null int64 offerType 150000 non-null int64 creatDate 150000 non-null int64 price 150000 non-null int64 v_0 150000 non-null float64 v_1 150000 non-null float64 v_2 150000 non-null float64 v_3 150000 non-null float64 v_4 150000 non-null float64 v_5 150000 non-null float64 v_6 150000 non-null float64 v_7 150000 non-null float64 v_8 150000 non-null float64 v_9 150000 non-null float64 v_10 150000 non-null float64 v_11 150000 non-null float64 v_12 150000 non-null float64 v_13 150000 non-null float64 v_14 150000 non-null float64 dtypes: float64(20), int64(10), object(1) memory usage: 35.5+ MB 1Test_data.info() RangeIndex: 50000 entries, 0 to 49999 Data columns (total 30 columns): SaleID 50000 non-null int64 name 50000 non-null int64 regDate 50000 non-null int64 model 50000 non-null float64 brand 50000 non-null int64 bodyType 48587 non-null float64 fuelType 47107 non-null float64 gearbox 48090 non-null float64 power 50000 non-null int64 kilometer 50000 non-null float64 notRepairedDamage 50000 non-null object regionCode 50000 non-null int64 seller 50000 non-null int64 offerType 50000 non-null int64 creatDate 50000 non-null int64 v_0 50000 non-null float64 v_1 50000 non-null float64 v_2 50000 non-null float64 v_3 50000 non-null float64 v_4 50000 non-null float64 v_5 50000 non-null float64 v_6 50000 non-null float64 v_7 50000 non-null float64 v_8 50000 non-null float64 v_9 50000 non-null float64 v_10 50000 non-null float64 v_11 50000 non-null float64 v_12 50000 non-null float64 v_13 50000 non-null float64 v_14 50000 non-null float64 dtypes: float64(20), int64(9), object(1) memory usage: 11.4+ MB 从上面的统计量与信息来看，没有什么特别之处，就数据类型来说notRepairedDamage的类型是object是个另类，后续要进行特殊处理。 2️⃣.2️⃣ 数据的缺失情况📌 pandas内置了isnull()可以用来判断是否有缺失值，它会对空值和NA进行判断然后返回True或False。 1Train_data.isnull().sum() SaleID 0 name 0 regDate 0 model 1 brand 0 bodyType 4506 fuelType 8680 gearbox 5981 power 0 kilometer 0 notRepairedDamage 0 regionCode 0 seller 0 offerType 0 creatDate 0 price 0 v_0 0 v_1 0 v_2 0 v_3 0 v_4 0 v_5 0 v_6 0 v_7 0 v_8 0 v_9 0 v_10 0 v_11 0 v_12 0 v_13 0 v_14 0 dtype: int64 1Test_data.isnull().sum() SaleID 0 name 0 regDate 0 model 0 brand 0 bodyType 1413 fuelType 2893 gearbox 1910 power 0 kilometer 0 notRepairedDamage 0 regionCode 0 seller 0 offerType 0 creatDate 0 v_0 0 v_1 0 v_2 0 v_3 0 v_4 0 v_5 0 v_6 0 v_7 0 v_8 0 v_9 0 v_10 0 v_11 0 v_12 0 v_13 0 v_14 0 dtype: int64 可以看出缺失的数据值主要集中在bodyType，fuelType，gearbox，这三个特征中。训练集中model缺失了一个值，但是无伤大雅。至于如何填充，亦或是删除这些数据，需要后期在选用模型时再做考虑。 同时我们也可以通过missingno库查看缺省值的其他属性。 矩阵图matrix 柱状图bar 热力图heatmap 树状图dendrogram 缺省热力图 热力图表示两个特征之间的缺失相关性，即一个变量的存在或不存在如何强烈影响的另一个的存在。如果x和y的热度值是1，则代表当x缺失时，y也百分之百缺失。如果x和y的热度相关性为-1，说明x缺失的值，那么y没有缺失；而x没有缺失时，y为缺失。至于 矩阵图，与柱状图没有查看的必要，我们可以用缺省热力图观察一下情况： 1msno.heatmap(Train_data.sample(10000)) 1msno.heatmap(Test_data.sample(10000)) 树状图 树形图使用层次聚类算法通过它们的无效性相关性（根据二进制距离测量）将变量彼此相加。在树的每个步骤，基于哪个组合最小化剩余簇的距离来分割变量。变量集越单调，它们的总距离越接近零，并且它们的平均距离（y轴）越接近零。 1msno.dendrogram(Train_data.sample(10000)) 1msno.dendrogram(Test_data.sample(10000)) 由上面的热力图以及聚类图可以看出，各个缺失值之间的相关性并不明显。 2️⃣.3️⃣ 数据的异常情况☢️ 因为之前发现notRepairedDamage的类型是object是个另类，所以看一下它的具体情况。 1Train_data['notRepairedDamage'].value_counts() 0.0 111361 - 24324 1.0 14315 Name: notRepairedDamage, dtype: int64 1Test_data['notRepairedDamage'].value_counts() 0.0 37249 - 8031 1.0 4720 Name: notRepairedDamage, dtype: int64 发现有’-‘的存在，这可以算是NaN的一种，所以可以将其替换为NaN 12Train_data['notRepairedDamage'].replace('-', np.nan, inplace=True)Test_data['notRepairedDamage'].replace('-', np.nan, inplace=True) 2️⃣.4️⃣ 待预测的真实值的分布情况📈 我们先来看看价格预测值的分布情况 1Train_data['price'] 0 1850 1 3600 2 6222 3 2400 4 5200 ... 149995 5900 149996 9500 149997 7500 149998 4999 149999 4700 Name: price, Length: 150000, dtype: int64 1Train_data['price'].value_counts() 500 2337 1500 2158 1200 1922 1000 1850 2500 1821 ... 25321 1 8886 1 8801 1 37920 1 8188 1 Name: price, Length: 3763, dtype: int64 嗯哼，平淡无奇，接下来最重要的是要看一下历史成交价格的偏度（Skewness）与峰度（Kurtosis），此外自然界最优美的分布式正态分布，所以也要看一下待预测的价格分布是否满足正态分布。再解释一下偏度与峰度，一般会拿偏度和峰度来看数据的分布形态，而且一般会跟正态分布做比较，我们把正态分布的偏度和峰度都看做零。如果算到偏度峰度不为0，即表明变量存在左偏右偏，或者是高顶平顶。 偏度（Skewness）是描述数据分布形态的统计量，其描述的是某总体取值分布的对称性，简单来说就是数据的不对称程度。 Skewness = 0 ，分布形态与正态分布偏度相同。 Skewness > 0 ，正偏差数值较大，为正偏或右偏。长尾巴拖在右边，数据右端有较多的极端值。 Skewness < 0 ，负偏差数值较大，为负偏或左偏。长尾巴拖在左边，数据左端有较多的极端值。 数值的绝对值越大，表明数据分布越不对称，偏斜程度大。 计算公式\gamma_{1}=\mathrm{E}\left[\left(\frac{X-\mu}{\sigma}\right)^{3}\right]=\frac{\mu_{3}}{\sigma^{3}}=\frac{\mathrm{E}\left[(X-\mu)^{3}\right]}{\left(\mathrm{E}\left[(X-\mu)^{2}\right]\right)^{3 / 2}}=\frac{\kappa_{3}}{\kappa_{2}^{3 / 2}} 峰度（Kurtosis）偏度是描述某变量所有取值分布形态陡缓程度的统计量，简单来说就是数据分布顶的尖锐程度。 Kurtosis = 0 与正态分布的陡缓程度相同。 Kurtosis > 0 比正态分布的高峰更加陡峭——尖顶峰。 urtosis { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>pandas</tag>
        <tag>Editor</tag>
        <tag>DataMining</tag>
        <tag>Tianchi</tag>
        <tag>Study</tag>
        <tag>Jupyter</tag>
        <tag>seaborn</tag>
        <tag>正态分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[相识]]></title>
    <url>%2F%E7%9B%B8%E8%AF%86.html</url>
    <content type="text"><![CDATA[相识，只是有缘，仅此而已。 此前的文章，已经过去了很久。这段很长的时光经历了很多，认识了很多人，看了很多书，也想通了很多，家里如今也非常和谐，早已抛弃了那些不太合适的想法。只是既然曾经某一瞬，某个深夜有过这样自我拷问的想法，我也无需否定它，仅是记录，也是警醒以后的自己：要思考，但不要自我陷入。 同时，我也想记录下这些。 19.10.30 研会换届，认识了很多人。小万哥哥很逗，刘小伙子很厚道，徐学姐很照顾人，最后偷偷在我耳边说赶紧走，不然留你下来干活。陈学姐总是很高冷，遇见了xsn，她的笑容真的很治愈。我顺便偷偷告诉她赶紧走，本来想一起走，但是，有其他人，唉。。。 19.10.31 累得一塌糊涂的一天，也算是得到了一些收入。 19.11.1 疯狂的一天，计划晚上实验室集体看《天气之子》，下午直接唱了一下午，不过真是几年没唱了，挺爽，好多老歌很触动。晚上的日料气氛很好，两杯生啤太爽了。《天气之子》故事上确实较前作不是那么令人满意，有些强行加上的意思。画面方面当然是无可挑剔，但却没有《君名》那种让人沉浸迷离在那个世界的感觉，《君名》一个小小的日记元素就令人神往，更没有让人重看很多遍的欲望，时至今日我都有点沉浸在《君名》的世界那种美好的爱情的情愫里面。这个无处不在的下雨真是搞得心情很糟，正是因为画的很真实。最后为了拯救她，居然让整个日本淹了。。。 19.11.2 校园马拉松日，9KM很累很爽。遇见了圣楠，好开心。一起吃了烤鸭泡饭，幸福！还能送她回宿舍，太棒了！ 19.11.3 累得迷迷糊糊。。。中午我尝试着推给她我刚听到的一首《careless whisper》的粤语版，她很开心。晚上她推给我《初秋和你》，《多想在平庸的生活拥抱你》，这似乎表明了什么。😃 19.11.4 白天我们聊了村上，圭吾，《白夜行》。晚上她又推给我《你笑起来真好看》，这是不是似乎又表明了什么。😄不同的是今天聊到了很晚，真是太幸福了。几度我开心地想要敲桌子，但一想大家都睡着了。明天也许又会是同一时刻呢！ 19.11.5 今天晚上好期待啊！仑还在说要真挚真诚，朱说我适合文科生，理科姑娘听不懂我说的那些书中的名句。后记，那天晚上聊得太干涩，所以有点崩。。 19.11.6 今日，忧心忡忡，感觉有些冷淡了。按照她有每日给我推歌的特点，开始了解，虽然我真的不会那么脸红心跳的聊天，不过我也很开心，她也很开心，感觉又进了一大步！ 19.11.7 今日送她去车站，去之前我们一起吃了顿饭，尴尬，见到她没话题说。转弯刹车猛了，她贴到了我背上，害羞。去到了门口，我原以为这么近了，她可以自己走过去了，但好像没有让我走的意思，真的是一直送到地铁站的门口，难道是给我机会吗？😕后记，晚上我说她做我的梅拉德女士，我做她的维勒探长，做她的灵光一闪的电灯泡! 19.11.8 中午她吐槽说坐了一个小时的车去打针，医生说今天给小朋友打针，让明天再去……(太惨了)。我说脑袋里都是在想她。 19.11.9 今天去了图书馆，借了阿加莎的书以及之前没看完的约翰克里斯多夫，她说为什么突然去图书馆，我说因为看不到你，只能看书了。晚上他们买衣服，又逛平江观前，告诉我说以后她可能要给我搭配新外表，现在这身装束太IT化。。。 19.11.10 上午听虎哥吹水，下午听老于讲勇士拯救小云公主的算法，最后总结小云公主获救，勇士被BOSS包围，舔狗不得好死。 19.11.11 双十一，想送她一个礼物。很明显大家都极力阻止我这么做，实验室坐下来聊了很久，不能胆怯。这种感觉有点不真实。总之经过那些岁月的双十一。 19.11.12 早上，政治课，她说对老师尴尬地微笑，想起来那笑容，是那么的令人沉醉。夜晚，在敬贤堂听讲座，讲了很多近代的名家还是很有收获，跟她聊了为什么不玩手游，她问了我的星座，她又夸我文学好，其实只是星际穿越的桥段。 19.11.13 跌宕起伏的一天。中午跟她聊了老歌。下午我们三个人又坐下开始扯，他们说我是没事找话聊，总是保持热情以后一旦淡了反而会出现猜疑，我觉得刚认识这段时间就是应该保持高度的热情与联系的热度。开组会时，最后一个报告，大家都已经很累了。但是这时候她正好说研会大会上有个男生准备做俯卧撑壁咚她，本来找另外一个女孩没同意，就挑了她，然后另外一个男生帮她顶替了，我认识他，她很感激，想请他吃夜宵，最后请他喝了奶茶，她说也请我喝一杯。我那个激动啊，她可能是觉得对我过意不去。她站在操场上，期盼我的归来，一连给我发了好几个表情，以前没有过，主要是我没回复，她挺着急吧，我说我在公司还要一会儿，我也想飞回来，等我紧赶慢赶到了操场，心情很激动，但是见到她又不知道说什么了，她比我还害羞，说话总是侧身站着，她说完今晚的故事，我太紧张也没话接，就边说边往宿舍走。然后就这样分开了，我呆呆地望着，那叫个尴尬啊。来的时候想象了一大堆，然后啥也没干，太逊了。。。曾一度以为是她对我没感觉了，我也悔恨今晚的表现。但我们后来又聊起来了，还好。我说明年我们就要被消灭了，她还问我为什么，太可爱了。 19.11.14 早上，她说展示被老师劈头盖脸一顿痛骂，我安慰她说别人是讲的太差，然后被追问又答不出来被骂，你是讲的太好，老师不甘心一定要出个问题刁难你。期待明天晚上。 19.11.15 按捺不住的心情。第一次正式的约会，日料的包厢氛围很好，只是拘谨，聊了好多小时候的事，很有趣开心，乖乖的童年，可爱的阿姨。遗憾没赶上的音乐喷泉。终于一起来到在脑海中幻想复现无数遍的金鸡湖畔，永远没有真实的体验来的这般幸福。牵手这件事有点欠妥。还是不会聊天，聊了很多，但自己没说到点子上，还是她老是照顾我引出话题，无论我说什么都会认真倾听，回应。初恋，感情观，电视剧，吸引的原因。遗憾的是那么美且意义重大的景色，没有拍什么照片。我问：如果那天我没有加你微信，是不是我们就错过了她答：你还没告诉我为什么喜欢我我其实也有同样的疑问，我却也不想简单回答“喜欢是没有理由的”，但我确实在第一眼见到她时就有那种特别感觉，诚然外表是一个维度，但是那个笑容是我将永远铭记的，在人群中太过耀眼，仅是在多年前的梦里见到过，并且在2017.2.13夜里我还记录了那个梦境中的人儿，现在的她就是真切的梦境的具象化存在，梦已不重要，因为梦走进了现实，连通着梦想。傍晚的阳光金黄而辽远，四季交替却如此温情，你没有迟到，而我的等待也是刚刚好。 这个世界上的任何人都有没见过的东西它很温柔，非常甜美大概，如果可以看见的话谁都会想要的吧正因为如此世界才把它藏了起来为了让人无法那么轻易地得到但是总有一天会有人找到应该得到的那个唯一的人，一定能把它找出来就像这样，产生了羁绊。 世间的爱情故事，其结果无非是两种“在一起”与“不在一起”。 任何美好的情感或想法，在这片星空下都会黯然失色，在这片土地上都会被挤压，在这片时空中都会被撕裂，只因为现实的引力太大。 他们的故事已经结束了，而你的还在继续，只是你已不在乎结果，因为你已经见过那个“东西”了。 19.11.16 我也不知道发生了什么，她约我见面，我大概已经感觉到了。在操场走了很久，她怀疑对我的感觉到底是喜欢还是只是一时冲动，她觉得自己对我并没有足够的了解，自己不会处理男女之间的关系，谈恋爱后会暴露自己的很多缺点。她也许不想说的太直白，其实就是对我没有那种感觉。坐在地上，她聊到了自己的家世，一时伤心哭了出来，我抱着她，却也不知道怎么安慰。回去之前，她还是告诉我给她一段时间冷静一下，我说尊重她的选择。我把很久以前写的文章给她看，其实那些想法早就模糊不清，自己也早已摆脱了那些情绪，只是想告诉她：「我也曾在某个深夜辗转反侧，顾影自怜，但如今我依然热爱生活。」想通过这种方式安慰她，她看完之后，也很有共鸣，很郑重地安慰我，我有些不好意思，本来是想让她不要难过，到头来却是她来安慰我，但我也不能多解释，她能倾述的多一点也是好事。我感觉到我好像正在失去她。 19.11.17 惴惴不安的一天。夜晚，她为失联道歉，我其实完全没在意。只是她好像觉得对我有所亏欠，我并不希望这样。她也强调我有话就要说出来，可是我也并没有隐瞒什么。 19.11.18 想送她点零食，最后变成了她送我，并且约我打羽毛球。只是我没想到这么快，打完球，她就说我们还是做朋友的更好，我一时间有些恍惚。虽然她说，我们目前不适合做情侣，她还没做好谈恋爱的准备；虽然她说，我的性格是体贴，细心但是敏感，她喜欢大大咧咧的；虽然她说我们家庭环境的相似造就了我们可以互相理解，就像认识多年的朋友，不适合成为男女朋友；虽然她说自己很任性，我不一定会喜欢并接受，就像现在就跟我提出分开，有时候情绪低落也会一连失联好几天，有时候睡不着吃褪黑素甚至安眠药；虽然她说，我一定会找到更好的，她不适合我……我知道此时的她应该是很理智的，清楚地知道对我没有感觉。所以她说可以做很好的朋友，我万分愿意。我一路没什么言语，只能淡然地送她回宿舍。我说我们可以一如既往。我们可以时光倒流，回到我们刚认识的时候，愉快地自我介绍。接下来反思自己的缺点，继续提升，做朋友也挺好的~ 19.11.22 那一天没有看到音乐喷泉，也知道她今晚上课，今夜谎称偶然路过，想着满足她之前的小小心愿，但又一想只是又一次让她看了喷泉视频。。。。。。然，夜色迷朦，水线苍劲又柔敛，霓颜扑面，虹色璨奇～不过她晚上课上的也很惊心动魄😄 19.11.23 我又“偶然”路过了她之前提过的南门包子，告诉她我也吃了那里的包子，像她说的一样那么地美味。然后歌手大赛复赛开始，我边摄影边跟她聊目前谁在唱，然后互相听后的感觉，因为她的宿舍正好在旁边。然而就是那么一瞬间，我好像感受到她就在我的身边。 19.11.26 她找我了，虽然是找我帮忙解决问题，但还是很开心，多么希望那一句“爱死你了”是我想的那种意思。 19.11.27 又是组会的一天。跟大师兄说了声抱歉。老板请吃汉堡王，等啊等，饿到虚脱，还是拿大师兄的手机下的单。等了两个小时的外卖，虽是与之立下绝食契约的汉堡王，但此刻已经不容犹豫了，真香。跟她分享了组会的日常与汉堡王的真香。 19.11.28 连日来的阴雨连绵，今天终于放晴了。日推里很巧就有一首《太阳》，分享给她，她也分享了一首《老街》给我。 19.11.29 创了一个公众号，想了半天的名字最后决定用“可爱的柯摩”，她太爱夸我可爱了，简介用了藏头诗。 19.12.1 组合数学完课了，下午回家跟母亲聊了一会儿天，告诉我慢慢去感受人生吧。晚上偷偷帮她抢了研究生歌唱大赛的票，正好她没抢到，然后我假装我去不了，这样她就没有怕见到我的负担了。 19.12.3 南京江苏省人工智能大会两日游，五星级酒店体验卡，跟着老王一起兜风，带老王体验宾馆浴池。正跟老王吹牛皮，她正好发信息过来，老王握住Cadillac方向盘说不要慌，拍段视频说在南京街头瞎逛，她直接猜出我在哪里，在干嘛，，，说是我的视频里面大楼标志很明显。原来是问作业的问题，还是挺开心的。之后晚宴也玩得很快乐，虽然没跟大佬合影，但喝了志华大佬敬的酒。之后上台替领奖，南京南站的狂奔。花开两朵，全都不表。 19.12.4 始终放心不下她那个问题，问她要了程序，居然是秒回，看了一下午，还是感觉抓不住要害，或许当面交流更加清晰，不过今天应该是这么多天来聊得感觉最好的一天了。 也许已经结束了，也许还未开始。我只知道，曾经很幸运，很开心，就已足够。 19.12.6 晚上回去后，我知道她应该没睡，很晚了，补作业会无聊吧，我第一次深夜找她，想着白天拍了一些风景分享给她，可能她不知道该回什么吧，就发了一连串，说要继续写作业了，道了晚安。 19.12.11 看了好久寒山寺跨年的活动，盘算着也许可以尝试，想象着那个场景，不过也许她根本不会接受。她的歌单更新了《想你想你》，我就偷偷对她可见的分享在了pyq。 19.12.12 公众号发了之前做好的关于几个写作工具的分享，又是周四，照例去操场跑步，不知道是不是错觉，背后那个快步走出操场的背影就是她的。 19.12.13 研究了一天的算法，没什么收获。昨天发了那首歌，她居然点赞了，这是第三次分享。还有前天晚上的晚安微博，那个藏头昵称，她肯定看到了。虽然已经三天没有联系了，可是我一直在她周边留下影子。 19.12.14 四六级监考，世界真小，遇见了她。 19.12.15 分享了一首《To the moon》给她顺便跟她分享了背后的感人故事。 19.12.16 早上，手贱点开了那个文档云，看见了她的PPT，又看见了最后有两页广告，没忍住提醒了她，虽然已经暴露了自己，但也没办法了。虽然被人说有点变态的意思了。她倒是不是特别惊讶的感觉，但是表示没办法了，调侃的感觉，就只能那样了。 19.12.20 突然发现是今夜的冬至活动，跟着他们商量去不去，大腿一拍，走！冒着冷雨，我们叁就这么出发去了独墅湖，路上一直在疑问为什么要去，虽然是她的部门组织的活动，但是她大概率是不在的，就算她在我又该怎么跟她说话？但是我说你永远不知道下次出行将给你带来什么。事实证明，此行收获颇丰。雨夜，校区很冷清，出奇的活动人还不少，我们又是猜灯谜，又是印年画，又是挑对联，又是蹭合影，玩得不甚快哉！还是对联好玩，虽然在那里从开始站到结束，也没猜出来几个。 19.12.21 赶项目的一天，区块信息改造，广播之类。深夜给她发了句冬至快乐，第二天不到10点收到了回复。 19.12.22 总算把算法讲完了，没人听，内心也一团烦躁。回家跟爸妈过了趟冬至，热腾腾的火锅，三人围坐，母亲没有说的过多，只是叫我慢慢经历，慢慢感受，一杯，一杯冬酿酒下肚，不知是热气还是什么，眼前渐渐模糊。回到学校，一个人静下来脑子里她又来了，只能掺和到他们中间，抓娃娃也好，打游戏也好，做什么都好。在群里即使发照片，她和舍友也依然没有回复。回来，深夜，继续工作，等广播模块终于加载成功了，闭着眼开回去，吓得别人不敢跟我一辆车。 19.12.23 雨还在下。上午参加区块链会议，秦发来R语言考试的问题，我也不是特别懂，有电脑在身旁还好一点。我在想也许她下午也会考这些吧，要不要告诉她呢，也许她也会来问我呢，最后，还是算了吧，那样会让她觉得可怕吧，生活像被窥视了般。 19.12.24 雨还在下。平安夜，嗯，怎么过的呢，没跟大家去看晚会也没去看电影吃火锅，当然也没收到苹果。她也告诉我她在复习，我也就结束了今天。 19.12.25 雨还在下。偷偷送完贺卡给他们假装不知道。将给她的贺卡放在操场只有我们知道的位置，晚上发pyq只对她可见告诉她位置，当然她没有去拿，那就等待着它某天会被人发现吧。晚上，一个女孩坐在雨中聆听我的琴声，直到我离开，我才明白那种感动的滋味。 19.12.26 终于放晴了。她也离开了这座城市，把雨一起带走了，追随她而去了，或许是我开始放下她了吧，虽然有很多种理由可以解释她没有接受贺卡，没看懂我的意思，下雨所以没去拿，不过确认贺卡还在时，我有种释然的感觉。晚上，又从西门出去，沿着曲径幽巷走了一遭，在星巴克畅谈了很久，心渐渐放开了。 19.12.28~20.1.5 31号下午跟行健在一起很开心，体验了用4DX体验了一下叶师傅。在金鸡从裤衩走到李公堤再走回来，聊了很多，我也是第一次听到行健那段痛苦孤独的时光。晚上的时候跟姿颖和他一起到寒山听钟声，在人群中穿梭，感受着千年古刹的神秘与此时张灯结彩欢庆，心中各自怀揣着心事，与对未来的憧憬与希冀以及来年压力的压迫感，但也挤出了笑容，毕竟有她突然出现，大家要开心一点。融在人群中，她不时回头确认我没离开，最后终于挤到了绝佳的位置，停住的刹那，现场众人一片寂静，钟声随之响起，沉重悠扬，浸入心灵，与刚刚的人声鼎沸仿若两个世界。站在他们的背后，当最后一声敲响的瞬间，他们应该都各自许了愿吧，那回头的两声“新年快乐”，是今年开头听到的发自心底的最令我快乐的话语。在那一瞬间我也发出了我的祝贺，不知那回复是应福而回还是本已经准备好发。 20.1.17 家里的雪菜豆干饼(这么多年家里只叫“饼”，暂且叫这个名字吧~)依旧美味，真希望与她一起品尝。好久没联系了，想着关心她今晚都吃些什么，相信她也喜欢这饼的味道拒绝外卖，只是还是在抱怨工作的辛苦，真的很累吧。 20.1.24 过年，她主动的祝福，始料未及的席卷，漫长的假期。 20.3.14~15 昨天看了泛式的直播，一开始关注他只是因为他解说动漫的角度确实挺有趣，对于新番的介绍也很全面。后来发现偶尔也会炫富什么的，其实也只是真实分享自己的生活。直播说的几点，值得记录一下，虽然他不是一个做直播的人，也不是精心准备的话，也正因为如此，才会引起我的注意。其一，一个人如果不在乎别人的看法是绝对不会再网络上输出视频，文字亦或是音乐，分享的本质是希望得到认同，所以很多人为什么会因为99个人夸赞，1个人的侮辱，而对其恨之入骨，揪着这个人满大街甩，别人即使劝说「不要理会那‘个别人’的感受」也于事无补，因为人的社交思维无法承受这么大量的虚拟互动的点触，因为需要认同所以在意，确实是这样简单的道理。其二，关于怀旧的问题，同样我们都是怀旧的人，所以这点让我倍感亲切，因为身边会怀旧的人太少了：会时长点进QQ空间的相册一看就是半天，看每张照片的笑脸，回想当初拍这张照片的时候，是怎样一番景象(因为知道自己会怀旧，所以拍很多身边人的照片，会把这些毫无遗漏的放在QQ空间的相册里，也不在乎看的人是网友还是现实的朋友，所以有人会说我QQ空间没照片，这大概就是有这样的情况在里面吧，因为不会怀旧，所以没照片，很畅通)；也不会去记日记；也不会去记录身边那些最容易忽略的细节。可以与时代人物一起谈笑风生，也可以和市井小民聊鸡毛蒜皮，可以品味鲍鱼燕窝，也可以咽下粗茶淡饭，可以惊叹喜马拉雅山脉的雪被，也可以为身边的一花一草驻足赋诗。好像渐渐偏离了主题，但是那些感受确实都很真实，也提到了身边的女性朋友都是属于会跟男生谈天说地，没有微尴的，没有传统意义上的女性朋友，其实也正常，因为她们中大部分人只会跟同性交流，所以他的感受其实是当代大部分男性的感受吧，毕竟无论男女我们的爱好都一样：异性。当然还有一点，以前也会这样想，就是当你周围的人所追求的娱乐品味与你不在个层次的时候，就像你阅读着各个时代大师的杰作，脑海中交叠着各个时空的思想与精神时，耳边却听到室友的韩剧亦或是抖音声时，表面上什么也不会表露，内心却有一种隐隐的自豪与轻蔑。有一个回答是这样的，无论是什么样的获取，本质上都是消费的一环，没错，再怎么高雅与脱世，再怎么荡涤灵魂与精神，你并没有给这个世界生产什么高价值高营养的东西。所以这种感觉其实一文不值，只有将其体系化，逻辑化地，并稳定地输出他们，你所拥有的自豪与关怀才有意义。 20.3.16~18 日子过得匆匆，这几天思考渐渐怠慢，思想渐渐枯竭，思维渐渐崩塌。记得什么时候看到的句子吧，那是法国工业革命时，有人说的话：“人们开始不再熟悉自己的工作以及日常，纯粹的简单变得奢侈，因为事物发展的过快；年轻人被时代所裹挟，过早地参与到社会进程中，从而被逐渐抛弃；人们强烈地追逐各种轮船，邮递等快捷的信息通讯工具，因为信息过载，平庸文化变得普遍化合理化”。记忆模糊不清，这不是原句，但大概能传达我此时的焦虑与自责。200年前的人所描述当时的状态其实与今天这个时代相差无几，甚至可以说是一模一样。今日的世界何尝不是如此呢，我们已经快得停不下来仔细思索了。在面对热闹与喧嚣时，我们已经做不到波澜不惊了；在面对网络谣言与非议时，我们已经做不到独立思考与判断了；在面对艰难的处境时，我们已经无法做到理智地决策与思考了。 20.5.1~12 少年不识愁滋味，爱上层楼。爱上层楼。为赋新词强说愁。而今识尽愁滋味，欲说还休。欲说还休。却道天凉好个秋。这半个月过的是真的辛酸，一忽疹疾缠身，一忽又骑车飞摔，蹭的皮开肉绽。身体动弹不得，疹疾又致全身不得按宁。着实不知如何是好，现如今母上也奔赴都市生活，不知不觉，这段儿时至现在都未曾体验过的与母上一起生活这么久的时光已然结束了。激动的三月，慵懒的四月，以前未曾想到会有这样的一段怀念终身的生活，原以为这样的生活只有很多年后的年老之时才可体验。如今只因其过于美好，但终究是要结束的，即使我迷恋这样的生活，但内心的忧虑却也不断地在增加，再美好的时光，但总也要面对现实的压迫的。它的日复一日，每天的日常又是如此的重复，母亲终日看着小说，而我终日地在楼上看着电脑，虽然不舍，但这样的生活终究是不平衡的，我们都不应该被束缚在这里，虽然我们幸福地平平淡淡地陪伴在彼此左右，但此时的它应该是短暂的，我们彼此都有应该去做的事情，于当前，它只是看上去那么平静罢了，可我们还未实现的东西太多了，这样的生活提前来到了，幸福了但也徒增了很多不安。 20.6.16~6.18 就是今天了，不知道会发生什么。还有几小时就出发了，该怎么办。 20.6.19~6.20 就这么结束了吗？无论那时的我怎么预测，也无法想到事情发展的结果会是这样。即使是现在我依然心很乱。我以为我已经放下了，可是一见到她，当初那些种种心情又会涌上心头，只是与之前不同的是，我已经不敢再踏出那一步了。现在的我只是想完整地记录下，过去的48小时所发生的故事。高铁上，我画下之前我告诉她想准备的晴天娃娃，因为找不到制作材料，我只能将它画下来了，我用浮夸的画技画了很多，也暗暗地在其中抽象地画了那晚牵手的一瞬间。老天很给面子，那之后没过多久就晴天了。时隔7个多月，我们再见于南京南下的地铁站，她穿着黑色的短袖礼装，天蓝色的休闲牛仔以及平底鞋，隔着口罩我也能想象到她向我打招呼时笑容的模样，那是我见过一次就永远无法忘记的感觉。走上地铁，我们走到车门边，我搭在扶手上，她靠在扶手上，几乎同时靠在了我的臂弯上。她始终不抓着任何栏杆，以后的几次坐车也是，我每次会在她重心不稳时，用臂膀轻轻地挽住她。我们聊到学校，聊到学习，聊到制造学术垃圾，聊到毕业答辩，聊到我们。她终于注意到我的发型是不是变了，我说懒得剪了，是不是很难看，她说，没有啊，很好看，我说，我还以为你会说很难看呐，哈哈，我知道虽然她可能只是想夸赞我而已。我们会因为地铁吵，听不见对方说话，而互相贴的很近地在耳旁说话。下了车，来到之前说好的，集市，跟想象中差距甚远，我们边走边聊。聊天天气时，她说曾经有天早晨也是很闷热的天气，自己一起床就什么也不想干，谁跟她说话她就要骂谁，很愤怒的那种。看到路边有人卖宠物时，我问你家养宠物嘛？她说，我妈说养你个小祖宗我已经够累啦！哈哈。。我提到了我的qq名的话题，想让她猜出来她是哪两个人物组合词，她果然猜出来了，我一个劲地夸她。我们很默契地随便走了一圈就离开了那个地方，在那一瞬，我好像懂了她应该知道：我并不是为了来这个集市吃东西，而只是用这个当借口为了见她。我们回到地铁上，我们头贴着头，在她的大众上寻找附近有什么好吃的，这真是个难题啊，她真的南京苏州什么小吃都知道。我们一出地铁就看到那个小象的雕塑，我们走过德基，走过那些奢侈品，听到了路很远的地方有着很大很持久的笑声时，我说，什么事情那么开心，她笑道我跟闺蜜们在一起时，就是这样笑的，我说，哈哈，人生就是要随性！走进了太二，她没给我点餐的机会，我们面对而坐，她明明可以把手机递给我来点想吃的菜，却让我坐过去，一起凑得很近地去点餐，我只觉心跳加速，发香夹杂着香水，我一时不知道我该干嘛。画面上有金针菇，我说点个明天见，她说啥是明天见，我指给她看，她说，呀怎么是这个啊，为什么叫明天见呢，我（汗），不解释了，回头慢慢意会，她说，好，今晚回去意会。席间，她又点了奶茶，我说这不行啊，都是你请客，她说没事，明天你来就好啦~我们从QQ的衰败聊到微信的崛起，从三体聊到文学，连我的研究方向她都愿意听（我居然把机器学习，分布式存储讲了一遍，并且黑了一波现在ai的现状就是宣传与在线教育，一般套路就是去大厂背书，然后开始搞培训捞钱），还有娱乐热点，肖战的惨痛教训，兴风作浪的姑奶奶，青你（她只看过开头结尾，我有点小惊讶），B站的后浪（连母上大人都知道了，很好玩地问她：B站是个啥，怎么起这么奇怪的名字，好好玩。。），聊到了28岁退休的字节跳动员工，然后她说她以后可能要失业，我结结巴巴地说了句很有问题的话（好后悔）：有些事注定做不成，只是以前你没发现，现在发现了痛苦罢了。。。我到底在说什么啊！！！有好几次没啥话题了，她拼命找了好多话题。我们出来，我想实在不行，想想最近发生了什么事情，把之前大黄失忆的事情说了，他回去想要复合，她说，一般人肯定不信，这种分手理由太扯了，我要是她女友肯定要揍她（哈哈，喜欢她的性格！）。我把口罩落在了太二，她跟我一起去寻找药店买口罩，逛了屈臣氏没有一次性的，又去了其他地方，我去上洗手间，她自己一个人找到了，然后掏钱帮我买了一袋口罩，我来的时候她正好付完钱，一转身一脸兴奋地告诉我，“这里有啊，我买到啦！”我既开心又感动也痛苦，她这么关心我，掏自己的钱，我说不行，不能这样，她说没事，这没关系啦~可是这怎么可能没关系呢。我们走到新街口的大转盘，她说这里是南京美女最多的地方，每过5秒，你就能看到一个美女，我转过头去，不敢看着她说：“是嘛？可是我环顾四周，只看了你啊！”，她笑道，你好会啊。……我们游移于各种品牌之间，我读一个名字，她就会解释一番这家店的特点或者名字或者产品，我记得origin，和她的蓝铃花以及大吉岭茶。我们站在一家店前，一个一个闻着那些香水小样儿，我们分开闻着两边，她转头看着我正在一起闻，莞尔一笑，笑道走吧。我告诉她，很喜欢那个小豆蔻的味道。我突然兴起想去她以前的家，老门东附近逛逛，我们走了没多久，我看她老捂着肚子，我就开始有点担心了，然后她就说肚子很痛，我很担心但不知道该怎么办，就说我们回去吧，她说想找了个地方坐下来休息一下，坐下来后，我想说帮她找杯开水，但一想开水是个禁忌词语，所以想问她要不要拍一下背缓解一下，这也是我仅能做的了。她查了一会儿地图，我们定下了明天逛博物院的时间，她说稍微好一点了，我也不置可否，不知道她是不是为了让我减轻担心这样说的。但还是先回家吧，毕竟这样也不能再去其他地方了。到地铁时，我问还是担心，问了下，是不是以前也会这样，吃了热的然后喝冷奶茶会肚子疼，她说很少会这样，我就有点自责，怪我没有考虑到这些、、她老是说要送我，我说不用，我知道怎么坐车，然后她的那个方向的列车先到了，她就上车了，走的时候她回了两次头，跟我道别，带着笑意也带着歉意。我在想为什么我不主动一点送她回家呢，明明我应该这样做的，我那一瞬间想的是，她应该是不愿意的吧，也许不愿意我知道她家在哪，可我那时连问的勇气都没有，就这样让她走了。她上车之后，还发信息跟我说“抱歉”，我实在不知道怎么表达自己的歉意，她太好了。总之今天就这样结束了。第二天。改了一晚上简历，忠太把我带到上班的地方，正巧她坐地铁去博物院会经过我们附近的站台，我就让她在那个站等我，我到的时候，我没想到，她已经下了地铁，走上楼梯，在检票口的地方等我，我受宠若惊。简短地聊了他的事情后，我们一起上着她的网课，我们一起靠在地铁的扶手上，她把一边耳机给了我，我们一人一个耳机，我端着手机，看着腾讯会议的画面，她听着课，看着窗外。我偶尔会说听课的感想，她会赶快偏过头来听我说，然后给我回应。到了博物院后，她自嘲自己太笨了，居然还要我带路去。她预约时，问我要了身份证号，听我报完之后，她沉默了很久，我不知道她在知道我是94年的后，心里面做过什么思考，我真的很想知道。还有一点，她还是跟以前一样，打错了我的名字，她也跟以前一样，超级自责，反复跟我说对不起。进了博物院感觉时间过的飞快，我们逛了几乎所有的馆，我们头贴着头，一起讨论错金是什么，我们一起盯着战国到秦时我国版图的变化，我们惊叹那些兵马俑的形态各异与精雕细节，我们一起欣赏古代的书卷经典，徜徉于玛瑙与璞玉之间，她说叔叔曾说过，那些现在玛瑙的形成是人工营造的那些自然环境，期间她还要兼顾课程与父母的聊天。我们一起走进镇馆之宝，驻足了很久，我记得，汉武釉里红岁寒三友梅纹瓶。走过书画馆时，我惊叹与那幅龙吸水，她也注意到了我的眼神，说快进去看看，美妙，只有我们二人漫步于整个大书画馆。进入民国馆，我们研究了为行动不便的人提供的楼梯轨道（我当时说了残疾人，唉）。我们一起走进小巷，一起路过邮局，一起看着大戏院的海报，路过火车站，走进南京历史馆。很多人在那里拍照，她却从来没有拍过一张，我问，你好像并不是喜欢拍照，她说，确实，我很多时候不想拍照，因为感觉很傻，发pyq之前会想很多事情，想着想着就不想发了，我说，所以90后正在慢慢地从pyq消失。即将离开时，我还是忍不住，提出我们一起在院前合个影吧，她同意了，但是我又不好意思说能不能一起正脸合个影，所以她就带着口罩拍了。一开始我们想去KFC吃中饭，跟着导航一路上了城墙，我知道她很累，穿着高跟鞋，说她是这么多年第一次在南京爬上城墙，我有点小激动，我们一起又爬下城墙，放弃了吃KFC的念头，顺便把高德骂了一顿，最后她说要不吃小马牛肉面，我没意见，但是我觉得不对等，这太便宜了。席间，我提出了这个想法，她说没关系，来日方长，我说，对，来日方长。她又很好奇导师和师母，我说老板是通过舞会认识的妻子，然后很诧异我们现在没有舞会是怎么社交的，她说对啊，但我们现在有KTV什么的呢，我说师母很凶的什么的，老板从来都是依着她，性格很软，有点像我。我想说陪她一起去面试，她说不用了，我去她更紧张。我们一起上了地铁，已经没什么话题了，她一直在跟叔叔阿姨聊天，可能是觉得我们这样太尴尬了，她说了她闺蜜的事情，我也懂，就跟着聊，但我觉得自己很差劲，找不到话题。之后她一直跟叔叔阿姨聊着天，我也很快到站，我准备离去，她打着那种抓猫的手势跟我说再见，我迅速下车，但魂也丢了。过了几个小时，她把合影发给我。便跟以往一样不再跟我聊天了，可我却再也放不下，我不知道我是不是丢了什么机会，还是本来就没什么机会，如果我跟以前一样，再勇敢一点会不会有什么发展呢，是关系更僵，从此再无交集，还是我们的关系会升华呢？我知道我表现得很差劲，即使她曾经对我有好感，现在也没好感了吧。而且我总是觉得她只是把我当兄弟愿意跟我出来一起玩，但真的是这样吗？可是她不愿意让我帮她拎包，当然我也知道她的包应该很贵重。可她微信上也不主动找我聊天，所以我们也许真的不能前进了吧，我真的不知道该怎么办……一切都源于我的懦弱，扭捏，想挑明却又怕失去，因为我没有把握，可正是因为我这样的性格，她才对我有些好感？ 20.6.21~6.22 21号开了组会，晚上正好聚齐，又跟老板后面蹭饭，酒有点多，没忍住给她发完两字，还是怂了，秒撤回，我不知道她的“你成功引起我的注意”代表着什么。 我的四周一片漆黑置身于孤船之上头顶是一片漆黑无星的夜空浪波从四处拍来我勉强保持着平稳却力不从心周遭除了水声之外还有着窸窸窣窣的私语声我感到无边的恐惧——它们会来攻击我 也许她会来到我身边也许她会为我照亮四周也许她会带我驶出黑暗也许她会为我点亮繁星也许她会为我停留于此 也许她只是静静地看着我黑暗便只是梦乡孤船便只是摇篮浪波便只是微风私语便只是歌声 可是她看不见我 20.6.23~6.25 她好似太聪明，一下却能看穿我所有的想法，却还在配合我演出？还是她只是不知道我的心思，一直在等待我的行动？抑或是完全不在意我做什么，随便我做什么事情，只是保持的最基础的礼貌？但有一点可以肯定、 我从去年开始做梦，后来梦醒了即使梦醒了，但我依然怀念那个梦我不知道怎样解脱自己你也许会说，既然是梦，那终究会有醒的一天 但事实是，这个世界的成功者往往是，那些会做梦的人他们在自己的梦中，用超乎常人的梦想力，做出了梦幻般的举世卓绝成就 我醒来，是因为它太痛苦了两个人都在梦中，只是你醒了没有你的梦中世界，是虚无的于是，我也醒了 我原本以为，不再进入梦中我的苦闷就会消失但，病情时常反复 可我依然怀念那个梦时隔多日的再一次相见忐忑，彷徨可是，原来，我又坠入了梦中当预示着离去的那辆地铁即将到来时我多么希望，它永远不会来我说：它怎么来的这么快你说：是啊，我也觉得也许，你以为我说的是他来的太慢了吧毕竟，谁会怪车来的太快呢 现在，我已经不想再回来梦中梦是什么梦即现实现实是什么现实连接着遥远的未来 20.6.26~6.29 今天晚上的月亮好美，想要跟你分享，但一想我们好久没聊天了，想到这，突然觉得月亮也不漂亮了。你的目光总是看着我看不见的远方，看着窗外，看着天空，我总想赶上，却总也追不上。目光交汇时，你的眼神满含惆怅，却故作轻松，我看得清，却又仿佛看不清。你应该有很多话要告诉我，却好像什么也没说，我只是静静倾听，别无他求。 20.6.30 今日真的难得的晴天，弄了一天的各种markdown以及overleaf的写作模板，大概算是一劳永逸吧。晚上大家一起厨房炒粉，圆桌谈，亭台聊人生，三国无限杀，乐趣实在无穷。不禁怀疑，这生活是不是太美滋滋了、 一些句子 是“Wow”!霍金对这个世界发出的词，是惊叹。即使他一生受尽病痛折磨，全身失去控制能力，他对于这个世界，依然像一个孩童般热爱。言犹在耳，霍金却已离我们而去。在迈克尔·怀特与约翰·葛瑞本所著的《霍金传》中，霍金首先是一个天赋异禀、敢于直言、敢于想象的科学家，其次才是一个身残志坚，可敬的人。这位从年轻时起就坐在轮椅上的科学家，从没放弃对宇宙的探索，也从未丢失对生活的信心。正如霍金自己说过：“如果生活没有了乐趣，那将是一场悲剧。”3月14日是爱因斯坦的生日，霍金于同一天去世，有人说，他大概是去给爱因斯坦过生日了。那么，祝福两位伟人在某个星球安好 老一辈人，先苦后甜，我们这一代，先甜后苦。先苦后甜易，先甜后苦难。正如钱钟书的《围城》里写道：天下只有两种人。譬如一串葡萄到手，一种人挑最好的先吃，另一种人把最好的留在最后吃。照例第一种人应该乐观，因为他每吃一颗都是吃剩的葡萄里最好的；第二种人应该悲观，因为他每吃一颗都是吃剩的葡萄里最坏的。不过事实上适得其反，缘故是第二种人还有希望，第一种人只有回忆。 最难熬的状态就是，眼里操心着不再年轻的父母，脑子里想着乱七八糟的事业，心里还藏着一个不可能的姑娘，胸膛里还撑起着一个遥远的远方！ 有一个夜晚我烧毁了所有的记忆，从此我的梦就透明了。有一个早晨我扔掉了所有的昨天，从此我的脚步就轻盈了。 每个人都会有缺陷，就像被上帝咬过的苹果，有的人缺陷比较大，正是因为上帝特别喜欢他的芬芳。 语言和文字真的是不可执取的东西，当一句话说出来或者写下来，它就不是你的了，你必须允许别人任意解读，甚至误读。所以我最想说的话，其实在我开口的一刹那就已经说完。 昨夜的月亮 20.7.1~7.3 近来时晴时雨，一号的晴天朗日，傍晚出门取外卖的温晴草香，令人陶醉。二号的又诉诸了衷肠，可是又如何，不过是徒增烦恼，我的内心早就知道那些现实的原因，可是我又能怎样呢，也没什么遗憾的。做不出来的事情，说出来也没有任何意义，我不会对她太过于直白，我也不会就此放弃，我恐怕永远也不敢说出我的心意，就算我向她展现了某些代表价值的东西，那又如何，又能表达什么呢，这样是告诉她我很有钱还是在乞求她的认可还是怜悯？除了把实验室的Latex模板弄出来，记得1号弄了一天简历的模板，然后用了一份跟白天弄的没关做的自己的简历，最后苏州微软没有任何回信。当然那个下午还是充实又开心的一段时光，一起创造新的具有纪念意义的事物，共同创造的乐趣，这恐怕就是古往今来人人乐于称道的事情吧。今天计划了7号的之后的事情，希望能实施顺利吧。 20.7.4~7.5 半年了，久违地回了一趟学校。那条林荫大道，此次此刻走起来，却有一种以往从未感受过的轻松感。推着车，从本部到东区，身体快要失去知觉。翻墙出去，烧烤很好吃，主要是我烧的，嗯。歌唱到声嘶力竭，回来墙翻得也是刺激，往来的人群，无不驻足观看我们四人的精彩树林翻墙秀。今天终于实施了计划，几个小时之前，成功了计划的第一步，彼时的我还是很开心的，但此时此刻，敲下这些文字的时的我，却被无边的绝望包围。我们很成功地合作完成了这次华丽的“壮举”，在我看来一切都很完美。可是，所有人都抓着那个“朋友”的字眼不放，我们坐下来，一直聊到深夜，我淡淡地描绘那个计划，我不知道结果，我知道我只是该这么做，也许结果终将令我失去自我。我只想说友人之后，还能更进一步嘛？我不会耍脾气，更无法完全忘记她，更做不到让她关注我，与我分享，更别提让她对我产生嫉妒，亦或使计让她认为我在拒绝谁。可是一切会怎么发展，我不敢想象…… 20.7.6 你要问我为什么总喜欢感动自我，或许是吧，但歌曲，文章如果连自己都感动不了，如何深入他人的心灵呢？与人交流，与世界对话，与灵魂私语，不常怀感动之心又何以与自己达成和解呢。有的时候，我无法想象富有的，掌权的，成功的我是什么样子，有时候我又觉得那样有时挺悲哀的，那时的我应该再也回不到现在这种总是会有一些思路在脑海中打转的状态了吧，那样的我不再是我，也挺令我厌恶的。我从来都是做最真诚的我，所以我也经常受伤害，这些伤害最多地来自于我自身的敏感体质。“真实自有万钧之力”，我知道有时你就算把所有的你自己掏出来给人看，也会被完全遗弃在一旁，所以你更多需要做的是找到那个想倾听你所有事情的人，在她面前你所有的内心都可以奔涌而出，而不用担心，这股洪流会将她带到永远看不见的远方。说到这，我知道，我们也许已经不可能了，我有预感，她永远愿意做我的倾听者，只是她一如既往地履行诺言与对我的愧疚。我的心不算平静，却也不失波澜的撞击。 20.7.7 又到了这一熟悉的一刻，又是这番场景。接下来几个小时会发生什么呢，是什么都不会发生，还是以后她不再敢见我，还是会向我预料的方向发展呢。我不敢想象，无论是哪一种结果我都不敢想象。可时光毕竟是走到了现在，我们已经不算是陌生人了，却总还是原地打转，我只能去打破这层阻隔，至于结果如何，已经不是我能奢望得了的了。计划好的流程，准备好要说的话，我知道我就算排练一万遍，也还是会出错，无法改变，因为我恐惧的是因为某一个举措或者某一句话让她离我远去。我该不该说出那些准备好的话，还是只是顺其自然，到头来与上次一样，简单的一次相见。她是在等待我，还是完全不在意我做什么……接下来会发生什么呢。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>Editor</tag>
        <tag>Diary</tag>
        <tag>感情</tag>
        <tag>温馨</tag>
        <tag>回忆</tag>
        <tag>真挚</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[LeetCode] 1.Two Sum]]></title>
    <url>%2FLeetCode-1-Two-Sum.html</url>
    <content type="text"><![CDATA[第一次写，不是很懂。 题目给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。 你可以假设每种输入只会对应一个答案。但是，你不能重复利用这个数组中同样的元素。 示例 给定 nums = [2, 7, 11, 15], target = 9因为 nums[0] + nums[1] = 2 + 7 = 9所以返回 [0, 1] 解法一次循环，先将目标值减去第一个数，然后将这个数加入到字典中，如果字典中能找到另外一个数与当前的数加和满足目标，就输出。123456789class Solution: def twoSum(self, nums: List[int], target: int) -> List[int]: dic = {} for index, num in enumerate(nums): another_num = target - num if another_num in dic: return [dic[another_num], index] dic[num] = index return None document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>OJ</tag>
        <tag>Algorithm</tag>
        <tag>Job</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[无声低语]]></title>
    <url>%2F%E6%97%A0%E5%A3%B0%E4%BD%8E%E8%AF%AD.html</url>
    <content type="text"><![CDATA[本文写于2018年4月2日，雨转晴，23℃。 实在没时间去看闲书了，真是一天比一天觉得自己“面目可憎”，没有时间去顾暇自己内心在日益复杂的心理疾病。没有办法治疗自己，还要在看到某些人某些行为就厌烦的状态，总是在因为别人的碌碌无为，浑浑噩噩惩罚自己，因为没有可以掏心的女性知己而终日抑郁。永远也无法原谅自己，“一切都始于那年夏天”，这一直是我认为的心中的痛的根源，但也许真正的根源来自很遥远的过去，无限地放逐自我，不得不说与那帮人脱不了干系。 不过我这个人失败是事实，连自己的表妹对自己都从来都没有过好眼色，关系不仅不亲密，一年又一年越形同陌路。追本溯源，我痛苦不是某一个巨大压力，而是这些的过往滴滴点点。可是悲哀的是，即使彼时我再怎么成功，内心再怎么丰富，我都想象不出，我已然摆脱这些痛苦的状态。每当深夜降临，我总会想起那些久久无法平息之事。我恐怕永远是那个走不出自我的真嗣。 永远幻想会有谁来帮助自己，渴望美里，渴望丽，渴望明日香，享受这种暧昧的感觉却又深深厌恶这种被人戏弄的感觉。丽或许是他内心最潜在的希望，可我不知道这种希望对我而言意味着设么？我永远害怕被人嘲笑、拒绝我献出的真心，是啊，这个世界不就是谁主动谁就输的可笑游戏吗？不是总在我付出真心的时候背叛我吗？不是总是我每次眼神交汇时就会收到最冷漠的回应吗？(虽然我知道大嘴的想法是错误的，可那仅仅是错误，其他与现实完全吻合，这种错误又有什么意义。) 是我封闭了自己的内心吗？不是吧？如果所有人都拒绝你，你去向谁敞开？为什么它们活得那么快活？那么无忧无虑，那么闲然自适？那么碌碌无为？那么终日无所事事？我却总是忧心忡忡，痛苦不堪，我跟谁都比不上。我————比不了任何人。我恐怕是最失败的那个人，没有能力，没有才华，没有外表，已经到了如今，何时才能让母亲······受苦了一辈子的母亲真正享受一次家子带给她的幸福，让她觉得在人生最困难的时刻毅然决然地只身一人带着尚在襁褓中的我四处求人，拼命生存，所有这一切努力不是白费的。母亲没有活的非常失败，她培养出了我。即使我的人生是失败的，内心是残缺的。可是，我无论如何也要证明，不，我终将证明母亲做出了这世上最伟大的壮举。母亲还在一个人坚强地与这世界做抗争，一个人孤单地生活着，受苦了一辈子也没有享受过物质生活，更没有精神生活，可是她那么地相信我，那么低积极向上，我没有资格总是放弃自己。至少，在今后不久远的日子直到世界终结，要让母亲，父亲真正地脱离期盼，真正地体会到这世界每个人都应该享有的幸福。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Diary</tag>
        <tag>Thinking</tag>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我于你，止于唇齿，掩于岁月]]></title>
    <url>%2F%E6%88%91%E4%BA%8E%E4%BD%A0%EF%BC%8C%E6%AD%A2%E4%BA%8E%E5%94%87%E9%BD%BF%EF%BC%8C%E6%8E%A9%E4%BA%8E%E5%B2%81%E6%9C%88.html</url>
    <content type="text"><![CDATA[痛苦的根源，无能的愤怒。 ExcaliburEX博客 2017.12.5 晴天 近来真的是很忙，心里想的事情太多了，不写出来真的会越来越郁闷，终于得个空好好静下心来写点东西。 思来想去，从哪里开始写是个问题。总结起来也就是感情与学习的事情，引申一下恐怕就是决定我人生走向的问题。 那就从感情开始说起吧。我从一开学便想快点遇见另一半，或许由于我的这种动机吧，我从一开始这种动机就是错误的，为了找到她而去找她，却没有想感情对于现在的我来说到底意味着什么。不过也没有什么契机去寻找，我唯一的目标也就是等着社团招新这个时刻的到来，如果运气好我会在当天就会遇到我心仪的对象，抑或是我将在社团长此以往的交流中找到她。就在那一天，我来来回回在各个社团铺子前搜寻，也许是冥冥之中自有定数（我承认这种表述非常地土，但用来形容我当时的心情完全不为过）我在动漫社遇见了她，我的眼神从几个Coser中准确定位到了她，我不知道她Cos的是什么人物（当然我后来问她是什么，她很狐疑地告诉我就是普通的JK），但是那样的装扮确实是我喜欢的类型，但是重点恐怕不在于这里，我在她那里重新寻找到了八年前第一次遇见“她”的感觉，那是很遥远并且我以为可能永远也体会不到的感觉。我在人群中壮着胆走上前去，毕竟虽然她们是为动漫社做宣传，但是并没有男生跟她们聊天。她背对着我，我小心翼翼地点了点她，她很惊讶地转过头来看着我，在人群的喧闹中，这样近距离地面对面，仿佛这一刻一切都静止了，或许这种场面都是影视剧里的东西，但是人生体验真的到了一种情境下这种感觉其实是非常自然并且你无法避免的，虽然很短暂，这看似转瞬间却又像永恒静止的时间里，我在她温和的笑颜中完全确定了我所找寻的恐怕就是这样的存在抑或是其所内含的心流。我努力表现我非常想为她拍一张照片，当然这时她虽不知所措但还是同意了。我掩饰紧张故作沉稳地举起从来只拍风景的手机终于第一次为一位女生摄影，一位我将来会为其付出一定心血的女性。我也很感激，她虽然很尴尬，笑容也有一丝地僵硬，但对于互为陌生人来说，她已经很尊重我们彼此的信任了。之后我道了谢之后害羞地走了。但是我久久地徘徊始终不肯离去，我总有一种预感，即使我加入了社团加入了群，也终究很难找到她，我相信自己的判断，事实上也确实如此。我最终还是在徘徊了八九圈之后又折回去厚着脸皮要了她的联系方式，当然她的态度从头至尾都是一脸狐疑的态度。我想主要原因还是我的外表或者是表现吧，我并没有太多对于处理陌生人生关系的技巧。 我开始了我人生的恐怕算是第一次的主动与一个陌生女孩的长期交流。我不知道促使我这样的理由是什么，也许是因为喜欢吧，或许我只能这么解释。但是我时常又不是非常的确定，因为我的感情是非常木讷的，甚至说我有时候连自己都不是非常爱，这样的人又怎么可能一直保持着爱着别人呢，我时常生活在对自己的一次又一次的自责中，从小到大一直如此。我总是在责备自己为什么在别人最需要帮助的时候，因为自己的害怕被拒绝而选择放弃事后却总是追悔莫及。小学上学走在路上，迎面走来一个初中女生，也许是因为低血糖直接朝我身上压过来，她自身已经没有支撑能力了，我没有防备被压倒在地，我的第一反应居然是吓得爬起来要逃走，事实上我确实这么做了。我一辈子也不能原谅自己，因为在我的记忆中，那时刚下过雨，地上都是积水，那个女生俯身趴在雨水中，孤独无助，失去了意识。而我却头也不回地跑了。我始终拒绝承认那是我，那是苍白无力的，完全是自欺欺人。还有一次是在高一的时候，我很关心的女生，在教室里独自一人哭泣着，教室里人很少，我本可以上前安慰她，明明我那么在乎她，不是因为喜欢，只是因为她的柔弱。但是至始至终我却什么都没有做。她一个人承受着一切。我可以解释小时候那是因为不懂事，弱小的人心灵总是不安定的，但是高中的我已经有了自己成熟的世界观与价值观了，却还是与七年前没有任何区别。那一刻我知道了我这些年没有丝毫成长，我总是在逃避，总是在得过且过，但谁又不是这样呢？只是我一直在寻求自身的问题的解决办法，我知道办法是不存在的，停滞不前永远没有收获，我只能向前。总之说了这么多，我是很难完整坚持我的感情的。但我却保持这样三天两头主动找人聊天的态势，维持了大概有一个半月。但是我是不会聊天的，况且人家也并不愿意与你聊下去。我想方设法要为她出点力，诸如帮她录视频，帮她剪辑舞蹈视频，我一遍又一遍地修改，改了一个版本后又改了一个版本。费了很多时间做出来的作品，我自己不满意，我试探性地说我不是很满意，你还要嘛？她只是一句那算了吧。我想做几次三番表示我想替她做后勤，多次表达想请她看电影，多次表达想购买她的手工作品。然后没有一个得到答复。我知道其实这都不算什么，对于她来说我只是一个陌生人，恐怕至始至终一直如此。我其实也并不觉得这些有什么，不过是一个好心人的多次善意的帮助罢了。只是一次又一次的徒劳，我没有累意是不可能的。恐怕《烟火》是个契机点吧，经过我个人邀请无果后，社团组织一次组团看电影，那我当然还是很兴奋的，终于又能见面了。我赶到电影院，他们已经在了，然后一堆人在看手机，背对着我，我走上前瞄了一眼，看到没人注意到我，我也就不好意思打招呼，便在旁边乱晃。她不久便注意到我，并且问：你也是我们后期部的小伙伴吗？我说是的。很温暖，她能注意到我。她让我自我介绍，我说我是…，她说：哦，原来是你啊（我能听出一丝失望），旁边的大家异口同声地说出我的网名，然后就在此时我举起手向大家打招呼的时候，所有人都不知是怎么都低下头看手机。我说不出来的感觉，从这一刻我就知道我不可能会融进这个圈子。而她带来的那个男生让我对她再也不会有任何兴趣了。不知为何，因为带男生来与自己一个人来完全就表示了她的心态。我专门挑了他们旁边买的票，想想真实煎熬啊。后面一排人全程的声音让我感觉我在菜市场，事实上这些我都能承受，最不能让我忍受的是，他们两个在一边看手机一边聊天。我喜欢的女生不会是在看电影时会表现出那样的表现。即使最让我有感触的地方，她好像也是无动于衷，并且一散场她便一溜烟地跑了。我不能接受，我实在不能接受。我独自坐了好久，一直到片尾曲放完，整场电影我连姿势都没变过，即使后面吵翻天，我连头都没歪过。我说不出来我到底是哪里难受，但我就是很痛苦，我飞也似地从影院出来，赶回宿舍，我什么都不想。但我知道，我不会再联系她了。他们都被网上连篇的差评左右了大脑。这部电影就是我的年少的幻影。典道一次又一次地想着如果那时那样就好了，只是为了跟她说一句：今天我想跟你在一起。仅仅是如此简单的话语，却因为胆小而如此大费周折。而我们呢？我呢？因为年少时的懦弱，害怕去爱人与被爱，对责任的恐惧，畏惧进入他人的内心世界，我犯了多少错误，但我永远也无法挽回，那个烟火是扁的世界永远是梦境中的一瞥。烟火绚烂却短暂，即使再美丽，却也转瞬即逝。一次又一次地轮回只为了这不过几秒的幸福，真的值得吗？这个问题恐怕是没有答案的。在这一小时里我看到了很多，我很好奇他们的从头笑到尾的看到了什么？她看到了什么？我内心只是有些痛楚，我不想再去多想。 我于你，止于唇齿，掩于岁月。 值得一提的是，这中间也因为发现了自己所爱而导致沉浸在自己打造的理想世界中，未免过于偏激而导致了令人失望的事情。大概是某天看到一篇关于谈“泡面爱情”的文章，以这样一个例子展开，具体是说到很多人在学生团体中，聚会时，男女一见面开口便是有没有另一半，没有要不要在一起试试之类的，抨击这种不加了解出于为了找而找，出于炫耀或者其他与爱情本质无关的理由而在一起的感情观。朋友曾经劝我为了更加了解她，便支招说约出来吃个饭，看个电影，行不行就在此时，不行就算。我极力反对这种做法，我只是一开始没明说，一是这样太过于直白，才认识几天就能做这些亲密的事，明显是出于恋爱而去的，可我的感情不是这么直白的；二是，我不希望以这种方式去博得见面的机会，进而两个人得以互相了解，我希望的是两个人在学习中，工作中，互帮互助中，点点滴滴中，能互相了解，让她感受到岁月中累积的善意，其中包含的深刻意味，从而我们开始能开始面对面的了解；三是靠这种方式建立的感情是不会长久的，因为在遭遇巨大挫折时，没有感情天秤的有力支撑点，换句话说就是只有一起经历过全身心的共同参与某件任务，或者共同承受某种苦难，或者共同享受某种快乐，在感情最为困难的时候会想起：那是我们在一起的理由，绝对不能动摇，那是最原初的寄托。四是我觉得这种娱乐会阻碍两个人更深入地了解，吃饭或者看电影只是在充实在一起的时间，这之间的交流是虚无没有内容的。虽然是共同参与但是我不会选择这种方式，只有两个人的心“共振”的时候，或许这种活动会非常幸福。便因此，我让友人去读这篇文章，答复是太长懒得读，让我解释，我还是抛出那句话：读书太少，想法太多。我们在感情观是起了巨大分歧，他认为自己没有持有这种“泡面爱情”的想法，但是这种建议本身就很欠缺考虑，我当然很感谢他的建议，但是在深入考虑后确实与我的想法大相径庭。他执着于文章本身过于矫情，怪我强加于这种帽子给他。我觉得我们确实该互相安静地待一会，遂一月有余没有任何交流。 时光匆匆，写到这里已经过去了几个日夜。该谈谈学习方向的事情了，我对计算机这个方向恐怕是无疑了，开学到现在学完了java，开始了C++和数据结构的课程。但是有一段时间迷茫的是，参与到实际项目中还是闭门造车好好自学基础知识。参与实际项目可以快速学习到很多网站的知识，并且能锻炼实际开发的能力，培养与人交流的能力，获奖了还能有助于考研的面试，然而我的储备知识基本很少，但是学起来也很快php这种东西门槛很低，所以我又觉得这么低门槛的东西真的有必要去学吗？对于不是科班毕业的人，培训几个月也能上手开发实际项目了，而我的目标并不止于此。仅仅是为了项目也没有深造的意义了，等我学完了数据结构、组原、操作系统再来学语言和现在去慢慢啃的速度完全是两个亮级。所以我虽然现在确定了我的想法是一心准备专业课的学习，但是刚开始那几个月真实焦头烂额，我到处实验室跑，一开始想搞一搞单片机，发现硬件不是我的目标。后来辗转往返创新实验室想参与网站开发，发现他们实现的功能确实很高大上，但却是拿现成的东西来“拼装”组成自己想要的东西罢了。即使花了很多精力去学会了，于我来说可能会有满足感，但是没有实际价值，项目这种东西对算法来说就显得低级了一点了。从陈老师那里终于得到了中肯的建议，对我这种情况不用考虑太多比赛的事情，最重要的还是好好深入地理解底层知识，比赛终究是浮云。 这段时间最让我窝一肚子火的恐怕是参加数学建模这个事情了。不是这个比赛本身的问题，而是队友的问题。一开始找不到人，在数学建模群里问，有人主动要求组队，看名字以为是男生，我觉得这么主动肯定是有责任心并且是有准备的啊，而且每句话都带着句号，合作肯定会很愉快，所以就欣然答应了。然后她又帮我找了一个队友，就这样队伍就组建成功了。从组建成功到比赛结束，我们总共就见过一次。比赛开始的第一天，基本对问题没怎么看。第二天我仔细看了一下，然后简单开了个小会，分配了一下任务，我对她们要求不高给我好好看看竞赛论文规范，然后搜集一下资料，敲一敲类似问题重述之类的文字就够了。最后一天，我花了整整一天建了四个模型，深夜收到她们给我的文稿，差点没气吐血，这么长时间敲点文字也能搞成这个样子。多次强调的格式一个都没做，好像在告诉我不会用Word改文字字体。我整合模型与文字到深夜最后发现有一个承诺书要手写签名，然后拍照上传，但是第二天一早就要交，我没办法，情急之下，只能用电子版充数，第二天早上起来打印签好名却上传不了了。早上怎么跟社团交流得到的回复都是规定就是规定，改变不了。说实话，几年来恐怕都没这么火大过，只是让她们好好看看仅仅只有两页纸的竞赛规定，都完成不了，里面明明有承诺书的要求，却没有一个人告诉我。我最后才知道这个事情，到底是什么样的原因，这么没有责任心的人会选择参加比赛？不过以她们这种行为，也告诉我依赖是一种多么可怕的事情。往后我对比赛这种事，兴趣没有那么大了。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Diary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java_castle]]></title>
    <url>%2Fjava-castle.html</url>
    <content type="text"><![CDATA[java类与对象的实例以及可扩展性 game.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394 package castle; import java.util.HashMap; import java.util.Scanner; public class Game { private Room currentRoom; private HashMaphandlers =new HashMap(); public Game() { //handlers.put("go", new HandlerGo()); handlers.put("bye", new HandlerBye(this)); handlers.put("help", new HandlerHelp(this)); handlers.put("go", new HandlerGo(this)); createRooms(); } private void createRooms() { Room outside, lobby, pub, study, bedroom; // 制造房间 outside = new Room("城堡外"); lobby = new Room("大堂"); pub = new Room("小酒吧"); study = new Room("书房"); bedroom = new Room("卧室"); // 初始化房间的出口 outside.setExit("east", lobby); outside.setExit("south", study); outside.setExit("north", pub); lobby.setExit("west", outside); pub.setExit("east", outside); study.setExit("north", outside); study.setExit("east", bedroom); bedroom.setExit("west",study); lobby.setExit("up", pub); pub.setExit("down", lobby); currentRoom = outside; // 从城堡门外开始 } private void printWelcome() { System.out.println(); System.out.println("欢迎来到城堡！"); System.out.println("这是一个超级无聊的游戏。"); System.out.println("如果需要帮助，请输入 'help' 。"); System.out.println(); showPrompt(); } // 以下为用户命令 public void goRoom(String direction) { Room nextRoom = currentRoom.getExit(direction); if (nextRoom == null) { System.out.println("那里没有门！"); } else { currentRoom = nextRoom; showPrompt(); } } public void showPrompt() { System.out.println("现在你在" + currentRoom); System.out.print("出口有："); System.out.print(currentRoom.getExitDesc()); System.out.println(); } public void play() { Scanner in = new Scanner(System.in); while ( true ) { String line = in.nextLine(); String[] words = line.split(" "); Handler handler=handlers.get(words[0]);//handler的子类赋给了父类 String value=""; if(words.length>1) value=words[1]; if(handler !=null) { handler.doCmd(value); if(handler.isBye()) break; } // if ( words[0].equals("help") ) { // printHelp(); // } else if (words[0].equals("go") ) { // goRoom(words[1]); // } else if ( words[0].equals("bye") ) { // break; // } // } } in.close(); } public static void main(String[] args) { Game game = new Game(); game.printWelcome(); game.play(); System.out.println("感谢您的光临。再见！"); }} Handler.java 1234567891011package castle;public class Handler {protected Game game;public Handler(Game game) { this.game=game;}public void doCmd(String word) {}public boolean isBye() {return false;}} HandlerBye.java 123456789101112131415 package castle; public class HandlerBye extends Handler { public HandlerBye(Game game) { super(game); } @Override public boolean isBye() { // TODO Auto-generated method stub return true; }} HandlerGo.java 123456789101112 package castle; public class HandlerGo extends Handler { public HandlerGo(Game game) { super(game); } @Override public void doCmd(String word) { game.goRoom(word); }} HandlerHelp.java 12345678910111213 package castle; public class HandlerHelp extends Handler { public HandlerHelp(Game game) { super(game); } @Override public void doCmd(String word) { System.out.println("迷路了吗？你可以做的命令有：go bye help"); System.out.println("如：\tgo east"); }} Room.java 12345678910111213141516171819202122232425262728293031323334353637 package castle; import java.util.HashMap; public class Room { private String description; private HashMap exits=new HashMap(); public Room(String description) { this.description = description; } public void setExit(String dir,Room room) { exits.put(dir, room); } @Override public String toString() { return description; } public String getExitDesc() { StringBuffer sb=new StringBuffer(); for(String dir:exits.keySet()) { sb.append(dir); sb.append(' '); } return sb.toString(); } public Room getExit(String direction) { return exits.get(direction); }} document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日记]]></title>
    <url>%2F%E6%97%A5%E8%AE%B0.html</url>
    <content type="text"><![CDATA[计数要从零开始。 11.2 我的动力来自于哪里？或许更多的来自于与移情对象的交流中来，一次次的寥寥几语让我感受到的是深深的自责，没有资本没有能力。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>Diary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于引力波新发现的总结]]></title>
    <url>%2F%E5%85%B3%E4%BA%8E%E5%BC%95%E5%8A%9B%E6%B3%A2%E6%96%B0%E5%8F%91%E7%8E%B0%E7%9A%84%E6%80%BB%E7%BB%93.html</url>
    <content type="text"><![CDATA[我与科学家的唯一相同点就是我可以跟他们一同喜怒哀乐。 首先可能要先声明一点就是，很多人认为大概持以下观点： 认为爱宣传天文的或者物理的数学的相关知识的人，只要你不是科学家，你就是在朋友圈装逼。 认为这些有的没的跟生活无关，并不是很感兴趣,时不时无聊了还喜欢抨击一下。 总是爱问这些发现有什么意义？对我买菜有什么帮助吗？ 谩骂与这些领域相关（大概是短期没有实际应用可能的）的专家的研究本身及其成果，认为这是在浪费人类的资源。 那我只有两个字：SB，呵呵。 先引用一段新华社消息： 10月16日晚间，美国、中国、德国、英国、法国等全球多国科学家联合宣布，人类第一次直接探测到来自双中子星合并的引力波信号。北京时间2017年8月17日20时41分，美国激光干涉引力波天文台(LIGO)、意大利Virgo探测器同时捕捉到了这个引力波信号GW170817；1.7秒钟之后，美国费米太空望远镜也观测到了同一来源发出的伽马射线暴GW170817A；之后不到11个小时，位于智利的Swope望远镜也报告在同一位置中观测到明亮的光学源。 此次新发现从观测到至全世界天文台决定合作发布仅历时二个月。 重大信号来源于中子星（密度是地球的1e+14倍）并合产生的引力波，电磁信号，可见光信号，Ｘ射线，伽马射线，从而形成了电磁波对应体。此次大概是1.15个和1.6个太阳质量的双中子星并合。 任意两个通过引力相互绕转的天体都会辐射引力波，只要天体之间的距离接近史瓦西半径，引力波才强到能被探测到。 以往只能探测到黑洞并合产生引力波，然而黑洞并合不发射光与电磁，只能“听见”其大概在600平方度（约3000个太阳组成的空间）的范围内。 清华大学工程物理系及天体物理中心教授，冯骅： 黑洞并合就像听故事，波澜壮阔的画面全靠想象；中子星并合更像声情并茂的电影，图像声音精彩丰呈。而这些不同“渠道”传递出来的信息，更容易让我们全面理解并合的物理过程。 北京大学李立新教授在上世纪发现了伽马暴与超新星的定量关系，和Princeton的Paczynski教授一起提出双中子星合并Li-Paczynski模型。 此次时空涟漪新发现的目前意义： 1.双中子星并合将揭示元素周期表上铁至铀这些元素的形成的起源。 2.“多信使”天文学的新纪元正式开启。（字面理解就是开始多途径研究，例如利用电磁波信号） 3.宇宙动力学，高维理论。 Ligo-Virgo Collaboration document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>Science Uranology</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《三体》随记]]></title>
    <url>%2F%E3%80%8A%E4%B8%89%E4%BD%93%E3%80%8B%E9%9A%8F%E8%AE%B0.html</url>
    <content type="text"><![CDATA[给岁月以文明，而不是给文明以岁月。 这句话原版出自于帕斯卡的：给时光以生命，而不是给生命以时光。 《三体》给我们描绘了一幅壮观的宇宙奇画，尤其是最后的二向箔对太阳系进行二维化中体现高级文明的认识超出了一般人的想象。故事开始于上世纪文化大革命时期，国家秘密进行宣称是进行军事研究实则是寻找外太空文明的“红岸工程”。叶文洁的父亲因为坚持宇宙大爆炸等资本主义理论被残害致死，由于她大学学习的是天体物理专业，身负罪名被破格邀请加入了“红岸工程”。不过在以后的工作中，她一直不清楚自己所在组织的真正目的，直到她随着不断深入工程的核心，雷政委才将真正的目的告诉了她。此后她便负责红岸的大功率电磁波发射器，发现了太阳能量镜面增益反射，向宇宙发射了人类历史上第一束深空电波，被４光年外的三体文明接受到，至此叶文洁便与三体文明开始了长达三年的深空通讯，直到她被发现，杀死了雷政委与她的丈夫杨卫宁。 ２１世纪，地 球三体组织开始建立虚拟游戏《三体》，模拟三体文明的运作，纳米物理学家汪淼在游戏中发现了三体文明是以恒纪元与乱纪元交替运作的，只有在恒纪元人类才能生存。事实上三体文明拥有两颗太阳，这个问题在数学上是不可解的。随着时间的推移军方终于发现了地球三体组织与三体文明所交流的所有信息。 军方开始行动，在地球三体组织的船舶开始通过巴拿马渡口时，利用事先固定安放好的横穿河流的数条纳米材料——“飞刃”（据说是世界上坚硬度最强的）横向切割了整俩船舶，船上所有被切割致死，保证了所有与三体有关的信息的完整性。 至此，三体文明的科技发达程度远超人类想象，他们可以将１１维的质子低维展开包围整个星球的平面，进行电路蚀刻从而创造出“智子”，再降回微观大小。他们拥有高度的智慧，以光速运动，可以影响宇宙微波背景辐射，两个智子之间因为量子纠缠产生的量子超距作用，他们之间的信息传递可以实现无限远而且是实时的。这样智子可以在地球研究基础物理学的粒子对撞机中来回 穿梭，毁坏实验结果，锁死了地球的技术突破。“智子”可以通过在人类眼球视网膜上进行光速运动形成文字与人类进行交流，但三体文明却有着致命缺点：由于其交流是通过心灵感应进行的，所以他们不会伪装，在人类这个种族面前，三体的心理隐藏能力为零。 整个人类社会进入了危机纪元，关键人物罗辑被任命为四名面壁人之一，他们的任务是欺骗所有人，获得人类最高的资源调动权限，进行拯救人类计划的研究。由于三体人不会识破谎言，所以这是一个精妙的计谋。相应的地球三体反抗组织也选出了四位破壁人，来揭露他们的阴谋。泰勒是四位面壁人最先被破壁的，他的方法是利用球状闪电攻击地球主力舰队，使其化为量子幻影，用这支宏观量子态的幽灵舰队去抵御三体文明。第二位被破壁的是雷迪亚兹，他的方法是在水星的地层中埋藏大量氢弹，一旦引爆改变水星的进动轨道，使得水星跌入太阳，太阳系瞬时会变得比三体星系更加恶劣，赌上全人类的生存。希恩斯的方法就更加可怕，研究思想钢印技术，对人类植入绝对失败主义的理念，从而让人类坚定逃离太阳系的想法。在危机纪元的２０５年，人类的科技已经到达空前高度，研究出了恒星级宇宙飞船和工质核聚变发动机，太空军规模已然发展到一定程度，地球人对末日之战充满信心。此时三体文明的探测器－－“水滴”（又名强相互作用探测器）到达太阳系，其本身只有卡车大小，人类第一位先驱－－物理学家丁仪与“水滴”进行了零距离接触，水滴表面光滑，即使放大一千倍还是光滑的，且坚硬无比，地球上所有的物质对它来说就是石头与水的区别，瞬时其发出发出了太阳核心般的超高温，所有先驱者瞬间汽化。 随后，“水滴”以１０倍、２０倍、３０倍的第一宇宙速度，摧毁了整个恒星级宇宙战舰阵列，每次撞击的都是核聚变发动机，瞬间引发核爆。最后以近乎光速轻而易举的摧毁了几乎所有的人类战舰。留下逃出升天的几艘战舰，已经意识到这场战争人类已经败了，开始自相残杀获取燃料与资源，用次声波氢弹瞬间灭绝了同伴，废墟中“蓝色空间”号以及“青铜时代”号向着太空深处永不回头的驶去。 不久之后，罗辑终于确认叶文洁曾经告诉他的： 第一，生存是文明的第一需要；第二，文明不断增长和扩张，但宇宙中物质总量保持不变。 这句话的所包含的一个可怕理论－－黑暗森林理论，宇宙深处存在太多文明，有和平自守的，有不断扩张的，也有等待捕猎的，当你对深空发出了特定时空坐标的电磁波信号，该位置便是黑暗森林深处随时被猎杀的猎物。 罗辑以此要挟三体文明，只要发出了太阳系的特定的空间坐标，届时不仅是太阳系三体文明也会被清除。三体文明不得不妥协，开始向地球分享科技，包括中微子发射台以及引力波发射天线。 历史跨入了威慑纪元，但在危机纪元有一件更重要的事，程心与云天明，程心是云天明的大学同学，可以说是唯一带给云天明希望的人，程心不知道从小到大一直自我封闭的云天明对自己情感。云天明在临终之际送了一个行星给程心。程心忍痛将云天明的大脑航行 取下，利用逐级推进技术，将他送给三体文明，希望通过三体的技术克隆再造一个云天明，作为一个间谍与三体生存在一起！ 威慑纪元的期，作为掌控引力波天线发射器的执剑人－－罗辑，结束了自己的使命将其转交给了程心，在程心接受转交的一瞬间，三体文明发动攻势，他们知道程心是不可能启动发射器将地球与三体置于打击之下的。三体占领地球后，开始命令全世界向澳大利亚移民，没过多久深空的“万有引力”号，发出了宣告一切结束的引力波波束。三体文明开始撤退，地球进入掩体纪元。 几年后，三体文明遭到打击，曾经的强大文明，在恒星爆炸中所有正在逃离的与没有逃离的烟消云散…… 地球希望在木星背后建立太空城，集体移民，逃离打击。但是就像拜占庭帝国的强大，作为曾经辉煌盛大的古罗马文明的延续，也一样逃离不了被灭绝的命运。在奥斯曼帝国攻进君士坦丁堡的那一刻，拜占庭人才意识到历史的洪流并没有偏袒谁。几亿年的地球文明，几千年的人类文明，在叶文洁向深空发射电波的那一刻起就已经宣布终结了。程心的选择是必然的，宇宙的进程也是必然的。更高智慧的文明－－“歌者文明”向太阳系发射了一颗二向箔，很快整个太阳系都被二维化了。程心通过由于云天明的情报所造出来的曲率推进飞船，用光速逃离了太阳系。 人类最后的种子，通过光速飞行了５２小时，事实上外面已经过了二百八十六年。 宇宙文明是纷繁复杂的，宇宙战争经久不衰，强大的宇宙文明运用的武器是宇宙规律，他们爱做的事是降低宇宙维度来杀死那些无法在低维生存的文明，还有一个方法是改变光速常数，降低光速。也许我们现存的宇宙在大爆炸之初并不是三维的，光速也不是３０万公里每秒。遗憾的是程心被困在曲率黑线中几天，外面已经过了一千八百万年，她最终还是没有见到云天明。 回到开始的那句话：给岁月以文明，而不是给文明以岁月。我想宇宙的浩瀚也许比《三体》描写的还要超越想象。但是我们可能永远无法去体会其丝毫。短暂的文明固然令人悲伤，但是如果以消极的态度不断地去延续最终会毁灭的东西，岂不是得不偿失了呢？在这转瞬即逝的一刻若能体会到爱的温暖，也就毫无遗憾了。《星际穿越》中也许更多的理论基础是引力的力量让库珀从五维空间传递信息给了墨菲，但是诺兰在更深处传递的是爱的力量穿越了一切，甚至超过了量子超距作用。爱是我们得以延续的前提，对亲人，家人的那一份情感浓度使我们存在过的证明。 生命的意义经过描画与刻画，对它的拷问便已经有了意义。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>Editor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《约翰·克利斯朵夫》]]></title>
    <url>%2F%E8%AF%BB%E3%80%8A%E7%BA%A6%E7%BF%B0%C2%B7%E5%85%8B%E5%88%A9%E6%96%AF%E6%9C%B5%E5%A4%AB%E3%80%8B.html</url>
    <content type="text"><![CDATA[“当你看到克利斯朵夫的面容之时，就是你将死不死的恶死之日。” 这是句话令人费解，却是作者真心的期望，希望克利斯朵夫在人生的考验中成为一个良伴与向导。 傅雷先生为此书献辞到： 真正的光明决不是永没有黑暗的时间，只是永不被黑暗所掩蔽罢了。真正的英雄决不是永没有卑下的情操，只是永不被卑下的情操所屈服罢了。 寥寥几笔却引起了无数自由灵魂在心中的共鸣。正如世人的评价，我的想法也是如此，那就是——这部小说不仅仅是一本小说，可以说它是一本纪实，一部英雄史诗的纪实。诸多夸赞之词在此就不再赘述。因为我想要表达的是这本书令我感动同时也是我想感动他人的地方。一千个读者便有一千个哈姆雷特，所以这本书面世（1912）的这一个多世纪以来，已有数不清的知识分子对它进行了数不尽的文学分析，社会价值分析与历史分析，产生了不计其数的文学观点，普世价值观点以及哲学观点。对于我而言，那是绝对做不来的并且也是绝对超越不了的，所以这篇文章并不是来对小说进行系统的研究，从而产生新的文学技巧抑或是文学材料，而是尽我所能的表达我的思想并且联系到我最近的生活以及遥远的生活，它所表达的思想对于我的帮助，并且希冀对于我的读者多多少少有些帮助。 克利斯朵夫为什么令我如此的如痴如醉？ 因为他是如此的真实，虽然后来的他超脱了常人的境界，但是一开始的他，童年时期，少年时期，青年时期的他与现在的我们是如此的相似，即使相隔一个世纪，即使相隔两个世界，即使相隔两个国家，但他依然给予我如此强烈的亲切感。童年时期的他在内心世界上是这样的，“随时随地有的是材料。单凭一块木头或是在篱笆上断下来的树枝（要没有现成的，就折一根下来），就能玩出多少花样！那真是根神仙棒。要是又直又长的话，它便是一根矛或一把剑；随手一挥就能变出一队人马。克利斯朵夫是将军，他以身作则，跑在前面，冲上山坡去袭击。要是树枝柔软的话，便可做一条鞭子。克利斯朵夫骑着马跳过危崖绝壁。有时马滑跌了，骑马的人倒在土沟里，垂头丧气的瞧着弄脏了的手和擦破了皮的膝盖。要是那根棒很小，克利斯朵夫就做乐队指挥；他是队长，也是乐队；他指挥，同时也就唱起来；随后他对灌木林行礼：绿的树尖在风中向他点头。”这不正是沈复的《幼时记趣》的真实写照嘛？小时候的我们不也是对着土堆对着沙堆对着树木，做着各种各样无穷的幻想，而自己却乐在其中。而这般细致的幻想重现早已经被我们抛诸脑后了，最简单的快乐如今永远的封闭在每个人内心的最深处。看到这样的深切入微的描写就好像让人感觉自己就是约翰，以为只有自己童年时期那样的充满对事物充满奇怪的联想，好比对于云的想象，对于桌子上纹路的想象一样，原来每个人都不例外，这样强烈的共鸣感可想而知。所以在这里克利斯朵夫的真实已经揪住了读者的心。还有比如他看见祖父米希尔在与邻人叫嚷似的聊天时，甚至以为邻人要将他的祖父杀死了，再比如他搂着母亲的时候“他多爱她！爱一切！一切的人与物！一切都是好的，一切都是美的……他睡熟了。”都体现童年时期的他是如此的真实，乐观，天真，心中充满了爱与能量，英雄气概。这一点影子在他以后的人生变化中，尤为让人痛心与怜悯。 南部的克利斯朵夫信仰的是新教，从宗教的角度，对于死亡总有个让人不那么绝望的解释。每个人在生命的各个时刻对于死亡都有各种各样的见解，对于青年时期的我们是非常恐惧的，幻想有朝一日自己的死亡是非常痛苦的。尤其是当身边有亲人离去时，这种忧虑会缠绕在心中，郁郁不欢，无法自拔。当至亲之人的死亡来临的一刻，所有的一切仿佛都不那么重要了，会自己想起之前为各种琐事而烦忧的自己是多么的幼稚，在死亡面前什么东西都显得一文不值。我们沉浸在失去的悲痛中，更确切的是沉浸在对生命之初的无限怀念之中。等到他稍大一点，家里人发现了他的音乐才华，便教他开始学音乐。他的可怜的父亲（曼希沃）与祖父（米希尔）希望他能实现他们未能实现的愿望，成为万人瞩目的音乐家与作曲家。但他们的理想本身就是迂腐的，他们之所以成功不了是因为他们只有弹奏演奏的技巧，或许曼希沃有弹钢琴的技巧，曼希沃有拉小提琴的技巧，但他们只是为演奏而演奏，却表达不了任何情感也没有任何思想，当然这在当时并不会怎么样，因为大家都是这样，所有的音乐家都隐藏自己，而听众也并没有什么欣赏水平，他们只是凑热闹罢了。另外理想迂腐是因为音乐家本来的高度就不是为了名垂千古，也许当你的音乐越来越被大多数人接受，随之而来的附带效益是你越来越出名，但是这终究不是你的目的。所以一味为这样的目标去努力，为了创作而创作，他们就一世也不能如己所愿。况且很多伟大的音乐家是死后才得以被众人所知晓，那么支撑着他们的只有坚定的真理信仰，艺术信仰。所以可怜的曼希沃和米希尔用自己的思想强加在克利斯朵夫身上，克利斯朵夫有自己的乐感，能感觉到什么样的技巧才更能达到艺术的魂魄，但每每他的意志与父辈相去甚远，他们就会用戒尺去打他，不许他这样弹，他们用自己固执迂腐的艺术思想去试图左右自认为乳臭未干的其实是天才的孙子儿子。孩子因此深藏了自己，即使这对于他童年的成长是有害的，会使身心受到很大的阻碍，但还不止这一点，以后还会遇到很多的痛苦需要他独自忍受，只要忍受过去，他的童年就算是结束了，但这些坎差点就把他压垮了。曼希沃有很多朋友，喜爱音乐却俗不可耐，谈论音乐时喜欢高谈阔论，含蓄的孩子痛苦不堪，让他都有点讨厌音乐，尤其是当谈论到他自己的音乐时，他都要放弃自己的音乐了。这和我们很多时候很相象，很讨厌庸俗的大人谈论自己喜爱的东西，一来他们根本就不懂只是把这些当作笑谈，二来他们总是带着什么也不屑一顾的态度，有时甚至常常要贬低某些东西，让人很是气愤。少年的克利斯朵夫经历了很多音乐上的，家庭上的，社会上的不如意的事情后，依然是很善良的，并且是充满爱的。比如他的两兄弟是狡猾又可耻讨人厌的，他也知道这一点，但只要这两兄弟假装跟他亲热一点，他就把整个心窝子掏给人家，亲王送的金表也毫不犹豫的送出去，并且会感动地流泪。因为他需要爱，他感受到的痛苦太多了，即使事后知道他们在自己背后窃窃私语骂他蠢，过一段时间他又是会上当的。他心中一直渴望着胜利，现在的在乐队弹奏的写着协奏曲的他并不是本体的他，这个庸俗的他跟未来的他根本不是一个样子，虽然有时他会觉得自己太过骄傲，他要叫自己屈辱，叫自己害臊，但他知道这世上没有一种思想，没有一桩行为，没有一件作品能将他自己完全的表现出来的。他充满着信仰将来的他一定能将自己完全显现出来的，只有不要被陷阱抓住，等到那个属于他的明天。我们何尝不是每天都在彷徨，总认为自己总是在失去。但我们从来没有一个坚定的思想，一个独特的属于自己的思想。我们更多想到的是今日的自己与当初所期望的那个自己天差地别，快要变成自己最讨厌的那种人了。沉浸在失去初心的痛苦循环中，永远跳不出来，永远想不到明天的自己会因为今天的忧虑而变成了今天的复制版本。因为我们没有独立的思想去克服这一切，去抗争这一切，那我们只能永远看不到明天。当他的祖父死去的那个瞬间，十五岁的孩子看到人生是一场无休、无歇、无情的战斗，凡是要做个够得上称为人的人，都得时时刻刻向无形的敌人作战：本能中那些致人死命的力量，乱人心意的欲望，暧昧的念头，使你堕落使你自行毁灭的念头，都是这一类的顽敌。他看到自己差点儿堕入深渊，也看到幸福与爱情只是一时的凄罔，为的是教你精神解体，自暴自弃。他问主无论怎么往前，结局（死亡）不是已经摆在那里了吗？主回答说：“啊，去死罢，你们这些不得不死的人！去受苦罢，你们这些非受苦不可的人！人不是为了快乐而生的，是为了服从我的意志的。痛苦罢！死罢！可是别忘了你的使命是做个人。——你就得做个人。” 南部的克利斯朵夫信仰的是新教，从宗教的角度，对于死亡总有个让人不那么绝望的解释。每个人在生命的各个时刻对于死亡都有各种各样的见解，对于青年时期的我们是非常恐惧的，幻想有朝一日自己的死亡是非常痛苦的。尤其是当身边有亲人离去时，这种忧虑会缠绕在心中，郁郁不欢，无法自拔。当至亲之人的死亡来临的一刻，所有的一切仿佛都不那么重要了，会自己想起之前为各种琐事而烦忧的自己是多么的幼稚，在死亡面前什么东西都显得一文不值。我们沉浸在失去的悲痛中，更确切的是沉浸在对生命之初的无限怀念之中。 当青年的克利斯朵夫同时失去祖父与父亲后，住进了老于莱一家。于莱说自己极喜欢音乐，要克利斯朵夫弹琴，但音乐一开场，老人便与其他人一起大声说话。就仿佛曲子是为这事增加兴致的。利斯朵夫气恼之下，不等曲子弹完就站了起来：可是谁也不注意。只有三四个老曲子，有极美的，也有极恶俗的，但都是大众推崇的，才能使他们比较的静一些，表示完全赞成。那时老人听了最初几个音就出神了，眼泪冒上来了，而这种感动，与其说是由于现在体会到的乐趣，还不如说是由于从前体会过的乐趣。虽然这些老歌曲也有克利斯朵夫极爱好的，例如贝多芬的《阿台拉伊特》，结果他都觉得厌恶了：老人哼着开头的几个小节，一边拿它们和”所有那些没有调子的该死的近代音乐”作比较，一边说着：“这个吗，这才叫做音乐。”——的确，他对近代音乐是一无所知的。这样无可救药的事情也发生在了我的周围。我的表妹的家境良好，从小便有机会学习钢琴，等到她稍微大一点的时候，她父亲工地上的那些小工就喜欢起哄让她弹琴来听，她也很开心地认为大家都喜欢她弹琴那是肯定她的才能并且认为他们喜欢音乐，可能她并没有想到这一层，只是因为周围的气氛，便弹起琴来。但音乐一开始，大家该聊天的聊天，该走开的走开，等音乐结束，房间里已经没有人了。只剩下我在那里，看着这一切感觉是多么的悲哀。我可怜的妹妹虽然脸上没有表情显现。但我知道她的内心一定是很后悔和难受的，而我也是很难过的，她成为了娱乐大众的牺牲品，虽然只有那么短短的一瞬，那也叫人够难过的了。这些人的行为跟老于莱又有几分区别呢？并且他们有过之而无不及，他们对于音乐是一窍不通的从来也不了解钢琴曲是个什么东西，不像老于莱还能道出几首曲子的名字，并且拿出来做比较，虽然比较的结果更加让人感受他的无知，但是这比现代的很多庸俗之物已经好很多了。所以我总结出也将之作为我以后的行为准则——不跟总是对自己不了解的事物保持轻蔑态度的人讨论任何新事物，更别说品味趣味低下的人了。并且这样的人只会认为你在显摆自己，你所有的行为只是为满足虚荣心的可耻行为。唯一的方法是不断增进自己，寻找真正值得交流的人去交流。 克利斯朵夫的蜕变是瞬间发生的事情。 不知不觉的某个时刻，他的身体不受控制，一片虚无，什么都思考不了并且总是要做出一些荒唐的举动。克利斯朵夫正在脱胎换骨，正在换一颗灵魂。他只看见童年时代那颗衰败憔悴的灵魂掉下来，可想不到正在蜕化出一颗新的，更年轻而更强壮的灵魂。一个人在人生中更换躯壳的时候，同时也换了一颗心；而这种蜕变并非老是一天一天的，慢慢儿来的：往往在几小时的剧变中，一切都一下子更新了，老的躯壳脱下来了。在那些苦闷的时间，一个人自以为一切都完了，殊不知一切还都要开始呢。 一个生命死了，另外一个已经诞生了。我们的蜕变往往是一生都完成不了的，或许某个瞬间就完成了。多年不见的老友完全跟换了个人似的，你以为他在这期间发生了蜕变，但某个动作，某句话语你会发现，这并不是蜕变，只是一种变化，一种可怕的世俗的打磨而产生的形状的变化，让你感到如此的陌生并且不敢靠近。真正的蜕变是令你眼前一亮的，会让你感到：啊，对啊，这才是完整的他，完整的自己。这样的过程的前奏是漫长的，并且是不断的苦难的叠加，然后形成思想的不断更迭进步，从而形成的必然结果。得过且过对于平和处事的人或许是一剂良药，但对于思想蜕变，人格蜕变却是一种无药可救的毒药。自从克利斯朵夫蜕变后他的思想迸发地更加激烈了。他忍受不了老于莱一家的虚伪，迂腐，总是拿责任来说事，没有一丝一毫的宽容心。他说这个责任反而会使他喜欢邪恶。他们拚命把”善”弄得可厌，使人不愿意为善。他们教人在对照之下，觉得那些虽然下流但很可爱的人倒反有种魔力。到处滥用责任这个字，无聊的苦役也名之为责任，无足重轻的行为也名之为责任，还要把责任应用得那么死板，霸道，那非但毒害了人生，并且亵渎了责任。责任是例外的，只有在真正需要牺牲的时候才用得着，绝对不能把自己恶劣的心绪和跟人过不去的欲望叫做责任。一个人不能因为自己愚蠢或失意而悲苦愁闷，就要所有的人跟他一块儿悲苦愁闷，跟他一样过那种残废的人的生活。最重要的德性是心情愉快。德性应该有一副快活的，无拘无束的，毫不勉强的面目！行善的人应该觉得自己快乐才对！但那个永不离嘴的责任，老师式的专制，大叫大嚷的语调，无聊的口角，讨厌的、幼稚的、无中生有的吵架，那种闹哄，那种毫无风趣的态度，没有趣味、没有礼貌、没有静默的生活，竭力使人生变得疲乏的、鄙陋的悲观主义，觉得轻蔑别人比了解别人更容易的、傲慢的愚蠢，所有那些不成起局、没有幸福、没有美感的布尔乔亚道德，都是不健全的，有害的，反而使邪恶显得比德性更近人情。克利斯朵夫这样想着，只顾对伤害他的人泄忿，可没有发觉自己和他们一样的不公平。无疑的，这些可怜虫大致和他心目中所见到的差不多。但这不是他们的错：那种可憎的面目，态度，思想，都是无情的人生造成的。他们是给苦难折磨得变了形的，——并非什么飞来横祸，伤害生命或改换一个人面目的大灾难，——而是循环不已的厄运，从生命之初到生命末日，点点滴滴来的小灾小难……那真是可悲可叹的事！因为在他们这些粗糙的外表之下，藏着多少的正直，善心，和默默无声的英勇的精神！……藏着整个民族的生命力和未来的元气！可悲可叹，但这就是那个时代残酷的事实。 青年时期的克利斯朵夫的爱情也是现代人爱情的真实写照。 他因为郁郁不得欢就把自己的初次给了一个轻浮的女子，并且所有人都对他产生了唾弃感，认为作为高雅的音乐家，他行为处事竟然如此的随便。确实此时的他处在人生的低谷期，但是不经历这些爱情，他怎么才能知道什么是真正的爱情呢？因为他是如此的正直，不知道一点点的邪恶。他认为爱谁是他自己的事情，跟这些世人又有什么关系，他们有什么权利来干涉他的感情自由？凭着这样的思想，他固执地并且理所当然地爱上了那个叫阿达的女人。而这个阿达是多少现代女子的真实写照呢？她不够聪明，不知道在一个象克利斯朵夫那样生机蓬勃的人身上，想法使她的爱情与日俱新。在这次爱情中间，她的感官与虚荣心已经把所有的乐趣都榨取到了。现在她只剩下一桩乐趣，就是把爱情毁灭。她有那种暧昧的本能，为多少女子（连善良的在内）多少男人（连聪明的在内）所共有的。——他们都不能在人生中有所创造：作品，儿女，行动，什么都不能，但还有相当的生命力，受不了自己的一无所用。他们但愿别人跟自己一样的没用，便竭力想做到这一点。有时候这是无心的；他们一发觉这种居心不良的欲望，就大义凛然的把它打消。但多数的时候他们鼓励这种欲望，尽量把一切活着的，喜欢活着的，有资格活着的，加以摧毁；而摧毁的程度当然要看他们的力量如何：有些是小规模的，仅仅以周围亲近的人作对象；有些是大举进攻，以广大的群众为目标。把伟大的人物伟大的思想拉下来，拉得跟自己一般高低的批评家，还有以引诱爱人堕落为快的女孩子，是两种性质相同的恶兽。——可是后面的一种更讨人喜欢。现代的很多女人岂不都是这样？钱欲两样已经占据了她们全部的生活，爱情早已经被埋葬了。并且她们容易厌倦，也喜欢埋葬爱情。这些女人的笨不仅是天生的也是自己不断麻木地对待世事造成的。社会上的女子如此，校园里现在这样的现象还少吗？追求财富与欲望的，追求名气的，默默无闻整天只知道娱乐一切，一事不做只知吃喝睡的，这样的情形不胜枚举。从前充满学术氛围的校园到如今已经是凤毛菱角了。此外，这种卑劣的心理是多少人的通病，自己完成不了的事就认为别人也完成不了，自己不能成为伟大的人，就认为跟他同时代的人都不能成为伟大的人。即使是贝多芬，毕加索，跟他同一个时代，他也认为他们一事无成。即使是再庸俗无知的人，比他早出生个几百年，他也认为他们一定是伟大的人。自己碌碌无为，一事无成，就必须相信别人也都是白活了一辈子。这不是没有鉴赏力，他就是瞧不起一切现代的东西，不愿意去相信。一般病歪歪，怨天尤人的可怜虫，彼此接近的最大理由就是能够同病相怜，在一块儿怨叹。他们为了自己的不快乐就否认别人的不快乐，便是这批俗物的无聊的悲观主义，最容易是健康的人认识到健康的重要。这种现象对于很多人来说既熟悉又陌生，因为他们多多少少经历过这样的阶段，随后便厌恶了。 在克利斯朵夫的生命中，高脱弗烈特舅舅占据了不可或缺的一部分，有好几次舅舅都拯救了他的灵魂。虽然舅舅是总是被人耻笑的乐天派，但他简单单纯的心理是克利斯朵夫的导向标。克利斯朵夫因为不受人理解，有段时间天天酗酒，没有人能劝阻他。舅舅适时地出现了，说他看见了克利斯朵夫就像看见了曼希沃。克利斯朵夫这时才意识到，自己是多么地糊涂，自己的这些行为与当初堕落的父亲有什么区别。当初在父亲的遗骸旁边起下的誓，如今他又做到了什么，思想，灵魂，艺术，全都被他背叛了。这一年来都是糟蹋过去了。是的，他做了一个他不愿意做的人，这便是他生活的总账。他从此以后再也没这样过，因为至此以后他终于要迎来他生命中最重要的时刻，那便是反抗。 克利斯朵夫的反抗是空前绝后的，是不顾一切的，是从上至下的，是席卷一切的。 他批判了整个德国音乐界。从钢琴师，小提琴家，指挥家，作曲家甚至连批评家和群众也不放过。他认识到一切的错误，一切的虚伪，便不顾一切的去指责。一切民族，一切艺术，都有它的虚伪。人类的食粮大半是谎言，真理只有极少的一点。人的精神非常软弱，担当不起纯粹的真理；必须由他的宗教，道德，政治，诗人，艺术家，在真理之外包上一层谎言。这些谎言是适应每个民族而各不同的：各民族之间所以那么难于互相了解而那么容易彼此轻蔑，就因为有这些谎言作祟。真理对大家都是一样的，但每个民族有每个民族的谎言，而且都称之为理想；一个人从生到死都呼吸着这些谎言，谎言成为生存条件之一；唯有少数天生的奇才经过英勇的斗争之后，不怕在自己那个自由的思想领域内孤立的时候，才能摆脱。“他把德国艺术赤裸裸的看到了。不论是伟大的还是无聊的，所有的艺术家都婆婆妈妈的，沾沾自喜的，把他们的心灵尽量暴露出来。有的是丰富的感情，高尚的心胸，而且真情洋溢，把心都融化了；日耳曼民族多情的浪潮冲破了堤岸，最坚强的灵魂给冲得稀薄，懦弱的就给淹溺在它灰色的水波之下：这简直是洪水；德国人的思想在水底里睡着了。象门德尔松，勃拉姆斯，舒曼，以及等而下之的那些浮夸感伤的歌曲的小作家，又有些怎么样的思想！完全是沙土，没有一块岩石。只是一片湿漉漉的，不成形的黏土……这一切真是太荒唐太幼稚了，克利斯朵夫不相信听众会不觉得。但他向周围瞧了一下，只看见一些恬然自得的脸，早就肯定他们所听到的一定是美的，一定是有趣的。他们怎么敢自动加以批评呢？对于这些人人崇拜的名字，他们是非常尊敬的。并且有什么东西他们敢不尊敬呢？对他们的音乐节目，对他们的酒杯，对他们自己，他们都一样的尊敬。凡是跟他们多少有些关系的，他们心里一概认为”妙不可言”。克利斯朵夫把听众与作品轮流打量了一番，觉得作品反映听众，听众也反映作品。克利斯朵夫忍俊不禁，装着鬼脸。等到合唱班庄严的唱起一个多情少女的羞怯的《自白》，他再也抑止不住，竟自大声的笑了。四下里立刻响起一气愤怒的嘘斥声。邻座的人骇然望着他，而他一看到这些吃惊的脸更笑得厉害，甚至把眼泪都笑了出来。这一下大家可恼了，喊着：“滚出去！”他站起来走了，耸耸肩膀，笑得浑身扭动。全场的人看了都气愤之极。从此克利斯朵夫就慢慢的跟他城里的人处于敌对的地位。” 诚然，他确实看到了很多德国音乐的虚伪。克利斯朵夫天真地以为大家都没经历过人生，只有自己才能重新来一遍，只有自己才能把这些表现的真切。那可错了，因为他充满热情，所以在自己的作品里不难发见热情，但除了他以外谁也不能辨别出来。他所批评的音乐家大多都是这样的，他们心中所有的，变现出来的，的确是深刻的感情，但语言已经随着肉体死去了。“克利斯朵夫不懂得人的心理，根本没想到这些理由：他觉得现在是死的一向就是死的。他拿出青年人的霸道与残忍的脾气，修正他对过去的艺术家的意见。最高贵的灵魂也给他赤裸裸的揭开了，所有可笑的地方都没有被放过。”门德尔松的过分忧郁，韦伯的有理想色彩却又卖弄技巧，舒伯特多愁善感，甚至巴赫脱不了诳语与唠叨。他认为他们的音乐差不多全是“建筑”起来的，一种情绪用音乐修辞学加以扩大，一种简单的节奏循环铺张，机械地翻覆。这种奏鸣曲与交响乐是房屋式的结构令他大为气恼。当然他的见解并不一定是对的，那时的他还不懂得音乐的条理之美，深思熟虑的结构之美，他以为这是建筑家的工作不是音乐家的工作。此外这段时期，是他反抗所有童年时代的偶像的时期，这种反抗是应当的。人生有一个时期应当敢把不公平，敢把别人敬重佩服的东西给否认。不管是真理还是谎言一并摈弃，敢把没有经过自己认为是真理的东西统统否认。要成为健全的人，少年时期第一件事就是把宿食统统呕吐干净。 克利斯朵夫看到那么多的自己否认的东西，他就照旧做自己的曲子。他指责别人，但自己的创作也一样不能避免。创作是一种需要，把大多数的情操的谎言与浮夸的表现都认出来 ，仍不能避免自己重蹈覆辙，因为那主要是靠长期坚苦努力的，并非是一朝一夕所能完成的了的。克利斯朵夫也没认识到缄默的好处，也没到能够缄默的年龄。由于父亲的遗传他爱大声说话，拼命想改却使自己的一部分的精力变得麻木了。祖父也给他出了难题，让他很难将自己准准确确地表达出来。作为演奏家，卖弄技巧有很大的诱惑，那是纯粹肉体方面的快感。运用肌肉，克服困难，炫耀本领，一个人控制着成千上万的人的快感。虽然这样的事情对与一个青年人来说是允许的，但是对于艺术来说终究是致命的。这样的重负他难以摆脱，他摇摇欲坠，无法自拔。不断地前行，他越对于自己感到愤怒与痛苦。这些寄生虫黏附在他的身上难以除去，使他有时甚至迷失了自我。所以他想通过现场演奏来找回真正艺术的感觉。但大家对于他这样的在艺术方面乳臭未干的孩子是不大宽容的，也对于他抱有偏见。一曲又一曲的音乐奏下去，场下的回应就是鸦雀无声，一片寂静，一无所有。每一句音乐都掉在了漠不关心的深渊里。凡是真正的艺术家都有一种精神上的触觉，感受到他的音乐能否在观众心理产生精神上的触觉。但那股沉闷的空气是他的心都凉了。终于奏完了，大家冷冰冰的拍手，便是怪叫一声也好，至少有点生命的表示，有点反响……可是什么都没有。这是有原因的，他的作品还不够成熟，太过于新鲜以至于大家不能一下子就懂得，并且都喜欢把这肆无忌惮的青年给教训一顿。他相信成功是一蹴而就的，但是目前德国人的鼻子太过于脆弱接受不了新作品的香浓，本能的觉得新作品就是站不住脚的，只有经过历史沉淀的东西才是有价值的东西。他的朋友支持他是因为他们希望他跟他小时候一样的庸俗，一直停留在那一个阶段不希望他去做出变化，这算不上是朋友，并且他的内心存着一股强大的勇往直前的力，他想不断的变化。克利斯朵夫不理解这些，他想群众如果一时不接受他就一向不要接受他，他企图改变整个德国的口味，殊不知得宽容地把自己的思想先整理好了，才能让别人听他的。终于他忍受不了了，进入了批评界开始了他的笔伐之行。他以“音乐太多了”开始了批判的长篇大论。音乐太多了，吃的东西太多了，喝的东西太多了！大家不饥而食，不渴而饮，不需要听而听，只是为了狼吞虎咽的习惯。说民族是什么都吃的，给他们什么都好，一概照收。“他们愣头傻脑的笑着，几小时的吸收声音，声音，声音。他们一无所思，一无所感，只象一些海绵。真正的欢乐与真正的痛告，——力，——决不会象桶里的啤酒般流上几小时的。它掐住你的咽喉，使你惊心动魄的慑服，以后你不会再想要别的：你已经醉了！” 他批判的最令人痛心的应该是大家对于音乐家呕心沥血而做成的音乐的态度了。对贝多芬的《弥撒祭乐》，惊心动魄的气势，最后的审判，在观众们的心里是一无所见，一无所感，一无所悟，一无所闻的。“一个艺术家的痛苦为你们原来只是一出戏，认为贝多芬临终的血泪给描写得非常精细！你们对耶稣上十字架竟喊着’再来一次！’这个超凡入圣的人在痛苦中挣扎了一辈子，结果只给你们这批愚夫愚妇消磨一个钟点！……”他引用歌德的名言：人们总是在嘲笑崇高与伟大，但当他们真正认识到了崇高与伟大，便连抬头看一眼的勇气也没有了。虽然他还没有深刻理解他的含义，但无疑它的份量是巨大的。现如今有多少商家拿贝多芬的名曲在为他们那无耻的商品在做宣传呢？ 克利斯朵夫在这一个阶段失去了所有的朋友，因为他批判了所有人，他是如此的清白，容不得半点浑水，所以大家对于他的痛恨是咬牙切齿的。他本来也没有几个真心的，志同道合的朋友，现如今更是没有了。他是真的孤独了。后来他认识了一个瞎女，本来是对生活失去了希望。被他舅舅引导了对生活充满了希望，甚至比失明前更加热爱生活。他想到他以前对于德国的理想精神是有偏见的了，看到了它的伟大。它的信念之美在于能在现在的世界上创造另一个截然不同的世界，虽然是一个谎言，但倘若把支持这些可怜虫的幻想加以破灭，那也是极大的恶行。然后他想到而我也认为是这样的，那就是艺术不是幻想，而是真理。我们要感受生命强烈的气息，看见真相，正视苦难。 此外，罗兰的很多观点也是鞭辟入里的。 生活中，我们能感受到却表达不出的。对父亲的崇拜，即使是父亲犯了错也并没有改变。这在儿童是一种天然的需要。也是自我之爱的一种方式。倘使儿童自认为没有能力实现心中的愿望，满足自己的骄傲，他就拿这些去期望父母；而在一个失意的成人，他就拿这些去期望儿女。在儿童心中，父母便是他自己想做而做不到的人物，是保卫他的人，代他出气的人；父母心中的儿女亦然如此，不过要等将来罢了。在这种“骄傲的寄托”中间，爱与自私便结成一片，其奋不顾身的气势，竭尽温存的情绪，都达到沉醉的境界。父母与子女之间要能底的推心置腹，哪怕彼此都十二分的相亲相爱，也极不容易办到：因为一方面，尊敬的心理使孩子不敢把胸臆完全吐露；另一方面，有自恃年长与富有经验那种错误的观念从中作梗，使父母轻视儿童的心情，殊不知他们的心情有时和成人的一样值得注意，而且差不多永远比成人的更真。 克利斯朵夫听见隐隐的炮声在响了，快要把这垂死的文明，这一息仅存的小小的希腊轰倒了。虽然如此，克利斯朵夫对这件作品依旧抱着好感；是不是因为他有点儿又轻视又怜悯的缘故呢？总之，他对它的关切远过于他口头的表示。他走出戏院回答高恩的时候，尽管口口声声说着”很细腻，很细腻，可是缺少奔放的热情，音乐还嫌不够”，心里却绝对不把《佩莱阿斯》和其余的法国音乐一般看待。他被大雾中间的这盏明灯吸住了。他还发见有些别的光亮，很强的，很特别的，在四下里闪耀。这些磷火使他大为错愕，很想近前去瞧瞧是怎么样的光，可是不容易抓握。克利斯朵夫因为不了解而更觉得好奇的那般超然派的音乐家，极难接近。克利斯朵夫所不可或缺的同情，他们完全不需要。除了一二个例外，他们都不看别人的作品，知道得很少，也不想知道。他们几乎全部过着离群索居的生活，由于故意，由于骄傲，由于落落寡合，由于憎厌人世，由于冷淡，而把自己关在小圈子里。这等人虽为数不多，却又分成对立的小组，各不相容。他们的小心眼儿既不能容忍敌人和对手，也不能容忍朋友，——倘使朋友敢赏识另外一个音乐家，或是赏识他们而用了一种或是太冷淡，或是太热烈，或是太庸俗，或是太偏激的方式。要使他们满足真是太难了。结果他们只相信一个得到他们特许的批评家，一心一意坐在偶像的脚下看守着。你决不能去碰这种偶像。——他们固然不求别人了解，他们对自己也不怎么了解。他们受着奉承，被盟友的意见和自己的评价改了样，终于对自己的艺术和才具也弄模糊了。一般凭着幻想制作的人自以为是改革家，纤巧病态的艺术家自命为与瓦格纳争雄。他们差不多全为了抬高声价而断送了自己；每天都得飞跃狂跳，超过上一天的纪录，同时也要超过敌人的纪录。不幸这些跳高的练习并不每次成功，而且也只对几个同行才有点儿吸引力。他们既不理会群众，群众也不理会他们。他们的艺术是没有群众的艺术，只从音乐本身找养料的音乐。但克利斯朵夫的印象，不论这印象是否准确，总觉得法国音乐最需要音乐以外的依傍。这株体态起娜的蔓藤似的植物简直离不开支柱：第一就离不开文学。它本身没有充分的生命力，呼吸短促，缺少血液，缺少意志，有如弱不禁风的女子需要男性扶持。然而这位拜占庭式的王后，纤瘦，贫血，满头珠翠，被时髦朋友，美学家，批起家，这些宦官包围了。民族不是一个音乐的民族；二十余年来大吹大擂的捧瓦格纳，贝多芬，巴赫，德彪西的热情，也仅仅限于一个阶级。越来越多的音乐会，不惜任何代价鼓动起来的、声势浩大的音乐潮流，并不是因为群众的趣味真正发展到了这个程度。这是一种风起云从的时髦，影响只及于一部分优秀人士，而且也把他们搅昏了。真正爱好音乐的人屈指可数，而最注意音乐的人如作曲家批评家，并不就是最爱好的人。在法国，真爱音乐的音乐家太少了！克利斯朵夫这么想着，可忘了这种情形是到处一样的，真正的音乐家在德国也不见得更多，在艺术上值得重视的并非成千成万毫无了解的人，而是极少数真爱艺术而为之竭忠尽智的孤高虔敬之士。这类人物，他在法国见到没有呢？不论是作曲家或批评家，最优秀的都是远离尘嚣而在静默之中工作的，例如法朗克，例如现代一般最有天分的人；多少艺术家过着没世无闻的生活，让以后的新闻记者争着以最先发见他们，做他们的朋友为荣；还有少数勤奋的学者，毫无野心，不求名利，一点一滴的把法兰西过去的伟大发掘出来；另外一批则是献身于音乐教育，为法兰西未来的光荣奠定基础。其中有多少聪明才智之士，性灵的丰富，胸襟的阔大，兴趣的广博，一定能使克利斯朵夫心向神往，要是认识他们的话。但他无意之间只瞧见了二三个这种人物，而他所了解的，见到的，又是他们被人改头换面的思想。克利斯朵夫只看到作者的缺点，被那些摹仿的人和新闻界的掮客抄袭而夸大的缺点。克利斯朵夫对那些音乐界的俗物尤其感到恶心的，是他们的形式主义。他们之间只讨论形式一项。情操，性格，生命，都绝口不提！没有一个人想到真正的音乐家是生活在音响的宇宙中的，他的岁月就等于音乐的浪潮。音乐是他呼吸的空气，是他生息的天地。他的心灵本身便是音乐；他所爱，所憎，所苦，所惧，所希望，又无一而非音乐。一颗音乐的心灵爱一个美丽的肉体时，就把那肉体看作音乐。使他着迷的心爱的眼睛，非蓝，非灰，非褐，而是音乐，心灵看到它们，仿佛一个美妙绝伦的和弦。而这种内心的音乐，比之表现出来的音乐不知丰富几千倍，键盘比起心弦来真是差得远了。天才是要用生命力的强度来测量的，艺术这个残缺不全的工具也不过想唤引生命罢了。但法国有多少人想到这一点呢？对这个化学家式的民族，音乐似乎只是配合声音的艺术。它把字母当作书本。克利斯朵夫听说要懂得艺术先得把人的问题丢开，不禁耸耸肩膀。他们却对于这个怪论非常得意：以为非如此不足以证明他们有音乐天分。象古耶这等糊涂蛋也是这样。他从来不懂一个人如何能背出一页乐谱，——（他曾经要克利斯朵夫解释这个神秘），——如今却向克利斯朵夫解释，说贝多芬伟大的精神和瓦格纳刺激感官的境界，对于音乐并不比一个画家的模特儿对于他所作的肖像画有更大的作用！“这就证明，”克利斯朵夫不耐烦的回答说，“在你们眼里，一个美丽的肉体并没有艺术价值！一股伟大的热情也没有艺术价值！唉，可怜虫！……你们难道没想象到一张妩媚的脸为一幅肖像画所增加的美，一颗伟大的心灵为一阕音乐所增加的美吗？……可怜虫！……你们只关心技巧是不是？只要一件作品写得好，不必问作品表现些什么，是不是？……可怜虫！……你们仿佛不听演说家的辞句，只听他的声音，只莫名片妙的看着他的手势，而认为他说得好极了……可怜的人啊！可怜的人啊！……你们这些糊涂蛋！”克利斯朵夫所着恼的不单是某种某种的理论，而是一切的理论。这些清谈，这些废话，口口声声离不开音乐而只会谈音乐的音乐家的谈话，他听厌了。那真会教最优秀的音乐家深恶痛绝。克利斯朵夫跟穆索尔斯基一样的想法，，以为音乐家最好不时丢开他们的对位与和声，去读几本美妙的书，或者去得点儿人生经验。光是音乐对音乐家是不够的：这种方式决不能使他控制时代而避免虚无的吞噬……他需要体验人生！全部的人生！什么都得看，什么都得认识。爱真理，求真理，抓住真理，——真理是美丽的战神之女，阿玛仲纳的女王，亲吻她的人都会给她一口咬住的！ 象保障你们的一样？可怜的克拉夫脱先生！你们所谓独立的保障也不见得怎么可靠！……可是那至少是你们喜欢的事业。我们可又配做些甚么呢？没有一件事情使我们感到兴趣。——是的，我知道，我们现在什么都参加，假装关心着一大堆跟我们不相干的事；我们多么需要能关心一点儿什么！我跟旁人一样参加团体，担任慈善会的工作，到巴黎大学去上课，听柏格森和于尔·勒曼脱的讲演，听古代音乐会，古典作品朗诵会，还做着笔记，笔记……我自己也不知道记些什么！……我骗自己，以为这些是我所热爱的，或者至少是有用的。啊！我明明知道不是这么回事，我对什么都不在乎，对什么都腻烦！……我这样把每个人的思想老实告诉了你，你可不能瞧不起我。我并不比别的女人更蠢。可是哲学，历史，科学，究竟跟我有什么相干？至于艺术，——你瞧——我乱弹一阵，东涂西抹，涂些莫名片妙的水彩画；——难道这些就能使一个人的生活不空虚了吗？我们一生只有一个目的：就是嫁人。可是嫁给那些我跟你看得一样明白的家伙，你想是有趣的吗？唉，我把他们看透了。我没有你们德国多情女子的那种运气，会自己造些幻象……噢，太可怕了！看看周围的人，看看已经结婚的女子，看看她们所嫁的男人，想到自己也得跟她们一样，让身心变质，跟她们一样的庸俗！……我敢说，没有艰苦卓绝的精神决计受不了这种生活种种义务。而那种精神就不是每个女子都能有的……光阴如流矢，日月如穿梭，一眨眼青春就完了；可是我们心中究竟藏着些美的，好的东西，——只是永远不加利用，让它们一天天的死灭，结果还得拿去送给我们瞧不起，而将来也要瞧不起我们的蠢货！……并且没有一个人了解你！人家说我们是一个谜。那些男人觉得我们乏味，古怪，倒也罢了。女人应该是懂得我们的啊！她们是过来人，只要回想一下自己的情形就得了……事实可不是这样。她们决不给你一点帮助。便是做我们母亲的也不了解我们，也不真心想认识我们。她们只打算把我们嫁人。除此以外，死也罢，活也罢，都归你自己去安排！社会把我们完全丢在一边。” 他是孤独的。他自以为孤独的。 可是志气一点儿不消沉。他再没有从前在德国时那种悲苦郁闷的心境。他更强了，更成熟了；他知道是应该这样的。他对巴黎的幻想已经没有了：人到处都是一样的；应当忍受，不该一味固执，跟社会作无谓的斗争；只要心安理得，我行我素就行了。象贝多芬所说的：“要是我们把自己的生命力在人生中消耗了，还有什么可以奉献给最高尚最完善的东西？ “他清清楚楚的体验到了自己的性格，也体验到了他从前批判得那么严厉的自己的种族。越受到巴黎气氛的压迫，他越觉得需要回到祖国，回到国魂所在的那些诗人与音乐家的怀抱中去。他一打开他们的书，仿佛满屋子都是阳光灿烂的莱茵的波涛，和那些被他遗弃的故人的亲切的微笑。他曾经对他们多么无情无义！他们那种其实的慈爱的宝藏，他怎么不早点儿发见的呢？他不胜羞愧的想起自己从前在德国对他们说过多少偏激与侮辱的话。那时他只看见他们的缺点，笨拙而多礼的举动，感伤的理想主义，小小的谎言，小小的懦怯。啊！这些缺点跟他们伟大的德性相比，真是太不足道了！可是他当初怎么对他们的弱点会那样苛刻的呢？此刻他反因之而觉得他们更动人，更近人情了。在这个情形之下，他现在最受吸引的人便是以前被他用最蛮横的态度贬斥的人。对于舒伯特和巴赫，他有什么不客气的话没说过呢！如今他倒觉得跟他们非常接近。那些伟大的心灵，受过他的挑剔与讪笑的，对他这个亡命异国，举目无亲的人，笑容可掬的说着：“朋友啊，我们在这里。你勇敢些罢！我们也受过非分的苦难！……可是临了我们还是达到了目的……”于是他听见约翰·赛巴斯蒂安·巴赫的心灵象海洋一般的呼啸着：风狂雨骤，掩盖生命的乌云都给扫荡了，——有极乐的，痛苦的，如醉如狂的民众，有慈悲与和气的基督在他们上空翱翔，——多少城市被守夜的人叫醒了，居民欢欣鼓舞的迎着神明走去，他的脚声把世界都震撼了，——无数的思想，热情，乐体，英雄生活，莎士比亚式的幻想，萨伏那洛式的预言，牧歌式的，史诗式的，《启示录》式的幻象，蕴藏在这个歌唱教师身上！克利斯朵夫好象亲眼看到他这个人：双叠下巴，眼睛很小很亮，多褶的眼皮，往上吊的眉毛，性格阴沉而又快乐，有点可笑，脑子里充满着讽喻和象征，人是老派的，易怒，固执，心情高远，对人生抱着热情，同时又渴念着死……——在学校里，他是一个天才的学究，而那些学生是又脏又粗野，生着疮疖，象乞丐一般，唱歌的嗓子是嗄的，他常常跟他们吵架，有时和他们扭殴……——在家里他有二十一个孩子，十三个都比他死得早，其中一个是白痴；其余都是优秀的音乐家，替他来些小小的家庭音乐会，……疾病，丧葬，争吵，贫困，侘傺不遇；——同时，他有他的音乐，他的信仰，解脱与光明，还有预感到的，一意追求而终于抓握到的欢乐，——神明的气息锻炼着他的筋骨，耸动着他的毛发，在他嘴里放出霹雳般的声音……噢！力！力！象雷震一般的欢乐的刀！……克利斯朵夫把这股力尽量吞下。他觉得在德国人心灵中象泉水般流着的这种音乐的力对他很有好处。这力往往是平庸的，甚至是粗俗的，可是有什么关系？主要的是有这股力，而且能浩浩荡荡的奔流。在法国，音乐是用滤水器一点一滴的注在瓶口紧塞的水瓶里的。这些喝惯无味的淡水的人，一看到长江大河式的德国音乐，就要吹毛求疵，挑德国天才的错误了。“这些可怜的孩子！”克利斯朵夫这么想着，可忘了自己从前也一样的可笑过来。“他们居然找出了瓦格纳和贝多芬的缺点！他们需要没有缺陷的天才。仿佛狂风暴雨在吹打的时候会特别小心，一点都不扰乱世界上完整的秩序！……”他在巴黎街上走着，对自己心中的力非常高兴。无人了解倒是更好！他可以更自由。天才的使命是创造，而要依着内心的法则创造一个簇新的有机体的世界，自己必须整个儿生活在里头。一个艺术家决不嫌太孤独。可怕的是，自己的思想反映到镜子里的时候被镜子把原来的形状改变了，缩小了。一件作品没有完成之前，不能告诉别人；否则你会没有勇气把作品写完；因为那时你在自己心中看到的已经不是你的，而是别人的可怜的思想。如今他的梦想既不受任何外物的扰乱，就象泉水一样从他心灵的每一个角落，从他路上碰到的每一颗石子里飞涌出来。他所生活的境界象一个能见到异象的人的境界。他所见所闻的一切，在心中唤引起来的生灵与事物，跟实际的见闻完全不同。他只要听其自然，就能发觉他幻想中的人物都在周围活动。那些感觉会自动来找到他的。路人的目光，风中传来的语声，照在草坪上的阳光，停在卢森堡公园树上歌唱的小鸟，远处修道院里的钟声，卧室中瞧见的一角苍白的天空，一日之间时时变化的声音与风光：这些他都不用自己的而用着幻想人物的心灵去体会。——他觉得非常幸福。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>Editor</tag>
        <tag>Reading</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记]]></title>
    <url>%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html</url>
    <content type="text"><![CDATA[关于Java学习的笔记。 123456789101112131415161718192021222324Scanner in = new Scanner(System.in);int x;double sum=0;int cnt=0;cnt=in.nextInt();if(cnt>0){int [] numbers=new int [cnt]; 定义数组 for(i=0;i=cnt;i++){ number[i]=in.nextInt(); 对数组元素赋值 sum+=numbers[i];} double average=sum/cnt; for(int i=0;i< numbers.length; i++) 遍历数组{ if(number[i]>average) 使用数组中的元素 { System.out.println(numbers[i]); } }System.out.println(sum/cnt);}} 定义数组变量 12345[]=new[元素个数]; int[] grades =new int[100];double[] averages= new double[20]; 元素个数必须是整数元素个数必须给出元素个数可以是变量 直接初始化数组 1int[] scores={87,98,54}; 数组变量是数组的管理者而非数组本身 数组必须创建出来然后交给数组变量来管理 数组变量之间的赋值是管理权限的赋予 数组变量之间的比较是判断是否管理同一个数组 投票统计 1234567891011121314151617181920写一个程序，输入数量不确定的[0,9]范围内的整数，统计每一种数字出现的次数，输入-1表示结束。 Scanner in=new Scanner(System.in); int x; int [] numbers =new int[10]; x=in.nextInt(); while(x!=-1) { if(x>=0&&x]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Editor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[胡思乱，想]]></title>
    <url>%2F%E8%83%A1%E6%80%9D%E4%B9%B1%EF%BC%8C%E6%83%B3.html</url>
    <content type="text"><![CDATA[本文写于2014年3月12日 周三 中雨 10℃ 两天前，周一的班会。我们在创造中心用几十根蜡烛，摆了个下面图案: 虽然期间出了不少差错，有说有笑。但当成功摆完之后，当我们在这里围成一圈的时候，周围的肃静是令人感动的。我们像这样站着 我们在这样的氛围中，一个接着一个发表对于生命的意义的理解。平时最嘻哈的人，此刻也异常的肃静。面对这世界有太多的未知，我们的内心或多或少有一种负重感。每个人所说的，我大概已经记不清了，我只记得我所说得或许太过于悲观。 烛火在被摆成的过程，是经过好多人的共同努力。摆成“生命”二字虽是毋庸置疑的。但它最终会成为什么样的形状，那细微的差别，由谁不小心的一脚，无意中的一个动作造成的，谁也无法弄清楚。我想说的是，人生就是一次次充满无数可能的事件拼接而成的。而烛火不断地被点燃，也不断地被风吹灭，或许它自己无法再次让自己燃烧，但只要有人轻微的一个点火的动作，它又会再次迸发出耀眼的火焰。 人生有太多的大风大浪，每一次来袭你都会被吹灭，但悲哀的是，你是否是孤独的？😶 你若孤零零的一根蜡烛，熄灭与否，对于大环境来说，没有人会在意。若四周一片黑暗，唯独只有你一根，那么你的存在至关重要，至少，你的光芒还不至于是毫无意义的。此时的你会引起太多人的关注，曾经闪耀的你，此刻奄奄一息。有的人关切你，有的人讽刺你，有的人嘲笑你，有的人问候你。这些或许你都可以不在乎，你只需记住点燃你的那一位。 你若是一群蜡烛中的一个，熄灭你无伤大雅，点燃为了更好的视觉效果或照明。这种微妙的关系，令人难以把握。他人在燃烧自己的同时，你被大风痛苦地吹灭了。啊，啊，世界多残酷啊。没有人会在意你是不幸的，并且靠你自身的能力，你无法点燃自己。但别忘了，蜡烛们的生命是短暂的，他们错误地在集体中用光了自己。因为一大团蜡烛都在发光，你却也加入了，与之前没有任何不同。你本可以等他们耗尽，此时的你便是独一无二的。 如今，有多少人是在无知与无畏中虚度了自己。你平庸是因为他人在做无意义的事情时，你也跟着做。一件事第一个做那是有意义的，第二个人也有意义，接下来三、四······便显得是拥挤不堪的无用功。一个内心充满斗志与一个斗志已燃烧殆尽的人对于社会的意义是不同的。燃烧殆尽的人或许会后悔当初自己在无意义的地方尽了力，而斗志满槽就是那唯独被风吹熄的蜡烛，它在挫折中变得睿智，在以后的生活中变得独一无二。 斗志是需要挫折来引导的，一味地奋斗，最后只会油尽灯枯，还落了个毫无意义。只有经历磨难，斗志被搁浅，才会想斗志，是否在最初的起点，走向既定的终点。路上会有分叉，我们可以适当歇一歇，当机立断未必是好事，仔细考虑，以免留下终生遗憾。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });]]></content>
      <categories>
        <category>Thinking</category>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Editor</tag>
        <tag>Diary</tag>
        <tag>Thinking</tag>
        <tag>随笔</tag>
        <tag>日记</tag>
      </tags>
  </entry>
</search>
